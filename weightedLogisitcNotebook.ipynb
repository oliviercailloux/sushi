{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USERS = 5000\n",
    "FEAT_USER = 3\n",
    "NB_ITEMS = 100\n",
    "FEAT_ITEM = 6\n",
    "\n",
    "NB_EPOCHS = 500\n",
    "LAMBDA_REG = 1e-5\n",
    "# LAMBDA_REG = 0\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv('data/sushi/sushi3.udata', sep='\\t', names=('uid', 'gender', 'age', 'time', 'old_prefecture', 'old_region', 'old_eastwest', 'prefecture', 'region', 'eastwest', 'same'))\n",
    "items = pd.read_csv('data/sushi/sushi3.idata', sep='\\t', names=('iid', 'name', 'style', 'major', 'minor', 'heaviness', 'frequency', 'price', 'popularity'))\n",
    "R = pd.read_csv('data/sushi/sushi3b.5000.10.score', sep=' ', header=None)\n",
    "triplets = []\n",
    "for i, line in enumerate(np.array(R)):\n",
    "    for j, v in enumerate(line):\n",
    "        if v != -1:\n",
    "            triplets.append((i, j, v))\n",
    "df_ratings = pd.DataFrame(triplets, columns=('user', 'item', 'rating'))\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "#A is not used (only the \"criterion on the items\" => only B)\n",
    "#A = tf.constant(np.array(users[['age', 'gender', 'region']]).astype(np.float32)) \n",
    "B = tf.constant(np.array(items[['style', 'major','heaviness', 'frequency', 'price', 'popularity']]).astype(np.float32))\n",
    "\n",
    "weights = tf.get_variable('W', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "alpha = tf.get_variable(\"alpha\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "beta = tf.get_variable(\"beta\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "weight_users = tf.nn.embedding_lookup(weights, user_batch)\n",
    "\n",
    "#beta_crit = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "#alpha_crit = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "\n",
    "feat_items = tf.nn.embedding_lookup(B, item_batch)\n",
    "#feat_users = tf.nn.embedding_lookup(A, user_batch)\n",
    "\n",
    "pred =tf.reduce_sum(tf.multiply(tf.nn.softmax(tf.multiply(feat_items, alpha)+beta),weight_users),1)\n",
    "\n",
    "cost_l2 = tf.losses.mean_squared_error(rate_batch, pred)\n",
    "\n",
    "regularizer=tf.nn.l2_loss(weight_users)\n",
    "\n",
    "#l2_user = tf.nn.l2_loss(weight_users)\n",
    "#l2_item = tf.nn.l2_loss(weight_items)\n",
    "#l2_bias_user = tf.nn.l2_loss(bias_users)\n",
    "#l2_bias_item = tf.nn.l2_loss(bias_items)\n",
    "#regularizer = tf.add(l2_user, l2_item)\n",
    "#regularizer = tf.add(regularizer, l2_bias_user)\n",
    "#regularizer = tf.add(regularizer, l2_bias_item)\n",
    "# regularizer = tf.nn.l2_loss(M)\n",
    "penalty = tf.constant(LAMBDA_REG, dtype=tf.float32, shape=[])\n",
    "cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 3.0930782320900163 test rmse 3.0293337083713316\n",
      "train rmse 2.9967815301060186 test rmse 2.936614207370401\n",
      "train rmse 2.9041518974932043 test rmse 2.84941626476841\n",
      "train rmse 2.8167533078684217 test rmse 2.766557008416174\n",
      "train rmse 2.7334436192786913 test rmse 2.6862265652663453\n",
      "train rmse 2.6525333207578585 test rmse 2.606837792287635\n",
      "train rmse 2.572498817851367 test rmse 2.5275450542294333\n",
      "train rmse 2.4925012660047248 test rmse 2.448224561009294\n",
      "train rmse 2.412397977420982 test rmse 2.369214389341402\n",
      "train rmse 2.3324792070678377 test rmse 2.291079521101194\n",
      "train rmse 2.2532605811606765 test rmse 2.2144390725500616\n",
      "train rmse 2.175301462963915 test rmse 2.1398506713175873\n",
      "train rmse 2.099120809799498 test rmse 2.0678005886556323\n",
      "train rmse 2.0251702507882476 test rmse 1.998719997884608\n",
      "train rmse 1.9538576506710241 test rmse 1.9330099438265\n",
      "train rmse 1.885559041529495 test rmse 1.8710453927116586\n",
      "train rmse 1.8206312039731543 test rmse 1.8131408216278582\n",
      "train rmse 1.759382911244251 test rmse 1.7595168603651343\n",
      "train rmse 1.702045413481624 test rmse 1.7102914923129269\n",
      "train rmse 1.648752842250685 test rmse 1.6654858300216941\n",
      "train rmse 1.5995581761924285 test rmse 1.6250400538276555\n",
      "train rmse 1.5544536932339186 test rmse 1.5888296731319596\n",
      "train rmse 1.5133817136203958 test rmse 1.5566613783464043\n",
      "train rmse 1.476237024028964 test rmse 1.5282744476943877\n",
      "train rmse 1.4428499430027613 test rmse 1.5033466833893516\n",
      "train rmse 1.412993221760749 test rmse 1.4815093638304104\n",
      "train rmse 1.38637609412257 test rmse 1.4623733237416952\n",
      "train rmse 1.3626744359896665 test rmse 1.4455383124379064\n",
      "train rmse 1.3415363175391677 test rmse 1.4306087339983915\n",
      "train rmse 1.3226033938685224 test rmse 1.417207623962387\n",
      "train rmse 1.3055144561674321 test rmse 1.4049871920616137\n",
      "train rmse 1.2899234093248069 test rmse 1.3936436497094544\n",
      "train rmse 1.2755232036667723 test rmse 1.3829426677236816\n",
      "train rmse 1.2620621913542478 test rmse 1.3727403192715466\n",
      "train rmse 1.2493738513538601 test rmse 1.3629920455969189\n",
      "train rmse 1.2373833032936963 test rmse 1.3537362319372965\n",
      "train rmse 1.2261033383234317 test rmse 1.3450757960407758\n",
      "train rmse 1.2156128733209057 test rmse 1.3371461899436967\n",
      "train rmse 1.2060334856277852 test rmse 1.3300823822177419\n",
      "train rmse 1.1974884373498853 test rmse 1.3239828254041013\n",
      "train rmse 1.190079247055872 test rmse 1.3189028375656258\n",
      "train rmse 1.1838665638364185 test rmse 1.314843900502605\n",
      "train rmse 1.1788617711559142 test rmse 1.311750697643748\n",
      "train rmse 1.1750266863450578 test rmse 1.3095384063361306\n",
      "train rmse 1.172280000002978 test rmse 1.308105423184373\n",
      "train rmse 1.1705157726061097 test rmse 1.3073370065716916\n",
      "train rmse 1.169606212387207 test rmse 1.3071243918750257\n",
      "train rmse 1.1694142764094768 test rmse 1.3073572038532935\n",
      "train rmse 1.169798422590377 test rmse 1.3079307584721447\n",
      "train rmse 1.1706299334746444 test rmse 1.308756029685339\n",
      "train rmse 1.1717821720232688 test rmse 1.309746351364771\n",
      "train rmse 1.1731475532814808 test rmse 1.310835100250767\n",
      "train rmse 1.1746322746315458 test rmse 1.3119616539747814\n",
      "train rmse 1.1761588570676649 test rmse 1.3130854254691382\n",
      "train rmse 1.1776713643844576 test rmse 1.3141755807236013\n",
      "train rmse 1.1791241039417089 test rmse 1.315206416722003\n",
      "train rmse 1.1804852851199394 test rmse 1.3161651209374166\n",
      "train rmse 1.1817382772149512 test rmse 1.3170365089766791\n",
      "train rmse 1.182870977813129 test rmse 1.3178159596915873\n",
      "train rmse 1.1838816679705426 test rmse 1.3184904810388614\n",
      "train rmse 1.1847699053014453 test rmse 1.3190636682132408\n",
      "train rmse 1.185544210364978 test rmse 1.3195272966216125\n",
      "train rmse 1.1862139011594384 test rmse 1.3198929972338795\n",
      "train rmse 1.1867874416940447 test rmse 1.3201605353848531\n",
      "train rmse 1.1872804338350333 test rmse 1.3203469446869551\n",
      "train rmse 1.1877063270814647 test rmse 1.320462731837546\n",
      "train rmse 1.1880741237320802 test rmse 1.3205275501524647\n",
      "train rmse 1.1883957646534886 test rmse 1.3205591908065482\n",
      "train rmse 1.1886820682084824 test rmse 1.3205740404384403\n",
      "train rmse 1.1889354663015925 test rmse 1.3205903342192973\n",
      "train rmse 1.189160591320263 test rmse 1.3206106447449728\n",
      "train rmse 1.1893601655260482 test rmse 1.3206480604289588\n",
      "train rmse 1.1895315460761906 test rmse 1.320693553584115\n",
      "train rmse 1.189675045764887 test rmse 1.3207504178244547\n",
      "train rmse 1.1897903740996696 test rmse 1.320807099106886\n",
      "train rmse 1.189875735919355 test rmse 1.3208584982623786\n",
      "train rmse 1.1899297351288278 test rmse 1.320899155878991\n",
      "train rmse 1.1899534779759289 test rmse 1.320923252033384\n",
      "train rmse 1.1899468160012443 test rmse 1.3209307425086738\n",
      "train rmse 1.1899120028200663 test rmse 1.3209138663176432\n",
      "train rmse 1.1898467316217567 test rmse 1.3208789851645766\n",
      "train rmse 1.1897550553584963 test rmse 1.320817117360851\n",
      "train rmse 1.1896406254553367 test rmse 1.320735840954938\n",
      "train rmse 1.1895047882836758 test rmse 1.3206315868265777\n",
      "train rmse 1.1893527986155263 test rmse 1.320511706972353\n",
      "train rmse 1.1891886099109832 test rmse 1.3203781382480009\n",
      "train rmse 1.1890181825176367 test rmse 1.3202395447207056\n",
      "train rmse 1.1888478309656163 test rmse 1.320100485126992\n",
      "train rmse 1.1886799120372755 test rmse 1.319966197448024\n",
      "train rmse 1.1885200436113212 test rmse 1.3198419669224954\n",
      "train rmse 1.1883726928759162 test rmse 1.319729783620343\n",
      "train rmse 1.1882401217855227 test rmse 1.3196355224027976\n",
      "train rmse 1.1881267499835568 test rmse 1.3195564769007013\n",
      "train rmse 1.1880307266423242 test rmse 1.319499335346293\n",
      "train rmse 1.1879551165961346 test rmse 1.319458499075115\n",
      "train rmse 1.1878985188004583 test rmse 1.3194438175846863\n",
      "train rmse 1.1878609861510474 test rmse 1.3194456245461703\n",
      "train rmse 1.1878411656237404 test rmse 1.3194719607289278\n",
      "train rmse 1.1878364989704762 test rmse 1.3195142872489674\n",
      "train rmse 1.1878466852974827 test rmse 1.319575583761715\n",
      "train rmse 1.1878676598237907 test rmse 1.3196490725848937\n",
      "train rmse 1.1878975654457 test rmse 1.3197284738555883\n",
      "train rmse 1.1879341937890302 test rmse 1.3198123865024094\n",
      "train rmse 1.187973630715253 test rmse 1.319890061915466\n",
      "train rmse 1.1880143707960336 test rmse 1.3199645718241728\n",
      "train rmse 1.1880557616887244 test rmse 1.3200244024939942\n",
      "train rmse 1.1880935390266234 test rmse 1.3200750193807975\n",
      "train rmse 1.1881296596606294 test rmse 1.3201123599435283\n",
      "train rmse 1.1881610134873355 test rmse 1.3201372380151293\n",
      "train rmse 1.1881898081129356 test rmse 1.3201520923933256\n",
      "train rmse 1.1882144385020004 test rmse 1.3201557946743327\n",
      "train rmse 1.1882375133523673 test rmse 1.320153808085815\n",
      "train rmse 1.1882569259772373 test rmse 1.3201439653986065\n",
      "train rmse 1.1882731280027325 test rmse 1.3201347095936835\n",
      "train rmse 1.1882890288470167 test rmse 1.320123241335616\n",
      "train rmse 1.1883036754932363 test rmse 1.3201151141632816\n",
      "train rmse 1.188317419099826 test rmse 1.32010829633012\n",
      "train rmse 1.1883301593814817 test rmse 1.3201062193673252\n",
      "train rmse 1.188343300788741 test rmse 1.3201076190600025\n",
      "train rmse 1.1883560407929208 test rmse 1.3201118632803381\n",
      "train rmse 1.1883677775262929 test rmse 1.320120351680074\n",
      "train rmse 1.1883796144562202 test rmse 1.32012915607958\n",
      "train rmse 1.1883907490877716 test rmse 1.3201422045431561\n",
      "train rmse 1.1884006798873543 test rmse 1.320152227842814\n",
      "train rmse 1.1884096075050328 test rmse 1.3201642376421827\n",
      "train rmse 1.188416278096937 test rmse 1.3201720033189492\n",
      "train rmse 1.1884226978793526 test rmse 1.3201818457971197\n",
      "train rmse 1.188427161613874 test rmse 1.3201867218600412\n",
      "train rmse 1.1884300705554378 test rmse 1.3201939456238578\n",
      "train rmse 1.1884334308756224 test rmse 1.3201967899700149\n",
      "train rmse 1.188435085955206 test rmse 1.320201801422143\n",
      "train rmse 1.1884377942622801 test rmse 1.3202041942709535\n",
      "train rmse 1.1884406530241627 test rmse 1.320209070251333\n",
      "train rmse 1.1884444646959755 test rmse 1.3202126820770406\n",
      "train rmse 1.1884479754355142 test rmse 1.3202174225832866\n",
      "train rmse 1.1884519876965736 test rmse 1.3202220727776368\n",
      "train rmse 1.1884560500970955 test rmse 1.3202287997288285\n",
      "train rmse 1.188460814623224 test rmse 1.3202355266457444\n",
      "train rmse 1.1884646763830184 test rmse 1.3202421180881332\n",
      "train rmse 1.188469741789179 test rmse 1.3202482580322608\n",
      "train rmse 1.1884732023017668 test rmse 1.3202527726788436\n",
      "train rmse 1.1884766628042789 test rmse 1.3202577387722538\n",
      "train rmse 1.1884785184319104 test rmse 1.3202609892960075\n",
      "train rmse 1.1884807251204579 test rmse 1.3202639237897449\n",
      "train rmse 1.1884821795265783 test rmse 1.3202649170015326\n",
      "train rmse 1.1884828816530342 test rmse 1.320266226234111\n",
      "train rmse 1.1884842357528844 test rmse 1.3202655490450084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 1.188483784386439 test rmse 1.3202672645900602\n",
      "train rmse 1.188484887726336 test rmse 1.320265413607146\n",
      "train rmse 1.1884849378781253 test rmse 1.3202685286744615\n",
      "train rmse 1.1884860412169516 test rmse 1.3202658650666328\n",
      "train rmse 1.1884860412169516 test rmse 1.3202703796530084\n",
      "train rmse 1.1884864925825398 test rmse 1.3202666776933196\n",
      "train rmse 1.188486743341126 test rmse 1.3202708762565674\n",
      "train rmse 1.188487395313202 test rmse 1.3202692058620358\n",
      "train rmse 1.188487746374941 test rmse 1.32026943159115\n",
      "train rmse 1.1884883481948239 test rmse 1.3202714180056914\n",
      "train rmse 1.1884885989530183 test rmse 1.3202680320700197\n",
      "train rmse 1.1884884986497468 test rmse 1.3202700184866665\n",
      "train rmse 1.18848814758823 test rmse 1.3202686641120043\n",
      "train rmse 1.188487846678276 test rmse 1.3202656393369088\n",
      "train rmse 1.1884882980431786 test rmse 1.320267806340666\n",
      "train rmse 1.188487746374941 test rmse 1.3202639237897449\n",
      "train rmse 1.1884874956165665 test rmse 1.320263652913673\n",
      "train rmse 1.1884872448581394 test rmse 1.320263427183571\n",
      "train rmse 1.1884871445547536 test rmse 1.3202602218119541\n",
      "train rmse 1.188486743341126 test rmse 1.320260673273216\n",
      "train rmse 1.1884870944030574 test rmse 1.3202585514039427\n",
      "train rmse 1.1884864424308164 test rmse 1.3202567455550662\n",
      "train rmse 1.188486291975633 test rmse 1.3202569712863108\n",
      "train rmse 1.1884854895476664 test rmse 1.3202545333868265\n",
      "train rmse 1.188483383171677 test rmse 1.3202530887035262\n",
      "train rmse 1.1884823299822824 test rmse 1.3202526372396708\n",
      "train rmse 1.1884812266400107 test rmse 1.3202495221348602\n",
      "train rmse 1.188478769192179 test rmse 1.320248574058024\n",
      "train rmse 1.1884769637170645 test rmse 1.3202458201166871\n",
      "train rmse 1.1884751582392075 test rmse 1.3202422535283853\n",
      "train rmse 1.1884728010834325 test rmse 1.3202402670699636\n",
      "train rmse 1.1884708451421127 test rmse 1.3202361135563327\n",
      "train rmse 1.1884686885876956 test rmse 1.3202332241478392\n",
      "train rmse 1.1884661809613597 test rmse 1.3202300638500588\n",
      "train rmse 1.1884642751618057 test rmse 1.3202255491258208\n",
      "train rmse 1.188461065387228 test rmse 1.3202220727776368\n",
      "train rmse 1.188458507591905 test rmse 1.320218099797075\n",
      "train rmse 1.1884551473426241 test rmse 1.3202133592932608\n",
      "train rmse 1.18845223846244 test rmse 1.3202103343914546\n",
      "train rmse 1.188449128961956 test rmse 1.3202052326756917\n",
      "train rmse 1.1884459191464714 test rmse 1.320201801422143\n",
      "train rmse 1.1884424084008582 test rmse 1.3201973768978241\n",
      "train rmse 1.1884387471836717 test rmse 1.3201923202804453\n",
      "train rmse 1.1884358382633458 test rmse 1.3201883472103462\n",
      "train rmse 1.18843237764196 test rmse 1.3201828842194363\n",
      "train rmse 1.18842856593138 test rmse 1.3201776920996853\n",
      "train rmse 1.188424453282569 test rmse 1.3201729063016552\n",
      "train rmse 1.188419939383345 test rmse 1.3201671272016648\n",
      "train rmse 1.1884151746933418 test rmse 1.3201619350199423\n",
      "train rmse 1.1884108112236338 test rmse 1.3201554786263596\n",
      "train rmse 1.1884062471174857 test rmse 1.3201490673511367\n",
      "train rmse 1.1884017833044076 test rmse 1.3201427011949312\n",
      "train rmse 1.1883970185416104 test rmse 1.3201359738050116\n",
      "train rmse 1.1883926550052377 test rmse 1.3201300590915939\n",
      "train rmse 1.1883876394263964 test rmse 1.3201227898273544\n",
      "train rmse 1.1883825235141707 test rmse 1.3201156108252485\n",
      "train rmse 1.1883774075799214 test rmse 1.3201091993564027\n",
      "train rmse 1.1883719906843577 test rmse 1.3201012978554834\n",
      "train rmse 1.1883667743912187 test rmse 1.3200943444955517\n",
      "train rmse 1.1883612069761658 test rmse 1.3200865783619413\n",
      "train rmse 1.188355388748779 test rmse 1.3200793088582672\n",
      "train rmse 1.188349971752846 test rmse 1.3200715426362002\n",
      "train rmse 1.18834410331272 test rmse 1.320063776368443\n",
      "train rmse 1.1883378837377054 test rmse 1.3200562358205652\n",
      "train rmse 1.1883313631805594 test rmse 1.3200479276221309\n",
      "train rmse 1.1883249930632587 test rmse 1.3200396645250834\n",
      "train rmse 1.188318572753002 test rmse 1.320031536838183\n",
      "train rmse 1.1883119517717184 test rmse 1.3200220544735493\n",
      "train rmse 1.1883048793191435 test rmse 1.3200130235867598\n",
      "train rmse 1.188297355387216 test rmse 1.32000412810287\n",
      "train rmse 1.1882899317276898 test rmse 1.3199946003861238\n",
      "train rmse 1.1882834109074132 test rmse 1.3199848919784156\n",
      "train rmse 1.18827608748198 test rmse 1.319974686784497\n",
      "train rmse 1.1882685132068256 test rmse 1.3199649330740904\n",
      "train rmse 1.1882608385609021 test rmse 1.3199545922561051\n",
      "train rmse 1.1882526622496 test rmse 1.3199442062001652\n",
      "train rmse 1.1882447366916937 test rmse 1.3199334136467573\n",
      "train rmse 1.1882362592959275 test rmse 1.3199227564781386\n",
      "train rmse 1.188228082815493 test rmse 1.3199118282751983\n",
      "train rmse 1.188219655463895 test rmse 1.3199008548233522\n",
      "train rmse 1.188210826745732 test rmse 1.3198892490569052\n",
      "train rmse 1.1882016468158039 test rmse 1.3198773270738307\n",
      "train rmse 1.1881930687843938 test rmse 1.3198658565793775\n",
      "train rmse 1.188183788388273 test rmse 1.3198535279446604\n",
      "train rmse 1.1881746082494444 test rmse 1.3198413798366366\n",
      "train rmse 1.1881648260562414 test rmse 1.319829096134162\n",
      "train rmse 1.1881551441139222 test rmse 1.3198163607042708\n",
      "train rmse 1.1881451610959641 test rmse 1.3198031735338862\n",
      "train rmse 1.188135478993398 test rmse 1.3197898055828474\n",
      "train rmse 1.1881248436395422 test rmse 1.319776572984447\n",
      "train rmse 1.1881144590276431 test rmse 1.319762572479918\n",
      "train rmse 1.1881037733177497 test rmse 1.3197493396084625\n",
      "train rmse 1.1880926359967032 test rmse 1.319734209712309\n",
      "train rmse 1.1880817995841244 test rmse 1.319721112050195\n",
      "train rmse 1.1880710132419687 test rmse 1.319704175225426\n",
      "train rmse 1.1880589725583095 test rmse 1.3196922515704108\n",
      "train rmse 1.1880477846470388 test rmse 1.3196747272152463\n",
      "train rmse 1.188036245435357 test rmse 1.319659777130376\n",
      "train rmse 1.1880244050842583 test rmse 1.3196451430464096\n",
      "train rmse 1.1880121130695032 test rmse 1.3196277987367466\n",
      "train rmse 1.187998917826407 test rmse 1.3196126674470778\n",
      "train rmse 1.187986475029453 test rmse 1.3195960905813289\n",
      "train rmse 1.1879735805418783 test rmse 1.319578248762897\n",
      "train rmse 1.1879597827834343 test rmse 1.3195623941851033\n",
      "train rmse 1.1879468378316262 test rmse 1.3195447777641338\n",
      "train rmse 1.187933290637981 test rmse 1.3195264383683412\n",
      "train rmse 1.1879198436411365 test rmse 1.3195095442166183\n",
      "train rmse 1.1879063463158586 test rmse 1.3194908881239624\n",
      "train rmse 1.1878912933558472 test rmse 1.3194716445171584\n",
      "train rmse 1.1878771433995075 test rmse 1.3194542075794249\n",
      "train rmse 1.1878626922062934 test rmse 1.3194339244266988\n",
      "train rmse 1.1878479899438923 test rmse 1.3194151317396607\n",
      "train rmse 1.1878327857063937 test rmse 1.319394667283228\n",
      "train rmse 1.1878168787545593 test rmse 1.3193762806245126\n",
      "train rmse 1.1878015737575491 test rmse 1.3193532404693225\n",
      "train rmse 1.1877852147558148 test rmse 1.3193365248107733\n",
      "train rmse 1.1877697588048517 test rmse 1.3193107732466691\n",
      "train rmse 1.1877528473553565 test rmse 1.3192943281252019\n",
      "train rmse 1.187735985848483 test rmse 1.3192679883964136\n",
      "train rmse 1.187718722629168 test rmse 1.3192495547914287\n",
      "train rmse 1.1877020111923875 test rmse 1.3192257895186401\n",
      "train rmse 1.1876842456233647 test rmse 1.3192026111881145\n",
      "train rmse 1.1876670318383116 test rmse 1.3191820078857597\n",
      "train rmse 1.1876491151848072 test rmse 1.3191546266833303\n",
      "train rmse 1.1876310978853268 test rmse 1.319134067816191\n",
      "train rmse 1.1876123274831794 test rmse 1.3191086737841442\n",
      "train rmse 1.187593406216056 test rmse 1.3190835051952865\n",
      "train rmse 1.1875751371203667 test rmse 1.319061273295454\n",
      "train rmse 1.187555111057978 test rmse 1.3190333021412282\n",
      "train rmse 1.1875350846578792 test rmse 1.3190089455157326\n",
      "train rmse 1.187514957534551 test rmse 1.3189835490747401\n",
      "train rmse 1.187494880263686 test rmse 1.3189552599366434\n",
      "train rmse 1.1874742003198093 test rmse 1.3189314893610793\n",
      "train rmse 1.1874536706018237 test rmse 1.3189029731434108\n",
      "train rmse 1.1874319860154765 test rmse 1.3188751342125475\n",
      "train rmse 1.187410953596242 test rmse 1.3188490120839476\n",
      "train rmse 1.1873894690220261 test rmse 1.3188191834151113\n",
      "train rmse 1.1873672812733336 test rmse 1.3187918398759992\n",
      "train rmse 1.1873450931100218 test rmse 1.3187629590595498\n",
      "train rmse 1.1873223523223504 test rmse 1.3187331284442643\n",
      "train rmse 1.187299661300972 test rmse 1.3187047435353216\n",
      "train rmse 1.1872767690346542 test rmse 1.3186737363919578\n",
      "train rmse 1.1872532738812969 test rmse 1.3186439489597306\n",
      "train rmse 1.1872289247809058 test rmse 1.3186133020069977\n",
      "train rmse 1.1872050270339154 test rmse 1.3185814338449726\n",
      "train rmse 1.1871812292195967 test rmse 1.3185514635089872\n",
      "train rmse 1.1871563765625086 test rmse 1.3185193678236626\n",
      "train rmse 1.187131171922324 test rmse 1.3184865028405135\n",
      "train rmse 1.18710626800723 test rmse 1.3184549480693304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 1.1870803593475232 test rmse 1.3184212225076233\n",
      "train rmse 1.187054701183431 test rmse 1.3183890332312143\n",
      "train rmse 1.1870291428913207 test rmse 1.3183553964060337\n",
      "train rmse 1.1870021780455227 test rmse 1.318321306597367\n",
      "train rmse 1.1869761164671924 test rmse 1.318287984539869\n",
      "train rmse 1.1869491504167518 test rmse 1.318252491327019\n",
      "train rmse 1.1869226357139566 test rmse 1.3182182179929138\n",
      "train rmse 1.186895919543127 test rmse 1.3181837628986997\n",
      "train rmse 1.1868681481491545 test rmse 1.3181479051317275\n",
      "train rmse 1.1868400245563238 test rmse 1.3181126794650757\n",
      "train rmse 1.1868116491845953 test rmse 1.3180768649852244\n",
      "train rmse 1.1867837753713009 test rmse 1.318041004310057\n",
      "train rmse 1.1867554488795287 test rmse 1.3180049165422723\n",
      "train rmse 1.1867272221637641 test rmse 1.31796760672094\n",
      "train rmse 1.1866975381850253 test rmse 1.317931833524666\n",
      "train rmse 1.1866691091776653 test rmse 1.3178945668615143\n",
      "train rmse 1.1866397251239007 test rmse 1.317857389601427\n",
      "train rmse 1.1866104408045288 test rmse 1.3178201660628006\n",
      "train rmse 1.1865810552979223 test rmse 1.3177825343936234\n",
      "train rmse 1.18655166906357 test rmse 1.3177449468820832\n",
      "train rmse 1.186522131397172 test rmse 1.3177072225975612\n",
      "train rmse 1.1864923920514088 test rmse 1.3176695877028022\n",
      "train rmse 1.186462953383795 test rmse 1.3176316350797515\n",
      "train rmse 1.1864332627931808 test rmse 1.317593455175973\n",
      "train rmse 1.1864028681025238 test rmse 1.3175556813151328\n",
      "train rmse 1.1863729248028725 test rmse 1.3175174992103151\n",
      "train rmse 1.186342428081608 test rmse 1.3174796779303044\n",
      "train rmse 1.1863128349617649 test rmse 1.3174420365353583\n",
      "train rmse 1.186283442083144 test rmse 1.3174039868688419\n",
      "train rmse 1.186253043553716 test rmse 1.3173659813486522\n",
      "train rmse 1.1862231467195248 test rmse 1.3173280199786097\n",
      "train rmse 1.186192796893609 test rmse 1.317290374250058\n",
      "train rmse 1.1861636522901648 test rmse 1.317252863193252\n",
      "train rmse 1.1861340044588167 test rmse 1.31721489856321\n",
      "train rmse 1.1861039538667582 test rmse 1.317177883126662\n",
      "train rmse 1.1860752091097486 test rmse 1.3171403236131325\n",
      "train rmse 1.1860456595774371 test rmse 1.3171038943870825\n",
      "train rmse 1.1860161093089006 test rmse 1.3170671926199489\n",
      "train rmse 1.1859866085615198 test rmse 1.317030444573199\n",
      "train rmse 1.1859582630295002 test rmse 1.3169942385982703\n",
      "train rmse 1.185929363961639 test rmse 1.31695821266529\n",
      "train rmse 1.185900916539224 test rmse 1.3169227741340177\n",
      "train rmse 1.185872669483467 test rmse 1.3168876967431336\n",
      "train rmse 1.1858443714913973 test rmse 1.316852437366038\n",
      "train rmse 1.1858173797045388 test rmse 1.3168183991770213\n",
      "train rmse 1.185790135974491 test rmse 1.3167844506387405\n",
      "train rmse 1.1857624392161608 test rmse 1.3167505012251968\n",
      "train rmse 1.1857357974407272 test rmse 1.3167170488802173\n",
      "train rmse 1.1857098588351997 test rmse 1.3166838672979129\n",
      "train rmse 1.1856841710135244 test rmse 1.316651771356533\n",
      "train rmse 1.1856582815499204 test rmse 1.316619584090798\n",
      "train rmse 1.1856334472419678 test rmse 1.316588120391183\n",
      "train rmse 1.1856080594056575 test rmse 1.3165568823052796\n",
      "train rmse 1.1855831737711704 test rmse 1.316526503687681\n",
      "train rmse 1.1855587903701315 test rmse 1.31649644129555\n",
      "train rmse 1.1855349092335292 test rmse 1.3164666951505506\n",
      "train rmse 1.1855119326122483 test rmse 1.3164374011058306\n",
      "train rmse 1.1854893074953186 test rmse 1.3164091930865887\n",
      "train rmse 1.1854668830644701 test rmse 1.3163813919754028\n",
      "train rmse 1.18544440792901 test rmse 1.3163537261174079\n",
      "train rmse 1.1854229379945045 test rmse 1.3163268747368488\n",
      "train rmse 1.1854018196467861 test rmse 1.316300385064174\n",
      "train rmse 1.185381002621536 test rmse 1.316274619383941\n",
      "train rmse 1.1853610400582963 test rmse 1.3162491701853456\n",
      "train rmse 1.1853407754499956 test rmse 1.3162243997632876\n",
      "train rmse 1.185321013351912 test rmse 1.316200534583022\n",
      "train rmse 1.1853016532162293 test rmse 1.3161761708216932\n",
      "train rmse 1.1852832985098711 test rmse 1.3161533010814628\n",
      "train rmse 1.1852650943833718 test rmse 1.3161307026705766\n",
      "train rmse 1.1852469402660761 test rmse 1.3161081491602238\n",
      "train rmse 1.1852297413721395 test rmse 1.316086184024632\n",
      "train rmse 1.1852127433896418 test rmse 1.3160649431634133\n",
      "train rmse 1.1851964492367442 test rmse 1.316044154867083\n",
      "train rmse 1.1851801548598304 test rmse 1.31602368328269\n",
      "train rmse 1.185164463766625 test rmse 1.3160039360544076\n",
      "train rmse 1.1851497280316077 test rmse 1.315984505579559\n",
      "train rmse 1.1851345897649088 test rmse 1.3159653465786993\n",
      "train rmse 1.18511914953955 test rmse 1.3159468214166443\n",
      "train rmse 1.1851051173687346 test rmse 1.315928930120163\n",
      "train rmse 1.1850917891673332 test rmse 1.3159105856268993\n",
      "train rmse 1.1850771028246567 test rmse 1.3158931467974335\n",
      "train rmse 1.1850643275703376 test rmse 1.3158766136677733\n",
      "train rmse 1.185051350989919 test rmse 1.315860125627481\n",
      "train rmse 1.1850386257556231 test rmse 1.3158433655945665\n",
      "train rmse 1.185025950682857 test rmse 1.3158277831021925\n",
      "train rmse 1.1850142814483855 test rmse 1.315812200425283\n",
      "train rmse 1.1850022097055943 test rmse 1.3157967534616766\n",
      "train rmse 1.1849906911363626 test rmse 1.3157818499143488\n",
      "train rmse 1.184979222755326 test rmse 1.3157675804023359\n",
      "train rmse 1.1849683578709067 test rmse 1.3157533560363426\n",
      "train rmse 1.184957995897587 test rmse 1.3157393580228673\n",
      "train rmse 1.1849474829291422 test rmse 1.315725948782922\n",
      "train rmse 1.184936768659592 test rmse 1.3157119051756578\n",
      "train rmse 1.1849265573172274 test rmse 1.3156990845904\n",
      "train rmse 1.1849171507318605 test rmse 1.3156861732740428\n",
      "train rmse 1.1849073919494038 test rmse 1.3156738507767558\n",
      "train rmse 1.1848980858191016 test rmse 1.3156612110364705\n",
      "train rmse 1.184888427487659 test rmse 1.3156492054359403\n",
      "train rmse 1.184879171512817 test rmse 1.3156374715544656\n",
      "train rmse 1.1848701166848545 test rmse 1.3156260547044745\n",
      "train rmse 1.184861413923877 test rmse 1.31561418469983\n",
      "train rmse 1.1848529626266053 test rmse 1.3156034925428954\n",
      "train rmse 1.1848446621866802 test rmse 1.315592030091703\n",
      "train rmse 1.1848365629134892 test rmse 1.3155815189814855\n",
      "train rmse 1.1848289163438908 test rmse 1.3155706453308162\n",
      "train rmse 1.1848212194180763 test rmse 1.3155609495831717\n",
      "train rmse 1.1848135224422602 test rmse 1.3155503023013342\n",
      "train rmse 1.1848059763389969 test rmse 1.3155408329442366\n",
      "train rmse 1.1847984804955063 test rmse 1.3155302761164984\n",
      "train rmse 1.1847918901509789 test rmse 1.3155209425415861\n",
      "train rmse 1.184784947610988 test rmse 1.315510612099912\n",
      "train rmse 1.184777954721611 test rmse 1.315502456430761\n",
      "train rmse 1.1847708611729515 test rmse 1.3154922617732183\n",
      "train rmse 1.1847641197469212 test rmse 1.3154844684706142\n",
      "train rmse 1.1847580323068765 test rmse 1.3154735487073212\n",
      "train rmse 1.1847517435962978 test rmse 1.315467567719711\n",
      "train rmse 1.1847458070228825 test rmse 1.3154558775290444\n",
      "train rmse 1.1847393673163893 test rmse 1.3154517542199557\n",
      "train rmse 1.1847336822339098 test rmse 1.3154385232947263\n",
      "train rmse 1.1847275946374691 test rmse 1.3154361670886023\n",
      "train rmse 1.1847222616760398 test rmse 1.3154227094458104\n",
      "train rmse 1.184716777756705 test rmse 1.3154184954081451\n",
      "train rmse 1.1847118975502624 test rmse 1.3154101126049098\n",
      "train rmse 1.1847068160768295 test rmse 1.315401412558033\n",
      "train rmse 1.184700979902233 test rmse 1.3153973343912508\n",
      "train rmse 1.1846957977575878 test rmse 1.3153862779643697\n",
      "train rmse 1.184690514965442 test rmse 1.3153818825557357\n",
      "train rmse 1.1846858862139136 test rmse 1.3153739526576465\n",
      "train rmse 1.18468015056193 test rmse 1.3153664305387054\n",
      "train rmse 1.1846755720829598 test rmse 1.3153618991209908\n",
      "train rmse 1.1846705407669818 test rmse 1.3153528362387295\n",
      "train rmse 1.1846656603700667 test rmse 1.3153479422563439\n",
      "train rmse 1.1846615346580234 test rmse 1.3153410997134074\n",
      "train rmse 1.184657257990129 test rmse 1.3153337586674982\n",
      "train rmse 1.1846529813067959 test rmse 1.3153298162370446\n",
      "train rmse 1.184649006493149 test rmse 1.3153219766563187\n",
      "train rmse 1.184645534809559 test rmse 1.3153172185065025\n",
      "train rmse 1.184641308398413 test rmse 1.3153118712318823\n",
      "train rmse 1.1846374341749497 test rmse 1.315304892552645\n",
      "train rmse 1.1846338115129758 test rmse 1.3153012672501159\n",
      "train rmse 1.1846303397848552 test rmse 1.3152941978814439\n",
      "train rmse 1.184626968676799 test rmse 1.315289439631136\n",
      "train rmse 1.1846235472438882 test rmse 1.315284998581985\n",
      "train rmse 1.184620176116502 test rmse 1.315278518248666\n",
      "train rmse 1.1846165534017514 test rmse 1.3152751194596854\n",
      "train rmse 1.1846133332015614 test rmse 1.3152696813790485\n",
      "train rmse 1.1846098110975805 test rmse 1.3152651043104198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 1.1846064902470925 test rmse 1.3152610710387231\n",
      "train rmse 1.1846034209679086 test rmse 1.3152550890848924\n",
      "train rmse 1.184600754210686 test rmse 1.3152519621436531\n",
      "train rmse 1.184597785549358 test rmse 1.3152471131041297\n",
      "train rmse 1.1845954206782303 test rmse 1.31524235468348\n",
      "train rmse 1.1845931564354928 test rmse 1.3152389558010469\n",
      "train rmse 1.184590087121761 test rmse 1.315233744164257\n",
      "train rmse 1.1845871184337005 test rmse 1.315230662491048\n",
      "train rmse 1.1845852567102733 test rmse 1.3152264025191338\n",
      "train rmse 1.184582841497143 test rmse 1.315222459766876\n",
      "train rmse 1.1845803256448961 test rmse 1.3152189248754926\n",
      "train rmse 1.1845780613733055 test rmse 1.3152147555042135\n",
      "train rmse 1.184575948049249 test rmse 1.3152114018698977\n",
      "train rmse 1.1845731805953787 test rmse 1.3152080482270305\n",
      "train rmse 1.184571067262615 test rmse 1.3152043773378725\n",
      "train rmse 1.1845691048788156 test rmse 1.3152015221947773\n",
      "train rmse 1.184566538679711 test rmse 1.3151972168085484\n",
      "train rmse 1.1845650794659761 test rmse 1.3151945429300065\n",
      "train rmse 1.1845635699326347 test rmse 1.3151911439240125\n",
      "train rmse 1.1845613559469208 test rmse 1.3151880168307395\n",
      "train rmse 1.1845596954549198 test rmse 1.315185206972471\n",
      "train rmse 1.1845577833703385 test rmse 1.3151823064673163\n",
      "train rmse 1.1845561228733295 test rmse 1.3151784542240184\n",
      "train rmse 1.1845545630103824 test rmse 1.3151770492854193\n",
      "train rmse 1.1845530534636395 test rmse 1.3151721999696921\n",
      "train rmse 1.1845510910499975 test rmse 1.3151719280449163\n",
      "train rmse 1.184549631817233 test rmse 1.3151670787103062\n",
      "train rmse 1.1845480719457384 test rmse 1.3151666255005505\n",
      "train rmse 1.184547417805468 test rmse 1.315163090459096\n",
      "train rmse 1.1845456063382187 test rmse 1.3151599632991195\n",
      "train rmse 1.1845444490104702 test rmse 1.3151596460505905\n",
      "train rmse 1.184543442637596 test rmse 1.3151549779562448\n",
      "train rmse 1.184541882757951 test rmse 1.3151537089568592\n",
      "train rmse 1.1845413795705306 test rmse 1.3151514882049882\n",
      "train rmse 1.1845400712822372 test rmse 1.3151484516606464\n",
      "train rmse 1.1845390145867916 test rmse 1.3151474999064992\n",
      "train rmse 1.1845375553391502 test rmse 1.3151443727094512\n",
      "train rmse 1.184536599279376 test rmse 1.315142151941815\n",
      "train rmse 1.1845356935378253 test rmse 1.3151411095393863\n",
      "train rmse 1.1845349387526707 test rmse 1.315138027649113\n",
      "train rmse 1.1845340330098504 test rmse 1.3151365773453088\n",
      "train rmse 1.1845329259899073 test rmse 1.3151350817178356\n",
      "train rmse 1.1845323724795478 test rmse 1.3151325436794434\n",
      "train rmse 1.184531164819684 test rmse 1.3151312746584105\n",
      "train rmse 1.1845300074778256 test rmse 1.315129189835484\n",
      "train rmse 1.184529403646842 test rmse 1.3151271503315969\n",
      "train rmse 1.1845287494962622 test rmse 1.3151262438844096\n",
      "train rmse 1.1845280450260032 test rmse 1.3151238871187987\n",
      "train rmse 1.1845277934293805 test rmse 1.31512221018685\n",
      "train rmse 1.1845275418327046 test rmse 1.3151212130911643\n",
      "train rmse 1.1845268876810966 test rmse 1.3151192642201408\n",
      "train rmse 1.1845260825709296 test rmse 1.3151180858316094\n",
      "train rmse 1.1845258812933022 test rmse 1.3151164995376883\n",
      "train rmse 1.184525076182451 test rmse 1.3151150492101429\n",
      "train rmse 1.1845248245851978 test rmse 1.3151137801722288\n",
      "train rmse 1.1845246233073567 test rmse 1.3151122845188292\n",
      "train rmse 1.1845244220294815 test rmse 1.3151112420927267\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_global_step()\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(cost, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(NB_EPOCHS):\n",
    "        _, train_pred, train_mse, reg, pen, train_cost = sess.run([train_op, pred, cost_l2, regularizer, penalty, cost], feed_dict={\n",
    "            user_batch: train['user'],\n",
    "            item_batch: train['item'],\n",
    "            rate_batch: train['rating']\n",
    "        })\n",
    "        test_pred, test_mse = sess.run([pred, cost_l2], feed_dict={\n",
    "            user_batch: test['user'],\n",
    "            item_batch: test['item'],\n",
    "            rate_batch: test['rating']\n",
    "        })\n",
    "        print('train rmse', train_mse ** 0.5, 'test rmse', test_mse ** 0.5)\n",
    "        # print('reg', reg, 'full cost', train_cost)\n",
    "        \n",
    "    #print(weights.eval(session=sess))\n",
    "    W_mat=weights.eval(session=sess)\n",
    "    alpha_vec= alpha.eval(session=sess)\n",
    "    beta_vec= beta.eval(session=sess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.8738337e-05  1.9294201e-05  1.3087435e-04  1.2566648e+00\n",
      "   7.3985589e-06  2.0610560e-02]\n",
      " [ 3.2607506e-06 -3.3195956e-06 -2.0274478e-05  5.0081193e-01\n",
      "  -1.3728210e-09 -1.5982183e-03]\n",
      " [-2.3860825e-05 -2.1961761e-05 -3.4835134e-04  2.2185493e+00\n",
      "  -3.1537449e-05 -5.7667527e-02]\n",
      " ...\n",
      " [-1.6882614e-05  2.3747984e-06  2.5604702e-06  2.0864081e+00\n",
      "  -3.3230835e-06 -1.0508324e-03]\n",
      " [ 1.0521260e-06 -1.4005027e-06 -1.4277983e-04  2.6976762e+00\n",
      "  -3.9130026e-07 -1.1859034e-02]\n",
      " [-1.7295177e-06 -5.9381155e-06 -5.2427469e-05  2.5884979e+00\n",
      "  -2.7851449e-06 -4.6925615e-03]]\n",
      "[-3.222721   -2.2427385  -0.4189402   6.2142725  -1.7571095  -0.91200113]\n",
      "[-0.95939165 -2.9488835   0.03732969  2.477148   -1.4513202   3.9942234 ]\n"
     ]
    }
   ],
   "source": [
    "print(W_mat)\n",
    "print(alpha_vec)\n",
    "print(beta_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(W_mat)\n",
    "\n",
    "# add some prints/visualisation\n",
    "# check encoding B\n",
    "# positivity on W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO => comparer avec SVD++ / baseline constante (predire la moyenne des ratins)\n",
    "# pourquoi le modele bilineaire n'Ã©tait pas convaicant ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
