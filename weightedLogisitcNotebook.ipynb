{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USERS = 5000\n",
    "FEAT_USER = 3\n",
    "NB_ITEMS = 100\n",
    "FEAT_ITEM = 6\n",
    "\n",
    "NB_EPOCHS = 500\n",
    "LAMBDA_REG = 1e-5\n",
    "# LAMBDA_REG = 0\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv('data/sushi/sushi3.udata', sep='\\t', names=('uid', 'gender', 'age', 'time', 'old_prefecture', 'old_region', 'old_eastwest', 'prefecture', 'region', 'eastwest', 'same'))\n",
    "items = pd.read_csv('data/sushi/sushi3.idata', sep='\\t', names=('iid', 'name', 'style', 'major', 'minor', 'heaviness', 'frequency', 'price', 'popularity'))\n",
    "R = pd.read_csv('data/sushi/sushi3b.5000.10.score', sep=' ', header=None)\n",
    "triplets = []\n",
    "for i, line in enumerate(np.array(R)):\n",
    "    for j, v in enumerate(line):\n",
    "        if v != -1:\n",
    "            triplets.append((i, j, v))\n",
    "df_ratings = pd.DataFrame(triplets, columns=('user', 'item', 'rating'))\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f52244ae3c8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Synthetic data\n",
    "# x in (0,1)\n",
    "# y in (0,1)\n",
    "# preciser x0, x1, y0 et y1 => les parametres de la fonction s'en déduisent\n",
    "# piecewise linear function (used in the partial value function )\n",
    "def piecewise_linear(x, x0, x1, y0, y1):\n",
    "    condlist = [x < x0, (x >= x0) & (x < x1), x >= x1]\n",
    "    funclist = [lambda x: (y0/x0)*x , lambda x: ((y1-y0)/(x1-x0))*(x-x0) + y0, lambda x: ((1-y1)/(1-x1))*(x-x1) + y1]\n",
    "    return np.piecewise(x, condlist, funclist)\n",
    "\n",
    "\n",
    "\n",
    "#NB_USERS = 5000\n",
    "#FEAT_USER = 3\n",
    "#NB_ITEMS = 100\n",
    "#FEAT_ITEM = 6\n",
    "\n",
    "for i in range(NB_USERS)\n",
    "    for j in range(FEAT_ITEM)\n",
    "        \n",
    "\n",
    "X_0 = 0.3\n",
    "X_1 = 0.6\n",
    "Y_0 = 0.1\n",
    "Y_1 = 0.9\n",
    "\n",
    "\n",
    "x = np.random.uniform(low=0.0, high=1.0, size=(100))\n",
    "x.sort()\n",
    "#print(x)\n",
    "#print(y)\n",
    "y = piecewise_linear(x, X_0, X_1, Y_0, Y_1)\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline  \n",
    "#plt.plot(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "#A is not used (only the \"criterion on the items\" => only B)\n",
    "#A = tf.constant(np.array(users[['age', 'gender', 'region']]).astype(np.float32)) \n",
    "B = tf.constant(np.array(items[['style', 'major','heaviness', 'frequency', 'price', 'popularity']]).astype(np.float32))\n",
    "\n",
    "weights = tf.get_variable('W', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "alpha = tf.get_variable(\"alpha\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "beta = tf.get_variable(\"beta\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "user_bias = tf.get_variable(\"user_bias\", shape=[NB_USERS],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "#item_bias = tf.get_variable(\"item_bias\", shape=[NB_ITEMS],\n",
    "#                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "bias_users = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "#bias_items = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "weight_users = tf.nn.embedding_lookup(weights, user_batch)\n",
    "\n",
    "#beta_crit = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "#alpha_crit = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "\n",
    "feat_items = tf.nn.embedding_lookup(B, item_batch)\n",
    "#feat_users = tf.nn.embedding_lookup(A, user_batch)\n",
    "\n",
    "pred =(tf.reduce_sum(tf.multiply(tf.nn.softmax(tf.multiply(feat_items, alpha)+beta),weight_users),1) + bias_users)\n",
    "       #s+bias_items)\n",
    "\n",
    "cost_l2 = tf.losses.mean_squared_error(rate_batch, pred)\n",
    "\n",
    "regularizer=tf.nn.l2_loss(weight_users)\n",
    "\n",
    "#l2_user = tf.nn.l2_loss(weight_users)\n",
    "#l2_item = tf.nn.l2_loss(weight_items)\n",
    "#l2_bias_user = tf.nn.l2_loss(bias_users)\n",
    "#l2_bias_item = tf.nn.l2_loss(bias_items)\n",
    "#regularizer = tf.add(l2_user, l2_item)\n",
    "#regularizer = tf.add(regularizer, l2_bias_user)\n",
    "#regularizer = tf.add(regularizer, l2_bias_item)\n",
    "# regularizer = tf.nn.l2_loss(M)\n",
    "penalty = tf.constant(LAMBDA_REG, dtype=tf.float32, shape=[])\n",
    "cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 3.1617887639540707 test rmse 2.9872651011801756\n",
      "train rmse 2.9886938670402143 test rmse 2.8202922957692595\n",
      "train rmse 2.8208548322916727 test rmse 2.6588119717698744\n",
      "train rmse 2.6586494827818394 test rmse 2.5037671316839605\n",
      "train rmse 2.5027551727835906 test rmse 2.3553251156260937\n",
      "train rmse 2.353076333483717 test rmse 2.2140613499371242\n",
      "train rmse 2.2100199490505545 test rmse 2.0814167998319806\n",
      "train rmse 2.0748239557433927 test rmse 1.9585568388423023\n",
      "train rmse 1.9484376737568525 test rmse 1.8461265118727253\n",
      "train rmse 1.8313579176002646 test rmse 1.7446852086930327\n",
      "train rmse 1.7240035482356826 test rmse 1.6549056733325238\n",
      "train rmse 1.6268947265695226 test rmse 1.577210567419808\n",
      "train rmse 1.5403534562866372 test rmse 1.5115674951079245\n",
      "train rmse 1.4644836797042557 test rmse 1.4575845521771988\n",
      "train rmse 1.3992612320048912 test rmse 1.4144432440462402\n",
      "train rmse 1.3443368029699017 test rmse 1.3809510541856278\n",
      "train rmse 1.2989453255686045 test rmse 1.3558976871635338\n",
      "train rmse 1.2621922032796826 test rmse 1.3381665467356327\n",
      "train rmse 1.2331617162915907 test rmse 1.3265798219374354\n",
      "train rmse 1.2107899729785783 test rmse 1.3199167053356238\n",
      "train rmse 1.193912658794023 test rmse 1.3170072276062643\n",
      "train rmse 1.1814016053131546 test rmse 1.3167352916040178\n",
      "train rmse 1.172192797484869 test rmse 1.3181124081469169\n",
      "train rmse 1.165346960293236 test rmse 1.3203802147830663\n",
      "train rmse 1.1601389874031358 test rmse 1.32298851819293\n",
      "train rmse 1.1560288810653117 test rmse 1.3255029263688933\n",
      "train rmse 1.1525610250175327 test rmse 1.3275559047558319\n",
      "train rmse 1.149328962755486 test rmse 1.3289089806117687\n",
      "train rmse 1.1460297965192212 test rmse 1.3295132281881619\n",
      "train rmse 1.1425187054695067 test rmse 1.3294309142154368\n",
      "train rmse 1.1387281976448866 test rmse 1.3287394729010642\n",
      "train rmse 1.1345886512675614 test rmse 1.3275403699465755\n",
      "train rmse 1.1300786672064083 test rmse 1.3259395794803839\n",
      "train rmse 1.1252483517648066 test rmse 1.3239975465999587\n",
      "train rmse 1.1201391109858654 test rmse 1.3217646270918102\n",
      "train rmse 1.1147813829006956 test rmse 1.319385225517515\n",
      "train rmse 1.1092451315070784 test rmse 1.3170643867707612\n",
      "train rmse 1.103610522485574 test rmse 1.3149688751830575\n",
      "train rmse 1.097923219191936 test rmse 1.3131649512741195\n",
      "train rmse 1.0922416095127605 test rmse 1.3116001950296696\n",
      "train rmse 1.0866197048660744 test rmse 1.3101469927866203\n",
      "train rmse 1.0810688021323378 test rmse 1.3087644551088247\n",
      "train rmse 1.0756023582367387 test rmse 1.307564584074251\n",
      "train rmse 1.0702504049512902 test rmse 1.30668815674594\n",
      "train rmse 1.0650056017697007 test rmse 1.3061903561203527\n",
      "train rmse 1.0598601476611915 test rmse 1.3059951716183353\n",
      "train rmse 1.0548142533606153 test rmse 1.3059766419499423\n",
      "train rmse 1.0498501648034841 test rmse 1.3061415741378437\n",
      "train rmse 1.0449612904182601 test rmse 1.3066273504703365\n",
      "train rmse 1.0401378602575067 test rmse 1.307531306984233\n",
      "train rmse 1.0353577615623937 test rmse 1.308793192226337\n",
      "train rmse 1.0306179537280684 test rmse 1.310244802577121\n",
      "train rmse 1.0259128367038761 test rmse 1.3118176730253552\n",
      "train rmse 1.021229524348798 test rmse 1.3136453172697171\n",
      "train rmse 1.016574591756182 test rmse 1.3158663766121268\n",
      "train rmse 1.0119478415553964 test rmse 1.3184121354460054\n",
      "train rmse 1.007356466571344 test rmse 1.3210446737187325\n",
      "train rmse 1.0028091551385232 test rmse 1.3236132112165941\n",
      "train rmse 0.9983083304127857 test rmse 1.3262296728649143\n",
      "train rmse 0.9938691973425069 test rmse 1.3290742062704082\n",
      "train rmse 0.9894951764375182 test rmse 1.33212633436805\n",
      "train rmse 0.9851921565071583 test rmse 1.335175413972966\n",
      "train rmse 0.9809660239445946 test rmse 1.338078751528722\n",
      "train rmse 0.9768168308953592 test rmse 1.3409100618491163\n",
      "train rmse 0.9727511991553757 test rmse 1.3437834447202872\n",
      "train rmse 0.9687675966695064 test rmse 1.3466686223428204\n",
      "train rmse 0.964867718974165 test rmse 1.3494570876680443\n",
      "train rmse 0.9610488330886177 test rmse 1.3521563157112864\n",
      "train rmse 0.9573101649332156 test rmse 1.3548702710679148\n",
      "train rmse 0.9536490328679701 test rmse 1.357599216650117\n",
      "train rmse 0.9500628199637334 test rmse 1.3602299249054912\n",
      "train rmse 0.9465492292439222 test rmse 1.3627303357494474\n",
      "train rmse 0.9431037635510515 test rmse 1.36521891965836\n",
      "train rmse 0.9397237154062063 test rmse 1.3677710023667946\n",
      "train rmse 0.9364057194431227 test rmse 1.370283534092909\n",
      "train rmse 0.9331476590266439 test rmse 1.3726509576129877\n",
      "train rmse 0.9299467670935999 test rmse 1.374956260332249\n",
      "train rmse 0.9268003882063645 test rmse 1.3773253025803718\n",
      "train rmse 0.9237081760338276 test rmse 1.3797063046445086\n",
      "train rmse 0.920667277477024 test rmse 1.3819603746008269\n",
      "train rmse 0.9176760267675692 test rmse 1.3841067792708932\n",
      "train rmse 0.9147335751693907 test rmse 1.3862829678797133\n",
      "train rmse 0.9118391857997152 test rmse 1.3884917623943833\n",
      "train rmse 0.9089905309530147 test rmse 1.390599454152211\n",
      "train rmse 0.9061884694751006 test rmse 1.392585786474695\n",
      "train rmse 0.9034308288602372 test rmse 1.3945694178211894\n",
      "train rmse 0.9007173884467885 test rmse 1.3965680717515885\n",
      "train rmse 0.8980461595158259 test rmse 1.398465800931332\n",
      "train rmse 0.8954169207490182 test rmse 1.4002371348774358\n",
      "train rmse 0.8928282072283156 test rmse 1.4019949646640313\n",
      "train rmse 0.8902767906110756 test rmse 1.403772120789777\n",
      "train rmse 0.8877618846543606 test rmse 1.405473664356268\n",
      "train rmse 0.8852799627947873 test rmse 1.4070936851056668\n",
      "train rmse 0.8828284338129261 test rmse 1.4087409104982282\n",
      "train rmse 0.8804025755472393 test rmse 1.4104328254138454\n",
      "train rmse 0.8779988670037678 test rmse 1.4120888609603042\n",
      "train rmse 0.875612623466247 test rmse 1.413725670589149\n",
      "train rmse 0.8732386982096609 test rmse 1.4154323455208673\n",
      "train rmse 0.8708698344147949 test rmse 1.4172040069863665\n",
      "train rmse 0.8685022645831849 test rmse 1.4189691718787225\n",
      "train rmse 0.8661276380532074 test rmse 1.420734325251987\n",
      "train rmse 0.8637398929805099 test rmse 1.422561395992898\n",
      "train rmse 0.8613329344461638 test rmse 1.4244047025736286\n",
      "train rmse 0.8589007369432342 test rmse 1.4261950584748622\n",
      "train rmse 0.8564376917709874 test rmse 1.42797407024681\n",
      "train rmse 0.8539370057482135 test rmse 1.4297877208831842\n",
      "train rmse 0.8513943917465007 test rmse 1.4316116475793508\n",
      "train rmse 0.8488057512335664 test rmse 1.4334263508947123\n",
      "train rmse 0.8461683020323911 test rmse 1.4352698233929335\n",
      "train rmse 0.8434789363467463 test rmse 1.4371719815386235\n",
      "train rmse 0.8407372268582165 test rmse 1.4390706313708075\n",
      "train rmse 0.8379431932582714 test rmse 1.4409549492511848\n",
      "train rmse 0.8350988084613977 test rmse 1.442868945641379\n",
      "train rmse 0.832205239313525 test rmse 1.4448251263356815\n",
      "train rmse 0.829266966371625 test rmse 1.4467916806579548\n",
      "train rmse 0.8262883097529279 test rmse 1.4487536730320656\n",
      "train rmse 0.8232748764987411 test rmse 1.45074908539794\n",
      "train rmse 0.8202337681362684 test rmse 1.4527475010278124\n",
      "train rmse 0.8171713862097733 test rmse 1.4547542339551127\n",
      "train rmse 0.8140959837135591 test rmse 1.4567659765532401\n",
      "train rmse 0.8110149824284877 test rmse 1.458846610432288\n",
      "train rmse 0.807936285927345 test rmse 1.460898250929784\n",
      "train rmse 0.8048684370446033 test rmse 1.4630023419615017\n",
      "train rmse 0.8018191055119726 test rmse 1.465015370805845\n",
      "train rmse 0.798795107133481 test rmse 1.46725558284465\n",
      "train rmse 0.7958079655250782 test rmse 1.4691517868774253\n",
      "train rmse 0.7928692609641529 test rmse 1.471729989005361\n",
      "train rmse 0.790009902397095 test rmse 1.4730690294135846\n",
      "train rmse 0.7873186916959699 test rmse 1.4769661972165569\n",
      "train rmse 0.7849919161410708 test rmse 1.4770759618204865\n",
      "train rmse 0.7826523973062085 test rmse 1.480561266099431\n",
      "train rmse 0.7792896471190305 test rmse 1.482132834500749\n",
      "train rmse 0.7764354747754095 test rmse 1.4831888378501048\n",
      "train rmse 0.7745325832420881 test rmse 1.4864336695648794\n",
      "train rmse 0.7715663045524405 test rmse 1.4884072398125754\n",
      "train rmse 0.7692000940057 test rmse 1.4892653368150084\n",
      "train rmse 0.7672036303283155 test rmse 1.4919650725911888\n",
      "train rmse 0.7644982903051626 test rmse 1.494830601768849\n",
      "train rmse 0.7626877709869426 test rmse 1.4958463220200637\n",
      "train rmse 0.7604637958779865 test rmse 1.4977929249436828\n",
      "train rmse 0.7583477843308516 test rmse 1.5008410638966303\n",
      "train rmse 0.7566589936176852 test rmse 1.5025762845190092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.7545042285358303 test rmse 1.50426837475132\n",
      "train rmse 0.7529145670954597 test rmse 1.5068031846428238\n",
      "train rmse 0.7511148511455213 test rmse 1.5090300234872362\n",
      "train rmse 0.7493787815358098 test rmse 1.5109075691233007\n",
      "train rmse 0.7479440404262209 test rmse 1.513204785543564\n",
      "train rmse 0.7462331032557346 test rmse 1.5154846755674842\n",
      "train rmse 0.7448718465759749 test rmse 1.517386366660426\n",
      "train rmse 0.7434396721454203 test rmse 1.5197787438780779\n",
      "train rmse 0.7420007169493839 test rmse 1.5221280461359412\n",
      "train rmse 0.7408064591761622 test rmse 1.523870475080805\n",
      "train rmse 0.7394527848409685 test rmse 1.5261962170071732\n",
      "train rmse 0.7382691801184542 test rmse 1.528777013432409\n",
      "train rmse 0.7371503857967958 test rmse 1.5305322794558829\n",
      "train rmse 0.7359473663122269 test rmse 1.5325711059254012\n",
      "train rmse 0.734933316261444 test rmse 1.535263873106085\n",
      "train rmse 0.7338855410913497 test rmse 1.5372535414990725\n",
      "train rmse 0.7328379352116526 test rmse 1.539078455808392\n",
      "train rmse 0.7319313950152515 test rmse 1.5416429449094993\n",
      "train rmse 0.7309734212685255 test rmse 1.543839838911948\n",
      "train rmse 0.7300515028670093 test rmse 1.5456850503438815\n",
      "train rmse 0.7292282260158929 test rmse 1.548070965448791\n",
      "train rmse 0.7283662944670837 test rmse 1.5502844334164645\n",
      "train rmse 0.7275401685053464 test rmse 1.5522851066090653\n",
      "train rmse 0.726791714844012 test rmse 1.5546148081898814\n",
      "train rmse 0.7260208161551361 test rmse 1.5566962219035603\n",
      "train rmse 0.7252693563808654 test rmse 1.5588212002642607\n",
      "train rmse 0.7245841625999311 test rmse 1.561207121969599\n",
      "train rmse 0.723897867399179 test rmse 1.5631381448332593\n",
      "train rmse 0.723211580268065 test rmse 1.5653227676173656\n",
      "train rmse 0.722576363761736 test rmse 1.5677601582863088\n",
      "train rmse 0.721963994208867 test rmse 1.569619400640235\n",
      "train rmse 0.7213429244688297 test rmse 1.5718520479441507\n",
      "train rmse 0.7207464603121476 test rmse 1.5742219304888\n",
      "train rmse 0.7201871601538604 test rmse 1.576112351363082\n",
      "train rmse 0.7196338857871126 test rmse 1.578408772855335\n",
      "train rmse 0.7190827553095763 test rmse 1.5805731942046086\n",
      "train rmse 0.7185542213166766 test rmse 1.58260314400374\n",
      "train rmse 0.7180514880503153 test rmse 1.5849043012784132\n",
      "train rmse 0.7175574152766069 test rmse 1.586861702862168\n",
      "train rmse 0.7170662438770286 test rmse 1.589104782937291\n",
      "train rmse 0.7165893337384333 test rmse 1.5912283601810222\n",
      "train rmse 0.7161319985642841 test rmse 1.5931797129236578\n",
      "train rmse 0.7156900284718906 test rmse 1.5955269575050257\n",
      "train rmse 0.7152532019811837 test rmse 1.5974099265481132\n",
      "train rmse 0.7148230294314875 test rmse 1.5995722616173045\n",
      "train rmse 0.7144026932976373 test rmse 1.6016852355076476\n",
      "train rmse 0.7139963849801622 test rmse 1.6036485438020933\n",
      "train rmse 0.7136033349409351 test rmse 1.6058430850187053\n",
      "train rmse 0.713219010477771 test rmse 1.6077254647144696\n",
      "train rmse 0.7128414607404895 test rmse 1.6099660584325401\n",
      "train rmse 0.7124695671006919 test rmse 1.6117952536668676\n",
      "train rmse 0.7121055565259278 test rmse 1.613976206508377\n",
      "train rmse 0.7117496085988219 test rmse 1.6159100485826783\n",
      "train rmse 0.7114006881111041 test rmse 1.6179425969167525\n",
      "train rmse 0.7110606914724262 test rmse 1.6199533150857315\n",
      "train rmse 0.710728960577193 test rmse 1.6219305245775468\n",
      "train rmse 0.710403115785592 test rmse 1.623967209311113\n",
      "train rmse 0.7100850118864888 test rmse 1.625851334798545\n",
      "train rmse 0.7097739874725574 test rmse 1.62800181503101\n",
      "train rmse 0.7094699678416029 test rmse 1.629721238691352\n",
      "train rmse 0.7091748951062125 test rmse 1.6320378414329555\n",
      "train rmse 0.7088927322723306 test rmse 1.6335149102909787\n",
      "train rmse 0.7086332938906174 test rmse 1.636245185606003\n",
      "train rmse 0.7084221830929198 test rmse 1.6371073477752303\n",
      "train rmse 0.7083130908392389 test rmse 1.6409601505004305\n",
      "train rmse 0.7084146948443182 test rmse 1.640843258826577\n",
      "train rmse 0.708653438455719 test rmse 1.6452697400571152\n",
      "train rmse 0.7086086906700146 test rmse 1.645577068739157\n",
      "train rmse 0.7075706658270798 test rmse 1.6463365228229054\n",
      "train rmse 0.7067224028225371 test rmse 1.6503562427076883\n",
      "train rmse 0.7068792569667219 test rmse 1.6514697638020999\n",
      "train rmse 0.7069982868751156 test rmse 1.6521540683966507\n",
      "train rmse 0.7063269083801398 test rmse 1.6558789971321306\n",
      "train rmse 0.7057556743265342 test rmse 1.656758284398842\n",
      "train rmse 0.7058623752414123 test rmse 1.657976582107545\n",
      "train rmse 0.7057683213599284 test rmse 1.661640248957507\n",
      "train rmse 0.7051549219803209 test rmse 1.6619971267684541\n",
      "train rmse 0.7048864130407532 test rmse 1.6636147769857794\n",
      "train rmse 0.7049625545053437 test rmse 1.6672782808880529\n",
      "train rmse 0.7046336836470132 test rmse 1.6674823275062847\n",
      "train rmse 0.704170170799731 test rmse 1.6691020179959857\n",
      "train rmse 0.7041113610716752 test rmse 1.672709390507922\n",
      "train rmse 0.7040235288342982 test rmse 1.6731782628709566\n",
      "train rmse 0.7036200370173047 test rmse 1.6745242937888036\n",
      "train rmse 0.7033671491939658 test rmse 1.677951714290406\n",
      "train rmse 0.7033162174884258 test rmse 1.6788509681088781\n",
      "train rmse 0.7030993139025711 test rmse 1.6799694445646982\n",
      "train rmse 0.7027798547282667 test rmse 1.6831349325187994\n",
      "train rmse 0.7026093181386379 test rmse 1.684334788789063\n",
      "train rmse 0.7025045836628336 test rmse 1.685431731745753\n",
      "train rmse 0.7022822097447887 test rmse 1.6883621838970666\n",
      "train rmse 0.7020178873780103 test rmse 1.6896281774600637\n",
      "train rmse 0.7018632163589769 test rmse 1.6908101712002384\n",
      "train rmse 0.7017462245580034 test rmse 1.6936141265761098\n",
      "train rmse 0.7015361848921248 test rmse 1.6948549558533366\n",
      "train rmse 0.7013027738408881 test rmse 1.6960847563573234\n",
      "train rmse 0.7011535557130959 test rmse 1.6987900467195032\n",
      "train rmse 0.7010260937809365 test rmse 1.700036424358822\n",
      "train rmse 0.7008399709129426 test rmse 1.7013434094775388\n",
      "train rmse 0.7006358273271017 test rmse 1.703881262062985\n",
      "train rmse 0.7004714910626734 test rmse 1.705088891763329\n",
      "train rmse 0.7003359686592966 test rmse 1.70661142454795\n",
      "train rmse 0.7001855229713582 test rmse 1.7089520783778627\n",
      "train rmse 0.7000064781434162 test rmse 1.7100598596948657\n",
      "train rmse 0.6998294315972807 test rmse 1.711817406266644\n",
      "train rmse 0.6996812622691845 test rmse 1.7139375764569755\n",
      "train rmse 0.6995486541416641 test rmse 1.715038870657339\n",
      "train rmse 0.6994007875258081 test rmse 1.7169922597063032\n",
      "train rmse 0.6992371625980025 test rmse 1.718819287377391\n",
      "train rmse 0.6990769524885777 test rmse 1.7199925872731714\n",
      "train rmse 0.6989311820345621 test rmse 1.7220912707527603\n",
      "train rmse 0.6987956381209113 test rmse 1.7236185651411873\n",
      "train rmse 0.6986608997102715 test rmse 1.7249936532166417\n",
      "train rmse 0.6985209088797096 test rmse 1.7270851552452844\n",
      "train rmse 0.6983744035933013 test rmse 1.72832678501479\n",
      "train rmse 0.6982267578127459 test rmse 1.7300149223477872\n",
      "train rmse 0.6980863170442376 test rmse 1.7318872704238157\n",
      "train rmse 0.6979529148386638 test rmse 1.7330707766899198\n",
      "train rmse 0.6978244625694221 test rmse 1.7350191174643923\n",
      "train rmse 0.6976964138042177 test rmse 1.7365245386175299\n",
      "train rmse 0.6975684910648461 test rmse 1.7379136972059797\n",
      "train rmse 0.6974405021312294 test rmse 1.7398468853460187\n",
      "train rmse 0.6973107801503186 test rmse 1.741103309646869\n",
      "train rmse 0.6971818889698195 test rmse 1.7427944614297002\n",
      "train rmse 0.6970541069560469 test rmse 1.7444720834349954\n",
      "train rmse 0.6969281830615178 test rmse 1.745738221569225\n",
      "train rmse 0.6968040541385848 test rmse 1.7476145289577851\n",
      "train rmse 0.696681656980391 test rmse 1.748916631235138\n",
      "train rmse 0.6965618054132776 test rmse 1.750534793336912\n",
      "train rmse 0.6964443723751516 test rmse 1.7521465624249735\n",
      "train rmse 0.6963321838294853 test rmse 1.7534858182709432\n",
      "train rmse 0.6962274253924879 test rmse 1.7552356057543284\n",
      "train rmse 0.6961347668483712 test rmse 1.7565828218422994\n",
      "train rmse 0.6960695410221027 test rmse 1.7582331853371997\n",
      "train rmse 0.6960548980864997 test rmse 1.7598146014749199\n",
      "train rmse 0.6961452126879573 test rmse 1.761462546113622\n",
      "train rmse 0.6963986261098983 test rmse 1.7630152361812426\n",
      "train rmse 0.6968324956205354 test rmse 1.765551000074001\n",
      "train rmse 0.697091687303076 test rmse 1.7647405789274202\n",
      "train rmse 0.696671005293312 test rmse 1.7691729521793227\n",
      "train rmse 0.6956551382659902 test rmse 1.7670421483227203\n",
      "train rmse 0.6951231537938275 test rmse 1.7705303680952593\n",
      "train rmse 0.6954522367499728 test rmse 1.7734714454393383\n",
      "train rmse 0.6957907371465292 test rmse 1.7709565811650567\n",
      "train rmse 0.6953914254645392 test rmse 1.7760363790334557\n",
      "train rmse 0.6947404887401816 test rmse 1.776418189207366\n",
      "train rmse 0.6947184178537441 test rmse 1.775911059755833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.6950300693182057 test rmse 1.7808109880163232\n",
      "train rmse 0.6948422545547125 test rmse 1.7800409329454832\n",
      "train rmse 0.6943359804220495 test rmse 1.780634455818089\n",
      "train rmse 0.6942534793441012 test rmse 1.7848847765023979\n",
      "train rmse 0.6944355524341532 test rmse 1.784172605136121\n",
      "train rmse 0.6943005259050754 test rmse 1.7853317341435293\n",
      "train rmse 0.693957283223836 test rmse 1.7887840887269664\n",
      "train rmse 0.6938388080287773 test rmse 1.788131738281354\n",
      "train rmse 0.6939006787845026 test rmse 1.7899157892163424\n",
      "train rmse 0.6938337181028237 test rmse 1.7928133629179592\n",
      "train rmse 0.6936017969278688 test rmse 1.7920326997506792\n",
      "train rmse 0.6934317967376727 test rmse 1.794226842893723\n",
      "train rmse 0.6934347622143893 test rmse 1.7967257893848467\n",
      "train rmse 0.6934190536390396 test rmse 1.7961316768828237\n",
      "train rmse 0.6932378098206746 test rmse 1.7984759289358503\n",
      "train rmse 0.6930576796383395 test rmse 1.8003726255643158\n",
      "train rmse 0.6930163327366302 test rmse 1.8001755628798934\n",
      "train rmse 0.6929979053716119 test rmse 1.802767173658838\n",
      "train rmse 0.6928840404803669 test rmse 1.804010503704509\n",
      "train rmse 0.6927280614429036 test rmse 1.8041106784347003\n",
      "train rmse 0.6926332566919383 test rmse 1.80690836804039\n",
      "train rmse 0.6925884850636717 test rmse 1.8075763650382648\n",
      "train rmse 0.6925250339114611 test rmse 1.8082249967876145\n",
      "train rmse 0.6924177195972985 test rmse 1.8108779620964432\n",
      "train rmse 0.6922973881417244 test rmse 1.8109695945179964\n",
      "train rmse 0.6922059041627033 test rmse 1.812498487274591\n",
      "train rmse 0.6921485536868981 test rmse 1.8145621834656354\n",
      "train rmse 0.6920882487557904 test rmse 1.8145279555806966\n",
      "train rmse 0.6919963710571239 test rmse 1.8167644844784856\n",
      "train rmse 0.6918874240852115 test rmse 1.8178642738003283\n",
      "train rmse 0.6917953473962237 test rmse 1.81841234566374\n",
      "train rmse 0.6917269119391237 test rmse 1.8207328212880391\n",
      "train rmse 0.6916645235771468 test rmse 1.8211205780396644\n",
      "train rmse 0.691589826792936 test rmse 1.822543827257265\n",
      "train rmse 0.6915036795553421 test rmse 1.824227245237513\n",
      "train rmse 0.691413922455885 test rmse 1.8246072686260186\n",
      "train rmse 0.6913296069753211 test rmse 1.8265332848632956\n",
      "train rmse 0.6912524812043344 test rmse 1.8274638618830872\n",
      "train rmse 0.6911840997914629 test rmse 1.8284002895607863\n",
      "train rmse 0.6911169837118329 test rmse 1.8301878156327898\n",
      "train rmse 0.6910462169441821 test rmse 1.8307423561746128\n",
      "train rmse 0.6909694477219419 test rmse 1.8322501305980743\n",
      "train rmse 0.6908899308301736 test rmse 1.8334892669022371\n",
      "train rmse 0.6908109224785143 test rmse 1.834193013666516\n",
      "train rmse 0.690734234969954 test rmse 1.835970419730627\n",
      "train rmse 0.6906604947596224 test rmse 1.8365439853900087\n",
      "train rmse 0.6905888612707848 test rmse 1.837997903642092\n",
      "train rmse 0.6905185582914772 test rmse 1.8391542215737162\n",
      "train rmse 0.6904506221548821 test rmse 1.8399352356666672\n",
      "train rmse 0.6903842117894041 test rmse 1.8416032092638994\n",
      "train rmse 0.6903195650802066 test rmse 1.842106490744275\n",
      "train rmse 0.6902546316735466 test rmse 1.8437385558726864\n",
      "train rmse 0.6901932976702106 test rmse 1.8444939098593687\n",
      "train rmse 0.6901354776632815 test rmse 1.845734708001111\n",
      "train rmse 0.6900850809205143 test rmse 1.8468617834144339\n",
      "train rmse 0.6900476156612166 test rmse 1.84785541001393\n",
      "train rmse 0.6900296057002618 test rmse 1.8490250982847458\n",
      "train rmse 0.6900489761067983 test rmse 1.850294813945549\n",
      "train rmse 0.6901255886183849 test rmse 1.8509424515386967\n",
      "train rmse 0.6902928628196383 test rmse 1.8532492764825461\n",
      "train rmse 0.6905487475869901 test rmse 1.8524622280373038\n",
      "train rmse 0.6908312416236726 test rmse 1.856667671351729\n",
      "train rmse 0.6909005638060075 test rmse 1.8533491697023596\n",
      "train rmse 0.6905399434112601 test rmse 1.858783724080643\n",
      "train rmse 0.6898611012541511 test rmse 1.8558861192796785\n",
      "train rmse 0.6893617607291057 test rmse 1.8586578268299758\n",
      "train rmse 0.6893698234052484 test rmse 1.8609559206367186\n",
      "train rmse 0.6896851233029606 test rmse 1.8586060673103897\n",
      "train rmse 0.6898447280825013 test rmse 1.8638761518371352\n",
      "train rmse 0.6895925148605271 test rmse 1.8613792964790163\n",
      "train rmse 0.6891520331726881 test rmse 1.8633585340038876\n",
      "train rmse 0.6889536185536315 test rmse 1.8660367987255766\n",
      "train rmse 0.6890809564865085 test rmse 1.8638367534584355\n",
      "train rmse 0.6892335450204462 test rmse 1.8680414776781724\n",
      "train rmse 0.6891412867270565 test rmse 1.8673139074464344\n",
      "train rmse 0.6888642646591265 test rmse 1.8676724622111032\n",
      "train rmse 0.6886658536806813 test rmse 1.8708236594966001\n",
      "train rmse 0.6886654858392869 test rmse 1.8692777417950655\n",
      "train rmse 0.6887412786537374 test rmse 1.871928496702849\n",
      "train rmse 0.6887251601265967 test rmse 1.8727498858249005\n",
      "train rmse 0.6885815046696174 test rmse 1.8723304181793015\n",
      "train rmse 0.6884072776183103 test rmse 1.8750591904516358\n",
      "train rmse 0.6883184156656365 test rmse 1.874629238346948\n",
      "train rmse 0.6883224855989094 test rmse 1.8759846327219374\n",
      "train rmse 0.6883374878578643 test rmse 1.8775098532711483\n",
      "train rmse 0.6882883233909558 test rmse 1.8772665311923962\n",
      "train rmse 0.6881670316909385 test rmse 1.8790966421248658\n",
      "train rmse 0.6880447873477704 test rmse 1.8796421441926705\n",
      "train rmse 0.68797571889591 test rmse 1.880190911745072\n",
      "train rmse 0.687959409152594 test rmse 1.8819149299675169\n",
      "train rmse 0.6879494671671825 test rmse 1.8819836576848825\n",
      "train rmse 0.6879031559822542 test rmse 1.8832215304089492\n",
      "train rmse 0.6878207282233215 test rmse 1.8843555379846249\n",
      "train rmse 0.6877286487441923 test rmse 1.8843252349405968\n",
      "train rmse 0.6876528960052777 test rmse 1.8863823316301565\n",
      "train rmse 0.6876018405392031 test rmse 1.8861577244377785\n",
      "train rmse 0.6875662988426526 test rmse 1.887615867241522\n",
      "train rmse 0.6875327492629917 test rmse 1.8885728401799613\n",
      "train rmse 0.6874897479246586 test rmse 1.888578016124537\n",
      "train rmse 0.6874365127033379 test rmse 1.8907968742494872\n",
      "train rmse 0.6873727810801841 test rmse 1.8899748133439405\n",
      "train rmse 0.6873058998767148 test rmse 1.8923569619913918\n",
      "train rmse 0.6872400095655116 test rmse 1.8919952723341218\n",
      "train rmse 0.6871798159875304 test rmse 1.8934173770281613\n",
      "train rmse 0.687127359142976 test rmse 1.8942831289881576\n",
      "train rmse 0.6870810142332983 test rmse 1.8944284942131993\n",
      "train rmse 0.6870385485346675 test rmse 1.8963775133223522\n",
      "train rmse 0.6869980106463774 test rmse 1.8957153395379724\n",
      "train rmse 0.6869574486743812 test rmse 1.8981042891331175\n",
      "train rmse 0.6869173398562015 test rmse 1.8973192602834412\n",
      "train rmse 0.686877640883113 test rmse 1.899557089630245\n",
      "train rmse 0.6868404562666488 test rmse 1.8990715443701949\n",
      "train rmse 0.6868073919413357 test rmse 1.9009331519801318\n",
      "train rmse 0.6867814427077101 test rmse 1.90077956670333\n",
      "train rmse 0.6867672309644075 test rmse 1.9024479580953346\n",
      "train rmse 0.6867698129671134 test rmse 1.902279956539136\n",
      "train rmse 0.686795871123343 test rmse 1.904279722478841\n",
      "train rmse 0.6868541675147114 test rmse 1.903429352174993\n",
      "train rmse 0.6869533272704641 test rmse 1.906603233253669\n",
      "train rmse 0.6870812961724754 test rmse 1.9040745062475357\n",
      "train rmse 0.6872122986236443 test rmse 1.909344522136177\n",
      "train rmse 0.6872675676178522 test rmse 1.9043621658478642\n",
      "train rmse 0.6871627717471744 test rmse 1.9113450047074871\n",
      "train rmse 0.6868699828390726 test rmse 1.9058723701023068\n",
      "train rmse 0.6864814799742532 test rmse 1.9110803533575285\n",
      "train rmse 0.6861772817637342 test rmse 1.9098161577954402\n",
      "train rmse 0.6860833310626471 test rmse 1.9097072331004052\n",
      "train rmse 0.6861750449917725 test rmse 1.913796355759345\n",
      "train rmse 0.6863259130365185 test rmse 1.9097926255937363\n",
      "train rmse 0.6863919345153989 test rmse 1.9152810713669224\n",
      "train rmse 0.6863022253873914 test rmse 1.9124431377260345\n",
      "train rmse 0.686094755251484 test rmse 1.9143695195811128\n",
      "train rmse 0.6858858536587493 test rmse 1.9160805787442037\n",
      "train rmse 0.6857710469082919 test rmse 1.9138897876279086\n",
      "train rmse 0.6857619423711222 test rmse 1.918248733005033\n",
      "train rmse 0.6858030965249549 test rmse 1.915650374725204\n",
      "train rmse 0.685831863854442 test rmse 1.918399117659524\n",
      "train rmse 0.6858118528715721 test rmse 1.9187125261356452\n",
      "train rmse 0.6857393000147259 test rmse 1.918056882243519\n",
      "train rmse 0.6856298151951219 test rmse 1.9210261509479074\n",
      "train rmse 0.6855190304049094 test rmse 1.9189755658198844\n",
      "train rmse 0.6854316853749418 test rmse 1.921883869926019\n",
      "train rmse 0.6853807253434083 test rmse 1.9211991525149774\n",
      "train rmse 0.6853640059751754 test rmse 1.922074780508887\n",
      "train rmse 0.6853641581689234 test rmse 1.9235491400268152\n",
      "train rmse 0.6853610490613656 test rmse 1.9226422522886686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.685337088892192 test rmse 1.9250943272256638\n",
      "train rmse 0.6852927321032621 test rmse 1.9240206394595367\n",
      "train rmse 0.6852242778555624 test rmse 1.925791216160983\n",
      "train rmse 0.6851502491112947 test rmse 1.9259463968710566\n",
      "train rmse 0.6850769736556215 test rmse 1.9261932241896353\n",
      "train rmse 0.6850131530264654 test rmse 1.9278717146690092\n",
      "train rmse 0.6849613569316554 test rmse 1.9268370668610293\n",
      "train rmse 0.6849190861534341 test rmse 1.9294123279654174\n",
      "train rmse 0.6848836880804875 test rmse 1.9279229749196154\n",
      "train rmse 0.6848509426901134 test rmse 1.9305093845118584\n",
      "train rmse 0.6848210679549065 test rmse 1.9293850186764094\n",
      "train rmse 0.6847924757642138 test rmse 1.9313036988332712\n",
      "train rmse 0.684768756831004 test rmse 1.9310611052245876\n",
      "train rmse 0.684748758303076 test rmse 1.9319380019252899\n",
      "train rmse 0.6847388349864462 test rmse 1.932856317021932\n",
      "train rmse 0.6847424039151272 test rmse 1.9324539061621957\n",
      "train rmse 0.6847685174614355 test rmse 1.9348364949700703\n",
      "train rmse 0.6848250933957811 test rmse 1.9327560922321645\n",
      "train rmse 0.6849265049399019 test rmse 1.9372035999616735\n",
      "train rmse 0.6850786919884561 test rmse 1.9326976819115134\n",
      "train rmse 0.685275119052501 test rmse 1.9399468138374796\n",
      "train rmse 0.6854667290683204 test rmse 1.932565990076343\n",
      "train rmse 0.685542310389785 test rmse 1.9417241103007068\n",
      "train rmse 0.6853840300337172 test rmse 1.93405275387696\n",
      "train rmse 0.6849686664877284 test rmse 1.9404348479894893\n",
      "train rmse 0.6844843703018799 test rmse 1.938337329697473\n",
      "train rmse 0.6841894769254088 test rmse 1.9377281915970277\n",
      "train rmse 0.6841938327695902 test rmse 1.9426142351885518\n",
      "train rmse 0.6843875526775037 test rmse 1.9372386756139668\n",
      "train rmse 0.6845648706848393 test rmse 1.9437248768056803\n",
      "train rmse 0.6845687452616689 test rmse 1.9401167764616603\n",
      "train rmse 0.6843827843767697 test rmse 1.941754622643367\n",
      "train rmse 0.684128731788342 test rmse 1.9440692774316175\n",
      "train rmse 0.6839575317274313 test rmse 1.9403847169392583\n",
      "train rmse 0.6839247855693867 test rmse 1.9460360242443766\n",
      "train rmse 0.6839791655546325 test rmse 1.9420928661955725\n",
      "train rmse 0.6840350225444698 test rmse 1.9452931966168578\n",
      "train rmse 0.6840310578071283 test rmse 1.9456600506863768\n",
      "train rmse 0.6839598846844003 test rmse 1.943825885235876\n",
      "train rmse 0.683842793467173 test rmse 1.9482307452867316\n",
      "train rmse 0.6837321114161449 test rmse 1.944178300384801\n",
      "train rmse 0.6836551311728057 test rmse 1.9486105052064604\n",
      "train rmse 0.6836226538950773 test rmse 1.9466966354445292\n",
      "train rmse 0.6836232642205613 test rmse 1.9476950276259983\n",
      "train rmse 0.6836320266906571 test rmse 1.9496595232167402\n",
      "train rmse 0.6836270133508667 test rmse 1.9471693254264546\n",
      "train rmse 0.6835883004107557 test rmse 1.951407753373171\n",
      "train rmse 0.6835235123653972 test rmse 1.948043560715987\n",
      "train rmse 0.6834418863819438 test rmse 1.9516598508451413\n",
      "train rmse 0.6833662690002114 test rmse 1.9501067376879284\n",
      "train rmse 0.6833103137287059 test rmse 1.9512254556777615\n",
      "train rmse 0.6832762934751195 test rmse 1.9524073388535197\n",
      "train rmse 0.683260002407776 test rmse 1.9510439963529669\n",
      "train rmse 0.6832514314493425 test rmse 1.9541550018461264\n",
      "train rmse 0.6832408975287284 test rmse 1.951553872473455\n",
      "train rmse 0.6832216394444734 test rmse 1.9551428785090161\n",
      "train rmse 0.6831959029917968 test rmse 1.952711443032889\n",
      "train rmse 0.6831598048928413 test rmse 1.9555750624028148\n",
      "train rmse 0.6831198439268257 test rmse 1.9542241170066335\n",
      "train rmse 0.6830763248228174 test rmse 1.9557914537719974\n",
      "train rmse 0.6830329556591984 test rmse 1.9557795071546484\n",
      "train rmse 0.6829910455161428 test rmse 1.9560437175959244\n",
      "train rmse 0.6829514237760821 test rmse 1.9572066116191709\n",
      "train rmse 0.6829143963167608 test rmse 1.9564284806899872\n",
      "train rmse 0.6828793525703726 test rmse 1.9584871461673883\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_global_step()\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(cost, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(NB_EPOCHS):\n",
    "        _, train_pred, train_mse, reg, pen, train_cost = sess.run([train_op, pred, cost_l2, regularizer, penalty, cost], feed_dict={\n",
    "            user_batch: train['user'],\n",
    "            item_batch: train['item'],\n",
    "            rate_batch: train['rating']\n",
    "        })\n",
    "        test_pred, test_mse = sess.run([pred, cost_l2], feed_dict={\n",
    "            user_batch: test['user'],\n",
    "            item_batch: test['item'],\n",
    "            rate_batch: test['rating']\n",
    "        })\n",
    "        print('train rmse', train_mse ** 0.5, 'test rmse', test_mse ** 0.5)\n",
    "        # print('reg', reg, 'full cost', train_cost)\n",
    "        \n",
    "    #print(weights.eval(session=sess))\n",
    "    W_mat=weights.eval(session=sess)\n",
    "    alpha_vec= alpha.eval(session=sess)\n",
    "    beta_vec= beta.eval(session=sess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.066331    1.9173399  -1.864329   -2.9763763   1.0018115  -3.957121  ]\n",
      " [-0.05180632  1.7177396   3.3304186   2.580505    0.12303312  1.479994  ]\n",
      " [ 1.0941166   1.3709662  -0.09860156  3.6763618   1.7377232   1.5315199 ]\n",
      " ...\n",
      " [ 0.82717294 -0.38746685  0.5889197   3.9532263   1.343571   -1.6000885 ]\n",
      " [ 4.504394    2.0626862   1.8114731   3.2950892  -3.4091275   1.5208752 ]\n",
      " [-0.9323296   1.3489004   1.5469805   3.4738061   2.5705705   2.968796  ]]\n",
      "[ 4.196883   6.4330873  1.8368745  2.8925083  1.9433355 -1.2747475]\n",
      "[0.8099757  0.78845453 0.52409124 0.4283868  0.06774634 0.42456144]\n"
     ]
    }
   ],
   "source": [
    "print(W_mat)\n",
    "print(alpha_vec)\n",
    "print(beta_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(W_mat)\n",
    "\n",
    "# add some prints/visualisation\n",
    "# check encoding B\n",
    "# positivity on W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO => comparer avec SVD++ / baseline constante (predire la moyenne des ratins)\n",
    "# pourquoi le modele bilineaire n'était pas convaicant ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
