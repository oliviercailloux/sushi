{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USERS = 5000\n",
    "FEAT_USER = 3\n",
    "NB_ITEMS = 100\n",
    "FEAT_ITEM = 6\n",
    "\n",
    "NB_EPOCHS = 500\n",
    "LAMBDA_REG = 1e-5\n",
    "# LAMBDA_REG = 0\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv('data/sushi/sushi3.udata', sep='\\t', names=('uid', 'gender', 'age', 'time', 'old_prefecture', 'old_region', 'old_eastwest', 'prefecture', 'region', 'eastwest', 'same'))\n",
    "items = pd.read_csv('data/sushi/sushi3.idata', sep='\\t', names=('iid', 'name', 'style', 'major', 'minor', 'heaviness', 'frequency', 'price', 'popularity'))\n",
    "R = pd.read_csv('data/sushi/sushi3b.5000.10.score', sep=' ', header=None)\n",
    "triplets = []\n",
    "for i, line in enumerate(np.array(R)):\n",
    "    for j, v in enumerate(line):\n",
    "        if v != -1:\n",
    "            triplets.append((i, j, v))\n",
    "df_ratings = pd.DataFrame(triplets, columns=('user', 'item', 'rating'))\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00362115 0.01561563 0.01875961 0.03046583 0.04244708 0.04685056\n",
      " 0.05233778 0.05412931 0.05860949 0.0604048  0.09480204 0.1190487\n",
      " 0.12366599 0.12415152 0.13614209 0.15213501 0.15671926 0.16063328\n",
      " 0.17949392 0.18410167 0.20667117 0.22036911 0.22439779 0.23672109\n",
      " 0.23690673 0.24537075 0.25861549 0.26034515 0.2779432  0.30225799\n",
      " 0.30840559 0.32137762 0.32210075 0.3502405  0.35502576 0.3590272\n",
      " 0.36765885 0.37326133 0.37624559 0.38408362 0.41747215 0.4194466\n",
      " 0.42304502 0.44193035 0.44250565 0.44993519 0.45516777 0.45555315\n",
      " 0.45622708 0.46155425 0.46513951 0.48251872 0.49080922 0.49627894\n",
      " 0.52137999 0.53303965 0.54424145 0.56056934 0.56137326 0.57358131\n",
      " 0.61193354 0.61207602 0.61795984 0.62092634 0.6244583  0.64870045\n",
      " 0.65064617 0.66296341 0.66962684 0.68026111 0.69655638 0.69798901\n",
      " 0.71444429 0.72175464 0.74032352 0.7568309  0.77519026 0.78686381\n",
      " 0.79505145 0.79863572 0.80291447 0.81557261 0.81636676 0.82908445\n",
      " 0.85817038 0.85960862 0.87195639 0.87674413 0.88536695 0.89747465\n",
      " 0.9333028  0.94923268 0.95167468 0.96223653 0.96241307 0.96445266\n",
      " 0.96538998 0.96997398 0.97283603 0.99835452]\n",
      "[0.00499884 0.00617855 0.00682053 0.0093719  0.00979123 0.01138281\n",
      " 0.01913323 0.02093291 0.02317866 0.02340794 0.02907516 0.03086808\n",
      " 0.03253772 0.04519142 0.04597403 0.04669639 0.04885295 0.05549969\n",
      " 0.06124624 0.06288295 0.06902519 0.07062609 0.07806323 0.08068086\n",
      " 0.08530126 0.0915739  0.94342929 0.99975075 1.0032108  1.01588764\n",
      " 1.0248577  1.0577744  1.06683951 1.0896423  1.11524294 1.1402968\n",
      " 1.14910597 1.159565   1.16465016 1.18377125 1.19432476 1.23750838\n",
      " 1.24665932 1.25719243 1.27685009 1.2770484  1.27992504 1.28667284\n",
      " 1.29296706 1.32105143 1.3954834  1.39771194 1.41358101 1.44995409\n",
      " 1.50774247 1.55869047 1.56008101 1.57031925 1.62756818 1.64607289\n",
      " 1.65624622 1.6643244  1.05375512 1.05959263 1.06631589 1.0671339\n",
      " 1.0682059  1.06854797 1.06876971 1.07251213 1.08354834 1.08518308\n",
      " 1.08550395 1.08580653 1.09024897 1.09193449 1.09281031 1.09817108\n",
      " 1.09850165 1.10669993 1.10907979 1.11036461 1.1144289  1.11469184\n",
      " 1.11754274 1.11838338 1.12046521 1.12114163 1.1227464  1.12349149\n",
      " 1.12542926 1.12900911 1.13435364 1.13579717 1.13891834 1.14048433\n",
      " 1.14068682 1.14694206 1.14809792 1.14907226]\n",
      "0.03333333333333334\n",
      "0.6333333333333334\n",
      "0.9075\n",
      "0.975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f30e8127518>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Synthetic data\n",
    "# x in (0,1)\n",
    "# y in (0,1)\n",
    "# preciser x0, x1, y0 et y1 => les parametres de la fonction s'en d√©duisent\n",
    "# piecewise linear function (used in the partial value function )\n",
    "def piecewise_linear(x, x0, x1, y0, y1):\n",
    "    condlist = [x < x0, (x >= x0) & (x < x1), x >= x1]\n",
    "    funclist = [lambda x: (y0/x0)*x , lambda x: ((y1-y0)/(x1-x0))*(x-x0) + y0, lambda x: ((1-y1)/(1-x1))*(x-x1) + y1]\n",
    "    return np.piecewise(x, condlist, funclist)\n",
    "\n",
    "\n",
    "\n",
    "X_0 = 0.3\n",
    "X_1 = 0.6\n",
    "Y_0 = 0.1\n",
    "Y_1 = 0.9\n",
    "\n",
    "\n",
    "x = np.random.uniform(low=0.0, high=1.0, size=(100))\n",
    "x.sort()\n",
    "#print(x)\n",
    "#print(y)\n",
    "y = piecewise_linear(x, X_0, X_1, Y_0, Y_1)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.plot(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-4-17292e11f245>\", line 6, in <module>\n    weights = tf.get_variable('W', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n  File \"/home/flo/anaconda3/envs/clut/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/flo/anaconda3/envs/clut/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0a90ca18d78e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'major'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'heaviness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frequency'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'popularity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNB_USERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEAT_ITEM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m alpha = tf.get_variable(\"alpha\", shape=[FEAT_ITEM],\n",
      "\u001b[0;32m~/anaconda3/envs/clut/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/envs/clut/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/envs/clut/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/envs/clut/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clut/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-4-17292e11f245>\", line 6, in <module>\n    weights = tf.get_variable('W', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n  File \"/home/flo/anaconda3/envs/clut/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/flo/anaconda3/envs/clut/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# TF\n",
    "#A is not used (only the \"criterion on the items\" => only B)\n",
    "#A = tf.constant(np.array(users[['age', 'gender', 'region']]).astype(np.float32)) \n",
    "B = tf.constant(np.array(items[['style', 'major','heaviness', 'frequency', 'price', 'popularity']]).astype(np.float32))\n",
    "\n",
    "weights = tf.get_variable('W', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "alpha = tf.get_variable(\"alpha\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "beta = tf.get_variable(\"beta\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "user_bias = tf.get_variable(\"user_bias\", shape=[NB_USERS],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "#item_bias = tf.get_variable(\"item_bias\", shape=[NB_ITEMS],\n",
    "#                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "#item_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "bias_users = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "bias_items = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "weight_users = tf.nn.embedding_lookup(weights, user_batch)\n",
    "\n",
    "#beta_crit = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "#alpha_crit = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "\n",
    "feat_items = tf.nn.embedding_lookup(B, item_batch)\n",
    "#feat_users = tf.nn.embedding_lookup(A, user_batch)\n",
    "\n",
    "pred =(tf.reduce_sum(tf.multiply(tf.nn.softmax(tf.multiply(feat_items, alpha)+beta),weight_users),1) + bias_user)\n",
    "       #s+bias_items)\n",
    "\n",
    "cost_l2 = tf.losses.mean_squared_error(rate_batch, pred)\n",
    "\n",
    "regularizer=tf.nn.l2_loss(weight_users)\n",
    "\n",
    "#l2_user = tf.nn.l2_loss(weight_users)\n",
    "#l2_item = tf.nn.l2_loss(weight_items)\n",
    "#l2_bias_user = tf.nn.l2_loss(bias_users)\n",
    "#l2_bias_item = tf.nn.l2_loss(bias_items)\n",
    "#regularizer = tf.add(l2_user, l2_item)\n",
    "#regularizer = tf.add(regularizer, l2_bias_user)\n",
    "#regularizer = tf.add(regularizer, l2_bias_item)\n",
    "# regularizer = tf.nn.l2_loss(M)\n",
    "penalty = tf.constant(LAMBDA_REG, dtype=tf.float32, shape=[])\n",
    "cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 3.230941645594412 test rmse 3.005104490675479\n",
      "train rmse 2.9894619871671364 test rmse 2.7804583162099648\n",
      "train rmse 2.762241772626559 test rmse 2.5725328310992923\n",
      "train rmse 2.5509448152358014 test rmse 2.3820202119996288\n",
      "train rmse 2.3557885808464403 test rmse 2.2100437905646526\n",
      "train rmse 2.177586462360544 test rmse 2.0580432018123727\n",
      "train rmse 2.017762699159549 test rmse 1.9267965429853517\n",
      "train rmse 1.8772513542663185 test rmse 1.8159325422916146\n",
      "train rmse 1.7559381282742135 test rmse 1.7240858309371823\n",
      "train rmse 1.6527956802224932 test rmse 1.6492148641001116\n",
      "train rmse 1.5661528803374316 test rmse 1.5887879561145435\n",
      "train rmse 1.4938373332649795 test rmse 1.5400098025022368\n",
      "train rmse 1.4333698815114406 test rmse 1.5001303298279542\n",
      "train rmse 1.3822945524567427 test rmse 1.4667247038253373\n",
      "train rmse 1.338435865245803 test rmse 1.4378475929957228\n",
      "train rmse 1.3000549946670066 test rmse 1.4120696973700388\n",
      "train rmse 1.2658684578786128 test rmse 1.3884400765796274\n",
      "train rmse 1.235019032953297 test rmse 1.366410519662493\n",
      "train rmse 1.2069954974857726 test rmse 1.3457354581849055\n",
      "train rmse 1.1815377687187982 test rmse 1.3263859302205923\n",
      "train rmse 1.1585541011639429 test rmse 1.3084559581576185\n",
      "train rmse 1.1380364295478538 test rmse 1.2920799773384968\n",
      "train rmse 1.119991491660089 test rmse 1.2773670812686688\n",
      "train rmse 1.1043840890023566 test rmse 1.2643586876232247\n",
      "train rmse 1.0910997160981026 test rmse 1.253009035447797\n",
      "train rmse 1.0799299417837585 test rmse 1.2431942201179313\n",
      "train rmse 1.0705836711328836 test rmse 1.2347315985418135\n",
      "train rmse 1.0627207807150814 test rmse 1.2274097990624788\n",
      "train rmse 1.0559883854581325 test rmse 1.2210089948824296\n",
      "train rmse 1.0500545328602156 test rmse 1.2153237918438058\n",
      "train rmse 1.0446253846119766 test rmse 1.210173581259676\n",
      "train rmse 1.0394739469315657 test rmse 1.2054180240898167\n",
      "train rmse 1.0344329034093434 test rmse 1.200958034430155\n",
      "train rmse 1.029402436085915 test rmse 1.1967375992673766\n",
      "train rmse 1.0243335324936969 test rmse 1.192734824917809\n",
      "train rmse 1.0192169055632552 test rmse 1.1889574743900213\n",
      "train rmse 1.014068584688385 test rmse 1.1854257537457746\n",
      "train rmse 1.0089149655028722 test rmse 1.1821679822221642\n",
      "train rmse 1.003786666219425 test rmse 1.179211450991162\n",
      "train rmse 0.9987116799863003 test rmse 1.176579404104112\n",
      "train rmse 0.9937139468580668 test rmse 1.1742894540338695\n",
      "train rmse 0.9888154949289717 test rmse 1.1723459441886297\n",
      "train rmse 0.9840328893631186 test rmse 1.170748970730199\n",
      "train rmse 0.9793820685330588 test rmse 1.1694874665468182\n",
      "train rmse 0.9748749457147874 test rmse 1.1685432883664146\n",
      "train rmse 0.9705219061264415 test rmse 1.1678924025266173\n",
      "train rmse 0.966333482449737 test rmse 1.167504719187574\n",
      "train rmse 0.9623174915425826 test rmse 1.1673472099880962\n",
      "train rmse 0.9584782152262319 test rmse 1.1673812153935976\n",
      "train rmse 0.9548164430439361 test rmse 1.167569146324392\n",
      "train rmse 0.9513326270150511 test rmse 1.1678744376898882\n",
      "train rmse 0.9480224731976585 test rmse 1.168263273836788\n",
      "train rmse 0.9448816562907144 test rmse 1.1687060424371691\n",
      "train rmse 0.941905320710381 test rmse 1.1691769885810452\n",
      "train rmse 0.9390897940479082 test rmse 1.1696528410407456\n",
      "train rmse 0.9364285386168537 test rmse 1.1701153068291774\n",
      "train rmse 0.9339172249797415 test rmse 1.1705502970047863\n",
      "train rmse 0.9315493556966514 test rmse 1.1709488324160866\n",
      "train rmse 0.9293187796667691 test rmse 1.1713076426063007\n",
      "train rmse 0.9272168452675394 test rmse 1.1716275780471477\n",
      "train rmse 0.9252335099011131 test rmse 1.1719122308538943\n",
      "train rmse 0.9233593386443432 test rmse 1.1721686948866004\n",
      "train rmse 0.921582159309342 test rmse 1.1724036995030591\n",
      "train rmse 0.9198898666223461 test rmse 1.1726243738445825\n",
      "train rmse 0.9182747513477979 test rmse 1.1728400262397098\n",
      "train rmse 0.9167276961789133 test rmse 1.1730586876720621\n",
      "train rmse 0.9152427291977272 test rmse 1.1732912279741865\n",
      "train rmse 0.9138165330473668 test rmse 1.1735465272116212\n",
      "train rmse 0.9124472929481721 test rmse 1.1738316794546328\n",
      "train rmse 0.9111358544400084 test rmse 1.1741475259444953\n",
      "train rmse 0.9098829260123343 test rmse 1.1744915552042743\n",
      "train rmse 0.908690455137449 test rmse 1.174856538333891\n",
      "train rmse 0.9075576293133689 test rmse 1.1752343410803088\n",
      "train rmse 0.9064860529786669 test rmse 1.1756169910959173\n",
      "train rmse 0.9054733763371688 test rmse 1.1759965262296843\n",
      "train rmse 0.9045177873725535 test rmse 1.176365501254395\n",
      "train rmse 0.9036178511245623 test rmse 1.1767201271195966\n",
      "train rmse 0.9027700366805008 test rmse 1.1770577835865978\n",
      "train rmse 0.901973136326314 test rmse 1.1773797508765165\n",
      "train rmse 0.9012237798445341 test rmse 1.1776865985852538\n",
      "train rmse 0.9005198024010627 test rmse 1.1779826394438828\n",
      "train rmse 0.8998590252986169 test rmse 1.1782711191403257\n",
      "train rmse 0.8992373016326235 test rmse 1.1785545719477049\n",
      "train rmse 0.8986533527642615 test rmse 1.1788349228624453\n",
      "train rmse 0.8981045976948195 test rmse 1.1791140444620771\n",
      "train rmse 0.897589042411906 test rmse 1.179393706463962\n",
      "train rmse 0.8971044851580274 test rmse 1.1796757274318161\n",
      "train rmse 0.8966498461165499 test rmse 1.1799632880593818\n",
      "train rmse 0.8962236744247816 test rmse 1.1802552227639203\n",
      "train rmse 0.8958234823409795 test rmse 1.1805510739010783\n",
      "train rmse 0.8954496042351144 test rmse 1.1808492232935905\n",
      "train rmse 0.8950997757305487 test rmse 1.1811488617888684\n",
      "train rmse 0.8947737252713015 test rmse 1.1814489792451417\n",
      "train rmse 0.8944689133425419 test rmse 1.1817466499114886\n",
      "train rmse 0.894184295134488 test rmse 1.1820410688336611\n",
      "train rmse 0.8939185230390782 test rmse 1.182329213670713\n",
      "train rmse 0.8936698797633704 test rmse 1.1826091737735556\n",
      "train rmse 0.8934376123994674 test rmse 1.1828788385974947\n",
      "train rmse 0.8932195316310343 test rmse 1.183135746636529\n",
      "train rmse 0.8930140125888242 test rmse 1.1833798054627478\n",
      "train rmse 0.8928208302879322 test rmse 1.1836106201581085\n",
      "train rmse 0.892637855985292 test rmse 1.1838269900911458\n",
      "train rmse 0.8924645616670919 test rmse 1.184029778969504\n",
      "train rmse 0.8922996169959331 test rmse 1.1842192454323826\n",
      "train rmse 0.892142191469646 test rmse 1.1843977611438135\n",
      "train rmse 0.8919911196848251 test rmse 1.1845662870912643\n",
      "train rmse 0.8918464048702927 test rmse 1.1847257834457134\n",
      "train rmse 0.8917076156399663 test rmse 1.1848780648153336\n",
      "train rmse 0.89157338426901 test rmse 1.1850243411401065\n",
      "train rmse 0.8914435456587188 test rmse 1.1851649666895017\n",
      "train rmse 0.8913175333125842 test rmse 1.1853024075131502\n",
      "train rmse 0.8911951147677522 test rmse 1.1854375697719313\n",
      "train rmse 0.891075221254854 test rmse 1.1855710072708636\n",
      "train rmse 0.8909594259303701 test rmse 1.185704278944733\n",
      "train rmse 0.8908450540771318 test rmse 1.1858375859044685\n",
      "train rmse 0.8907341806515017 test rmse 1.185971480977136\n",
      "train rmse 0.8906249330754841 test rmse 1.1861067680013346\n",
      "train rmse 0.8905169772843553 test rmse 1.1862422908295296\n",
      "train rmse 0.8904129578997058 test rmse 1.1863784513076647\n",
      "train rmse 0.8903088928877979 test rmse 1.1865162036816528\n",
      "train rmse 0.8902074939507484 test rmse 1.186655095335089\n",
      "train rmse 0.8901085611096887 test rmse 1.186795125867981\n",
      "train rmse 0.8900103204643132 test rmse 1.1869361944427952\n",
      "train rmse 0.8899147815838578 test rmse 1.1870774973097336\n",
      "train rmse 0.8898210075493128 test rmse 1.1872184821271101\n",
      "train rmse 0.8897287309510231 test rmse 1.1873585466159942\n",
      "train rmse 0.8896379522549478 test rmse 1.1874960849095104\n",
      "train rmse 0.8895491409579783 test rmse 1.1876314993880144\n",
      "train rmse 0.8894620631074188 test rmse 1.1877626329415398\n",
      "train rmse 0.889376786230871 test rmse 1.1878895873416784\n",
      "train rmse 0.8892917022514423 test rmse 1.1880126147868806\n",
      "train rmse 0.8892086210664344 test rmse 1.1881319673304824\n",
      "train rmse 0.8891270739765667 test rmse 1.1882472447853563\n",
      "train rmse 0.88904706140398 test rmse 1.1883595517936878\n",
      "train rmse 0.8889670751544279 test rmse 1.1884696414843159\n",
      "train rmse 0.888889126893515 test rmse 1.188576862550722\n",
      "train rmse 0.8888125800783274 test rmse 1.188681717204134\n",
      "train rmse 0.8887370997376579 test rmse 1.18878400551406\n",
      "train rmse 0.8886627196794022 test rmse 1.188884329762338\n",
      "train rmse 0.8885894066409485 test rmse 1.1889817379606975\n",
      "train rmse 0.8885170602614968 test rmse 1.1890782860248166\n",
      "train rmse 0.8884467877396642 test rmse 1.1891727712180928\n",
      "train rmse 0.888377247691716 test rmse 1.1892656451020756\n",
      "train rmse 0.8883094132252415 test rmse 1.1893568579392135\n",
      "train rmse 0.8882422110669407 test rmse 1.1894457586652287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.8881764131143559 test rmse 1.1895334501658268\n",
      "train rmse 0.8881116505531648 test rmse 1.1896185799012902\n",
      "train rmse 0.8880475208974465 test rmse 1.1897023007329193\n",
      "train rmse 0.8879851653840328 test rmse 1.18978491354012\n",
      "train rmse 0.887923577465395 test rmse 1.1898646652777936\n",
      "train rmse 0.8878633279292334 test rmse 1.1899444617606698\n",
      "train rmse 0.8878031414415536 test rmse 1.1900220991534964\n",
      "train rmse 0.887744730128871 test rmse 1.1900993808953397\n",
      "train rmse 0.8876872214440634 test rmse 1.190175004961326\n",
      "train rmse 0.8876297090333229 test rmse 1.1902504239128655\n",
      "train rmse 0.8875741068028256 test rmse 1.1903242857838545\n",
      "train rmse 0.8875184675095165 test rmse 1.190397592288705\n",
      "train rmse 0.8874637314276324 test rmse 1.1904702934618627\n",
      "train rmse 0.8874104360599994 test rmse 1.190541888762941\n",
      "train rmse 0.8873575405171678 test rmse 1.1906126787644484\n",
      "train rmse 0.8873050112831823 test rmse 1.1906821630177182\n",
      "train rmse 0.8872531171385482 test rmse 1.1907509424276745\n",
      "train rmse 0.8872018581947082 test rmse 1.1908194675977506\n",
      "train rmse 0.8871513353416095 test rmse 1.1908865874074985\n",
      "train rmse 0.8871015151098542 test rmse 1.1909533530996663\n",
      "train rmse 0.8870515912888397 test rmse 1.1910189139676373\n",
      "train rmse 0.887003311004824 test rmse 1.1910839207611597\n",
      "train rmse 0.886955532103301 test rmse 1.1911487238484306\n",
      "train rmse 0.8869068097572177 test rmse 1.1912118721911555\n",
      "train rmse 0.8868599665781001 test rmse 1.1912746169117574\n",
      "train rmse 0.8868123479842068 test rmse 1.1913372082326155\n",
      "train rmse 0.8867663400121615 test rmse 1.1913989457703864\n",
      "train rmse 0.8867201279952983 test rmse 1.1914600797905983\n",
      "train rmse 0.8866739807926709 test rmse 1.191520760458182\n",
      "train rmse 0.8866286042894594 test rmse 1.1915812379497719\n",
      "train rmse 0.8865843011369313 test rmse 1.1916404618981093\n",
      "train rmse 0.8865395923730139 test rmse 1.191699382804166\n",
      "train rmse 0.8864956881896454 test rmse 1.1917586008813439\n",
      "train rmse 0.8864524542273273 test rmse 1.1918163156689876\n",
      "train rmse 0.8864089491850319 test rmse 1.191873777615941\n",
      "train rmse 0.8863665179448093 test rmse 1.1919304866907483\n",
      "train rmse 0.8863239501747204 test rmse 1.191987043054297\n",
      "train rmse 0.8862815821176153 test rmse 1.1920433467241702\n",
      "train rmse 0.8862401872422536 test rmse 1.192098247741891\n",
      "train rmse 0.8861985886569655 test rmse 1.1921527462514598\n",
      "train rmse 0.8861579297856924 test rmse 1.1922068423080752\n",
      "train rmse 0.8861178408008112 test rmse 1.1922602860016436\n",
      "train rmse 0.886077380028306 test rmse 1.1923133773644512\n",
      "train rmse 0.8860371528568516 test rmse 1.1923659164893727\n",
      "train rmse 0.8859981684289089 test rmse 1.1924173535991875\n",
      "train rmse 0.8859592832009949 test rmse 1.192468938442971\n",
      "train rmse 0.8859204635462203 test rmse 1.192519221520808\n",
      "train rmse 0.8858818776803105 test rmse 1.1925702521785564\n",
      "train rmse 0.8858436938481712 test rmse 1.1926197313384852\n",
      "train rmse 0.8858056093030157 test rmse 1.192669908107264\n",
      "train rmse 0.8857687343669413 test rmse 1.1927191332643228\n",
      "train rmse 0.8857319924841364 test rmse 1.1927680565597267\n",
      "train rmse 0.8856943405670009 test rmse 1.1928168279395917\n",
      "train rmse 0.885657629247874 test rmse 1.1928649477462678\n",
      "train rmse 0.8856225653206385 test rmse 1.1929125659555475\n",
      "train rmse 0.8855862212053388 test rmse 1.1929603321550382\n",
      "train rmse 0.8855502457926858 test rmse 1.1930071971321285\n",
      "train rmse 0.885515446856117 test rmse 1.1930536106307421\n",
      "train rmse 0.8854807138652881 test rmse 1.1930996726191891\n",
      "train rmse 0.8854462824337652 test rmse 1.1931452332704877\n",
      "train rmse 0.8854115130704605 test rmse 1.1931906922741133\n",
      "train rmse 0.8853780551046064 test rmse 1.19323515050357\n",
      "train rmse 0.8853435523568919 test rmse 1.1932799067782527\n",
      "train rmse 0.8853103611277641 test rmse 1.1933238621998306\n",
      "train rmse 0.8852766636881105 test rmse 1.1933676661626358\n",
      "train rmse 0.8852437056107865 test rmse 1.1934113686281076\n",
      "train rmse 0.8852112849769812 test rmse 1.1934546699497446\n",
      "train rmse 0.8851784254698583 test rmse 1.193497919759228\n",
      "train rmse 0.8851464738162862 test rmse 1.1935401692144947\n",
      "train rmse 0.8851146893621965 test rmse 1.193583016425334\n",
      "train rmse 0.8850822976743257 test rmse 1.1936246636386516\n",
      "train rmse 0.885050746626604 test rmse 1.1936662095307007\n",
      "train rmse 0.8850189587345805 test rmse 1.1937076041796955\n",
      "train rmse 0.8849886514183914 test rmse 1.1937487477400837\n",
      "train rmse 0.8849571980616481 test rmse 1.1937898898824735\n",
      "train rmse 0.8849263497865282 test rmse 1.1938297324990148\n",
      "train rmse 0.8848961403352064 test rmse 1.1938703725951003\n",
      "train rmse 0.8848659635325717 test rmse 1.1939101626019168\n",
      "train rmse 0.8848362235561391 test rmse 1.19394935221576\n",
      "train rmse 0.8848060110276864 test rmse 1.1939887901463642\n",
      "train rmse 0.8847767742877642 test rmse 1.1940276776656973\n",
      "train rmse 0.8847469639441748 test rmse 1.1940666138359233\n",
      "train rmse 0.8847180621097422 test rmse 1.194105299157905\n",
      "train rmse 0.8846888898368975 test rmse 1.1941431846004729\n",
      "train rmse 0.8846603566718233 test rmse 1.1941810189285536\n",
      "train rmse 0.8846315193857778 test rmse 1.1942189019689846\n",
      "train rmse 0.8846033212706771 test rmse 1.194256534260679\n",
      "train rmse 0.8845748864188505 test rmse 1.1942935664723207\n",
      "train rmse 0.8845472255733787 test rmse 1.1943307472546667\n",
      "train rmse 0.8845191932372687 test rmse 1.194367228212727\n",
      "train rmse 0.884492137147275 test rmse 1.1944037579598046\n",
      "train rmse 0.8844651476202378 test rmse 1.1944399871792293\n",
      "train rmse 0.8844377866098504 test rmse 1.194476115499323\n",
      "train rmse 0.8844107617268242 test rmse 1.1945115940430309\n",
      "train rmse 0.8843838708115898 test rmse 1.1945474208138913\n",
      "train rmse 0.8843567094832275 test rmse 1.1945827475525177\n",
      "train rmse 0.8843304235324468 test rmse 1.1946174745146034\n",
      "train rmse 0.8843038671887588 test rmse 1.1946524000387455\n",
      "train rmse 0.8842777481794354 test rmse 1.1946868256275889\n",
      "train rmse 0.884252032839806 test rmse 1.1947213998944624\n",
      "train rmse 0.8842260808214448 test rmse 1.1947553246094693\n",
      "train rmse 0.8842002965683732 test rmse 1.194789547684156\n",
      "train rmse 0.8841745789761406 test rmse 1.1948233208070287\n",
      "train rmse 0.884149534782758 test rmse 1.1948567437849338\n",
      "train rmse 0.8841240179638106 test rmse 1.1948902655938383\n",
      "train rmse 0.8840991745937063 test rmse 1.1949228387126047\n",
      "train rmse 0.8840743305254788 test rmse 1.1949556104643488\n",
      "train rmse 0.8840496206036476 test rmse 1.194988580832728\n",
      "train rmse 0.8840254493843201 test rmse 1.1950211512716813\n",
      "train rmse 0.8840007043831379 test rmse 1.195053271937768\n",
      "train rmse 0.883976835251571 test rmse 1.1950854914901339\n",
      "train rmse 0.8839527631864801 test rmse 1.195117859794227\n",
      "train rmse 0.8839289601918257 test rmse 1.1951492297793864\n",
      "train rmse 0.8839046845229206 test rmse 1.1951809480369133\n",
      "train rmse 0.8838809476686562 test rmse 1.1952119672796602\n",
      "train rmse 0.8838573787693034 test rmse 1.1952430355856107\n",
      "train rmse 0.8838341127157566 test rmse 1.1952735046807577\n",
      "train rmse 0.883810542567439 test rmse 1.1953049204467612\n",
      "train rmse 0.8837877810980006 test rmse 1.1953353381000933\n",
      "train rmse 0.8837645132122202 test rmse 1.195366004294893\n",
      "train rmse 0.8837422564004058 test rmse 1.195396869150229\n",
      "train rmse 0.883719290829723 test rmse 1.1954264866950888\n",
      "train rmse 0.8836971003281194 test rmse 1.1954562530841624\n",
      "train rmse 0.8836743696621215 test rmse 1.195486118448251\n",
      "train rmse 0.8836518070430192 test rmse 1.19551583349575\n",
      "train rmse 0.8836298509368299 test rmse 1.1955448996816667\n",
      "train rmse 0.883607826829078 test rmse 1.1955741147241867\n",
      "train rmse 0.8835859033588652 test rmse 1.195603129640033\n",
      "train rmse 0.8835639456150036 test rmse 1.1956319444437749\n",
      "train rmse 0.8835425607439438 test rmse 1.1956605092990713\n",
      "train rmse 0.8835202983409118 test rmse 1.1956895719680434\n",
      "train rmse 0.8834995195919646 test rmse 1.1957178363561773\n",
      "train rmse 0.8834779982296478 test rmse 1.195746100076213\n",
      "train rmse 0.8834569148820337 test rmse 1.1957745625124356\n",
      "train rmse 0.8834359659695067 test rmse 1.1958023264428308\n",
      "train rmse 0.8834143418527766 test rmse 1.1958302392598352\n",
      "train rmse 0.883393661820101 test rmse 1.1958579022124391\n",
      "train rmse 0.8833724077747269 test rmse 1.1958850162693113\n",
      "train rmse 0.8833518954492003 test rmse 1.1959125782742537\n",
      "train rmse 0.8833312814316819 test rmse 1.1959398904481975\n",
      "train rmse 0.8833111392837784 test rmse 1.1959670026462947\n",
      "train rmse 0.8832916039980689 test rmse 1.1959941142297807\n",
      "train rmse 0.8832708536100037 test rmse 1.196020627169247\n",
      "train rmse 0.8832507775673357 test rmse 1.1960471893556914\n",
      "train rmse 0.8832306335835348 test rmse 1.1960737509522479\n",
      "train rmse 0.8832106241129691 test rmse 1.1960999631315452\n",
      "train rmse 0.8831907491647814 test rmse 1.1961263242306208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.8831711774714334 test rmse 1.196152335936607\n",
      "train rmse 0.8831517065806094 test rmse 1.1961785463938506\n",
      "train rmse 0.8831317965603012 test rmse 1.1962040586827798\n",
      "train rmse 0.8831127972579814 test rmse 1.196229570427602\n",
      "train rmse 0.8830934263228979 test rmse 1.196255530062562\n",
      "train rmse 0.8830743586983021 test rmse 1.1962806421100338\n",
      "train rmse 0.8830552906619831 test rmse 1.1963065009889697\n",
      "train rmse 0.8830361884640836 test rmse 1.196331611966517\n",
      "train rmse 0.8830174571090461 test rmse 1.1963567224169966\n",
      "train rmse 0.8829990291179707 test rmse 1.1963816828781706\n",
      "train rmse 0.8829803644785755 test rmse 1.1964063937199687\n",
      "train rmse 0.8829610918962122 test rmse 1.1964312036887943\n",
      "train rmse 0.8829426289753254 test rmse 1.1964553156956426\n",
      "train rmse 0.8829241994224712 test rmse 1.1964799253833713\n",
      "train rmse 0.8829058707493674 test rmse 1.1965040862240421\n",
      "train rmse 0.8828876429622968 test rmse 1.1965281967621877\n",
      "train rmse 0.8828693135302682 test rmse 1.196552555882759\n",
      "train rmse 0.8828513888005338 test rmse 1.1965760178797422\n",
      "train rmse 0.8828332949189397 test rmse 1.1965996786634525\n",
      "train rmse 0.882814863083639 test rmse 1.1966243850035378\n",
      "train rmse 0.8827975786686658 test rmse 1.1966480448309067\n",
      "train rmse 0.8827796862420024 test rmse 1.1966710566773997\n",
      "train rmse 0.8827616584113848 test rmse 1.1966945163510956\n",
      "train rmse 0.8827449468912606 test rmse 1.1967177763378478\n",
      "train rmse 0.8827273910142198 test rmse 1.1967410856783152\n",
      "train rmse 0.8827095309271816 test rmse 1.1967639961260976\n",
      "train rmse 0.8826924132645436 test rmse 1.1967871053508463\n",
      "train rmse 0.8826755316153247 test rmse 1.1968094172823742\n",
      "train rmse 0.8826586834074954 test rmse 1.1968322268181077\n",
      "train rmse 0.8826410920497217 test rmse 1.1968549363170233\n",
      "train rmse 0.8826237704661911 test rmse 1.1968770975836558\n",
      "train rmse 0.882606718672787 test rmse 1.1968997066328206\n",
      "train rmse 0.8825903418878024 test rmse 1.196922016465338\n",
      "train rmse 0.8825733232160721 test rmse 1.1969444752740364\n",
      "train rmse 0.8825566418979249 test rmse 1.1969662863079567\n",
      "train rmse 0.882540230414898 test rmse 1.1969880969444453\n",
      "train rmse 0.8825234471629226 test rmse 1.1970104051295474\n",
      "train rmse 0.882506967522454 test rmse 1.1970321153747912\n",
      "train rmse 0.8824905888863412 test rmse 1.1970535762623342\n",
      "train rmse 0.882474277488888 test rmse 1.1970753355165153\n",
      "train rmse 0.8824576618423716 test rmse 1.1970966462562265\n",
      "train rmse 0.8824415862443203 test rmse 1.1971181059869163\n",
      "train rmse 0.8824253077142623 test rmse 1.1971396649113575\n",
      "train rmse 0.8824097043600123 test rmse 1.197160725564265\n",
      "train rmse 0.8823936278882635 test rmse 1.1971819352090582\n",
      "train rmse 0.8823773146984145 test rmse 1.197202696398838\n",
      "train rmse 0.882361440290395 test rmse 1.197223706157224\n",
      "train rmse 0.8823461060165746 test rmse 1.1972446159772239\n",
      "train rmse 0.8823305012617019 test rmse 1.1972651769441198\n",
      "train rmse 0.8823150313405178 test rmse 1.1972858371242008\n",
      "train rmse 0.8822995611480898 test rmse 1.197306397383219\n",
      "train rmse 0.8822838542341639 test rmse 1.1973270070706068\n",
      "train rmse 0.8822689577413144 test rmse 1.1973472679392156\n",
      "train rmse 0.8822527098058743 test rmse 1.1973673791257562\n",
      "train rmse 0.8822376438852734 test rmse 1.1973877388690384\n",
      "train rmse 0.8822221723358189 test rmse 1.19740779959778\n",
      "train rmse 0.8822073085829828 test rmse 1.1974276111042532\n",
      "train rmse 0.8821923094512613 test rmse 1.1974478204942363\n",
      "train rmse 0.8821776816740278 test rmse 1.1974678802164531\n",
      "train rmse 0.8821624117725283 test rmse 1.1974876409541926\n",
      "train rmse 0.8821470402551669 test rmse 1.1975072520440617\n",
      "train rmse 0.8821324792960821 test rmse 1.197527161451439\n",
      "train rmse 0.8821178505267111 test rmse 1.1975459755372195\n",
      "train rmse 0.8821033904423569 test rmse 1.1975662327012389\n",
      "train rmse 0.8820881868270044 test rmse 1.1975845484661167\n",
      "train rmse 0.8820736586830483 test rmse 1.197604207739003\n",
      "train rmse 0.8820595357463025 test rmse 1.1976234685364266\n",
      "train rmse 0.8820446354650294 test rmse 1.1976426294874853\n",
      "train rmse 0.8820301403920549 test rmse 1.1976618896670106\n",
      "train rmse 0.882015915391881 test rmse 1.1976801542022892\n",
      "train rmse 0.8820012846891947 test rmse 1.1976994635443257\n",
      "train rmse 0.8819869916436692 test rmse 1.1977182251583742\n",
      "train rmse 0.8819729349001721 test rmse 1.1977362897769412\n",
      "train rmse 0.8819588103505216 test rmse 1.1977556479776115\n",
      "train rmse 0.8819448207410542 test rmse 1.1977741598974245\n",
      "train rmse 0.8819307971175451 test rmse 1.1977926715311351\n",
      "train rmse 0.8819169760270381 test rmse 1.1978107350269493\n",
      "train rmse 0.8819025126490521 test rmse 1.1978292460954165\n",
      "train rmse 0.8818885221465207 test rmse 1.1978472592798846\n",
      "train rmse 0.8818749031591329 test rmse 1.1978657697839286\n",
      "train rmse 0.8818605742704144 test rmse 1.1978834341111135\n",
      "train rmse 0.8818476307580557 test rmse 1.1979017450260223\n",
      "train rmse 0.8818341463224991 test rmse 1.1979200556610379\n",
      "train rmse 0.881819951957045 test rmse 1.1979377689437896\n",
      "train rmse 0.8818064670982289 test rmse 1.197955680985846\n",
      "train rmse 0.881793184817655 test rmse 1.1979732444781368\n",
      "train rmse 0.8817795643577926 test rmse 1.1979913052512445\n",
      "train rmse 0.8817662478735376 test rmse 1.1980090174805282\n",
      "train rmse 0.8817527959923668 test rmse 1.1980263314290818\n",
      "train rmse 0.8817395129033062 test rmse 1.1980435953757687\n",
      "train rmse 0.8817265000138769 test rmse 1.1980610580773965\n",
      "train rmse 0.8817127095214478 test rmse 1.198078371273889\n",
      "train rmse 0.881700000445286 test rmse 1.1980954354727382\n",
      "train rmse 0.8816866151563639 test rmse 1.1981131959113795\n",
      "train rmse 0.8816738042981181 test rmse 1.1981302098661923\n",
      "train rmse 0.8816607904388614 test rmse 1.198146875347929\n",
      "train rmse 0.8816478101904888 test rmse 1.1981641873043818\n",
      "train rmse 0.8816350325718455 test rmse 1.1981806035837508\n",
      "train rmse 0.8816226604154247 test rmse 1.198198014543366\n",
      "train rmse 0.881609476778981 test rmse 1.1982146790820347\n",
      "train rmse 0.8815964957750881 test rmse 1.1982314926204918\n",
      "train rmse 0.8815836498018214 test rmse 1.1982483556661738\n",
      "train rmse 0.8815708712532279 test rmse 1.198264621565025\n",
      "train rmse 0.8815583291643064 test rmse 1.1982810862102489\n",
      "train rmse 0.8815454488279684 test rmse 1.1982980480401988\n",
      "train rmse 0.8815326021108292 test rmse 1.198314462485746\n",
      "train rmse 0.8815201270931751 test rmse 1.198330379309128\n",
      "train rmse 0.8815083280657879 test rmse 1.1983469922680445\n",
      "train rmse 0.881495311763541 test rmse 1.1983633065664359\n",
      "train rmse 0.8814829714553534 test rmse 1.1983792724787872\n",
      "train rmse 0.881470833833023 test rmse 1.1983956360747687\n",
      "train rmse 0.8814583241309358 test rmse 1.1984116512927787\n",
      "train rmse 0.8814463552227484 test rmse 1.1984274176182186\n",
      "train rmse 0.8814338113620647 test rmse 1.198443531881618\n",
      "train rmse 0.881422078802499 test rmse 1.1984588999124492\n",
      "train rmse 0.8814102108384056 test rmse 1.1984752624212234\n",
      "train rmse 0.881397801713602 test rmse 1.1984910279098495\n",
      "train rmse 0.8813855614792889 test rmse 1.1985064947965272\n",
      "train rmse 0.8813738620909228 test rmse 1.198522309606098\n",
      "train rmse 0.8813617567799499 test rmse 1.1985377760890916\n",
      "train rmse 0.8813499556323884 test rmse 1.1985538888687484\n",
      "train rmse 0.8813376471038297 test rmse 1.198569305214413\n",
      "train rmse 0.8813261161560153 test rmse 1.1985846219034109\n",
      "train rmse 0.8813145174258099 test rmse 1.198599988125235\n",
      "train rmse 0.8813029523591619 test rmse 1.1986153541500646\n",
      "train rmse 0.8812909475241872 test rmse 1.198630471341484\n",
      "train rmse 0.8812793145140064 test rmse 1.1986460358821271\n",
      "train rmse 0.8812673093570315 test rmse 1.1986607548783954\n",
      "train rmse 0.8812560480329605 test rmse 1.1986755234193451\n",
      "train rmse 0.8812438058297277 test rmse 1.1986910376502282\n",
      "train rmse 0.8812328485754237 test rmse 1.1987053583005174\n",
      "train rmse 0.8812219588235066 test rmse 1.1987213196570408\n",
      "train rmse 0.8812101558037738 test rmse 1.1987358388372586\n",
      "train rmse 0.8811980820641444 test rmse 1.1987504075639368\n",
      "train rmse 0.8811868198553678 test rmse 1.198765175000384\n",
      "train rmse 0.8811761324610803 test rmse 1.198780389744637\n",
      "train rmse 0.8811645993996803 test rmse 1.1987949579298964\n",
      "train rmse 0.8811533367629545 test rmse 1.1988092276189082\n",
      "train rmse 0.8811415328240192 test rmse 1.1988238948922652\n",
      "train rmse 0.8811306419435454 test rmse 1.1988382139555158\n",
      "train rmse 0.881119378872769 test rmse 1.1988528808742418\n",
      "train rmse 0.8811086568367947 test rmse 1.1988673984611549\n",
      "train rmse 0.8810970214198535 test rmse 1.1988818661554093\n",
      "train rmse 0.8810864005853672 test rmse 1.1988959359449092\n",
      "train rmse 0.881075914922544 test rmse 1.198910303863429\n",
      "train rmse 0.8810649894056257 test rmse 1.198924671609764\n",
      "train rmse 0.8810533872377868 test rmse 1.1989388900404356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.8810429011820577 test rmse 1.1989530088746698\n",
      "train rmse 0.8810315693363646 test rmse 1.1989667795493042\n",
      "train rmse 0.8810210830209553 test rmse 1.198980947767875\n",
      "train rmse 0.8810099538593681 test rmse 1.1989952649555136\n",
      "train rmse 0.880998790729312 test rmse 1.1990090351448348\n",
      "train rmse 0.8809883716803073 test rmse 1.1990231034420857\n",
      "train rmse 0.8809778848507145 test rmse 1.1990368733116994\n",
      "train rmse 0.8809673640672018 test rmse 1.199050493893578\n",
      "train rmse 0.8809563357155206 test rmse 1.1990644125765335\n",
      "train rmse 0.8809456455250618 test rmse 1.1990779334281259\n",
      "train rmse 0.8809357671324813 test rmse 1.1990916032517664\n",
      "train rmse 0.8809249752002778 test rmse 1.1991053226271693\n",
      "train rmse 0.8809140816424678 test rmse 1.1991184950681641\n",
      "train rmse 0.8809040337383489 test rmse 1.199132661494157\n",
      "train rmse 0.8808930722576872 test rmse 1.1991458336348384\n",
      "train rmse 0.8808826857905886 test rmse 1.199159303863027\n",
      "train rmse 0.8808729420228241 test rmse 1.1991726745302984\n",
      "train rmse 0.8808622846534192 test rmse 1.1991861444569885\n",
      "train rmse 0.8808520669908876 test rmse 1.1991997633434366\n",
      "train rmse 0.8808412063653392 test rmse 1.1992127856376518\n",
      "train rmse 0.8808316313102492 test rmse 1.1992262551138066\n",
      "train rmse 0.8808212441186514 test rmse 1.1992388298018628\n",
      "train rmse 0.8808107891343595 test rmse 1.1992524480900082\n",
      "train rmse 0.8808009430648127 test rmse 1.1992649728024432\n",
      "train rmse 0.8807902848242363 test rmse 1.1992789883969859\n",
      "train rmse 0.8807801678352956 test rmse 1.1992905685341495\n",
      "train rmse 0.8807704906066992 test rmse 1.199306124508081\n",
      "train rmse 0.8807598996717272 test rmse 1.1993150703441995\n",
      "train rmse 0.8807496808214175 test rmse 1.1993339557790987\n",
      "train rmse 0.8807395295282233 test rmse 1.199338627393431\n",
      "train rmse 0.8807305624552442 test rmse 1.1993627306463175\n",
      "train rmse 0.8807201402330594 test rmse 1.1993600470091355\n",
      "train rmse 0.8807108345733159 test rmse 1.1993910575614757\n",
      "train rmse 0.8807002429209453 test rmse 1.1993863364554092\n",
      "train rmse 0.8806903617757705 test rmse 1.1994108860040273\n",
      "train rmse 0.8806806835603537 test rmse 1.1994203280043159\n",
      "train rmse 0.8806708698766793 test rmse 1.1994264901117888\n",
      "train rmse 0.8806613606512806 test rmse 1.1994509891452978\n",
      "train rmse 0.8806515805934972 test rmse 1.1994511879183711\n",
      "train rmse 0.8806423418924401 test rmse 1.19947056813484\n",
      "train rmse 0.8806326293073353 test rmse 1.1994844322514158\n",
      "train rmse 0.8806229842997455 test rmse 1.1994886063626546\n",
      "train rmse 0.880613034602274 test rmse 1.1995110668070972\n",
      "train rmse 0.8806033893800687 test rmse 1.199516681852495\n",
      "train rmse 0.880594353233945 test rmse 1.199529303186031\n",
      "train rmse 0.8805838617120061 test rmse 1.1995469430163022\n",
      "train rmse 0.8805747576771575 test rmse 1.199551166599144\n",
      "train rmse 0.8805660596823373 test rmse 1.1995694023684635\n",
      "train rmse 0.8805559062723076 test rmse 1.1995804331352329\n",
      "train rmse 0.8805466665674349 test rmse 1.199588532237246\n",
      "train rmse 0.8805376636848142 test rmse 1.1996069661858142\n",
      "train rmse 0.8805281530208612 test rmse 1.1996141210652818\n",
      "train rmse 0.8805188791784425 test rmse 1.1996273872913783\n",
      "train rmse 0.880509097538078 test rmse 1.1996417961347914\n",
      "train rmse 0.8804997896479063 test rmse 1.199648603010972\n",
      "train rmse 0.8804910909125089 test rmse 1.1996651480307317\n",
      "train rmse 0.8804812751156694 test rmse 1.1996753332897843\n",
      "train rmse 0.880471831538876 test rmse 1.1996849222595232\n",
      "train rmse 0.8804629294364681 test rmse 1.199700771217642\n",
      "train rmse 0.8804541964880938 test rmse 1.199708869507266\n",
      "train rmse 0.88044502341416 test rmse 1.1997216875472019\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_global_step()\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(cost, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(NB_EPOCHS):\n",
    "        _, train_pred, train_mse, reg, pen, train_cost = sess.run([train_op, pred, cost_l2, regularizer, penalty, cost], feed_dict={\n",
    "            user_batch: train['user'],\n",
    "            item_batch: train['item'],\n",
    "            rate_batch: train['rating']\n",
    "        })\n",
    "        test_pred, test_mse = sess.run([pred, cost_l2], feed_dict={\n",
    "            user_batch: test['user'],\n",
    "            item_batch: test['item'],\n",
    "            rate_batch: test['rating']\n",
    "        })\n",
    "        print('train rmse', train_mse ** 0.5, 'test rmse', test_mse ** 0.5)\n",
    "        # print('reg', reg, 'full cost', train_cost)\n",
    "        \n",
    "    #print(weights.eval(session=sess))\n",
    "    W_mat=weights.eval(session=sess)\n",
    "    alpha_vec= alpha.eval(session=sess)\n",
    "    beta_vec= beta.eval(session=sess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.8738337e-05  1.9294201e-05  1.3087435e-04  1.2566648e+00\n",
      "   7.3985589e-06  2.0610560e-02]\n",
      " [ 3.2607506e-06 -3.3195956e-06 -2.0274478e-05  5.0081193e-01\n",
      "  -1.3728210e-09 -1.5982183e-03]\n",
      " [-2.3860825e-05 -2.1961761e-05 -3.4835134e-04  2.2185493e+00\n",
      "  -3.1537449e-05 -5.7667527e-02]\n",
      " ...\n",
      " [-1.6882614e-05  2.3747984e-06  2.5604702e-06  2.0864081e+00\n",
      "  -3.3230835e-06 -1.0508324e-03]\n",
      " [ 1.0521260e-06 -1.4005027e-06 -1.4277983e-04  2.6976762e+00\n",
      "  -3.9130026e-07 -1.1859034e-02]\n",
      " [-1.7295177e-06 -5.9381155e-06 -5.2427469e-05  2.5884979e+00\n",
      "  -2.7851449e-06 -4.6925615e-03]]\n",
      "[-3.222721   -2.2427385  -0.4189402   6.2142725  -1.7571095  -0.91200113]\n",
      "[-0.95939165 -2.9488835   0.03732969  2.477148   -1.4513202   3.9942234 ]\n"
     ]
    }
   ],
   "source": [
    "print(W_mat)\n",
    "print(alpha_vec)\n",
    "print(beta_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(W_mat)\n",
    "\n",
    "# add some prints/visualisation\n",
    "# check encoding B\n",
    "# positivity on W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO => comparer avec SVD++ / baseline constante (predire la moyenne des ratins)\n",
    "# pourquoi le modele bilineaire n'√©tait pas convaicant ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
