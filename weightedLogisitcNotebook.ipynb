{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USERS = 5000\n",
    "FEAT_USER = 3\n",
    "NB_ITEMS = 100\n",
    "FEAT_ITEM = 6\n",
    "\n",
    "NB_EPOCHS = 500\n",
    "LAMBDA_REG = 1e-5\n",
    "# LAMBDA_REG = 0\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv('data/sushi/sushi3.udata', sep='\\t', names=('uid', 'gender', 'age', 'time', 'old_prefecture', 'old_region', 'old_eastwest', 'prefecture', 'region', 'eastwest', 'same'))\n",
    "items = pd.read_csv('data/sushi/sushi3.idata', sep='\\t', names=('iid', 'name', 'style', 'major', 'minor', 'heaviness', 'frequency', 'price', 'popularity'))\n",
    "R = pd.read_csv('data/sushi/sushi3b.5000.10.score', sep=' ', header=None)\n",
    "triplets = []\n",
    "for i, line in enumerate(np.array(R)):\n",
    "    for j, v in enumerate(line):\n",
    "        if v != -1:\n",
    "            triplets.append((i, j, v))\n",
    "df_ratings = pd.DataFrame(triplets, columns=('user', 'item', 'rating'))\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "#A is not used (only the \"criterion on the items\" => only B)\n",
    "#A = tf.constant(np.array(users[['age', 'gender', 'region']]).astype(np.float32)) \n",
    "B = tf.constant(np.array(items[['heaviness', 'frequency', 'price', 'popularity', 'style', 'major']]).astype(np.float32))\n",
    "\n",
    "weights = tf.get_variable('W', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "alpha = tf.get_variable(\"alpha\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "beta = tf.get_variable(\"item_bias\", shape=[FEAT_ITEM],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "weight_users = tf.nn.embedding_lookup(weights, user_batch)\n",
    "\n",
    "#beta_crit = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "#alpha_crit = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "\n",
    "feat_items = tf.nn.embedding_lookup(B, item_batch)\n",
    "#feat_users = tf.nn.embedding_lookup(A, user_batch)\n",
    "\n",
    "pred =tf.reduce_sum(tf.multiply(tf.nn.softmax(tf.multiply(feat_items, alpha)+beta),weight_users),1)\n",
    "\n",
    "cost_l2 = tf.losses.mean_squared_error(rate_batch, pred)\n",
    "\n",
    "regularizer=tf.nn.l2_loss(weight_users)\n",
    "\n",
    "#l2_user = tf.nn.l2_loss(weight_users)\n",
    "#l2_item = tf.nn.l2_loss(weight_items)\n",
    "#l2_bias_user = tf.nn.l2_loss(bias_users)\n",
    "#l2_bias_item = tf.nn.l2_loss(bias_items)\n",
    "#regularizer = tf.add(l2_user, l2_item)\n",
    "#regularizer = tf.add(regularizer, l2_bias_user)\n",
    "#regularizer = tf.add(regularizer, l2_bias_item)\n",
    "# regularizer = tf.nn.l2_loss(M)\n",
    "penalty = tf.constant(LAMBDA_REG, dtype=tf.float32, shape=[])\n",
    "cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 3.043606457468563 test rmse 2.9442904290119025\n",
      "train rmse 2.955217377078812 test rmse 2.861597583568599\n",
      "train rmse 2.8711359345327816 test rmse 2.7804012932436475\n",
      "train rmse 2.7890179627866916 test rmse 2.6997193508519097\n",
      "train rmse 2.707494830886542 test rmse 2.619899062476396\n",
      "train rmse 2.626626736810232 test rmse 2.541334526581432\n",
      "train rmse 2.546658653010331 test rmse 2.4641378271457657\n",
      "train rmse 2.4677236813902184 test rmse 2.3884672512391076\n",
      "train rmse 2.390025543650988 test rmse 2.3146385408324623\n",
      "train rmse 2.313867525467921 test rmse 2.2430533089903717\n",
      "train rmse 2.239624366088648 test rmse 2.1740809153963596\n",
      "train rmse 2.167665642521594 test rmse 2.10797219431452\n",
      "train rmse 2.098283670315933 test rmse 2.0449247897618315\n",
      "train rmse 2.0317203857396513 test rmse 1.9851842116391203\n",
      "train rmse 1.9682487576322196 test rmse 1.9289688440103743\n",
      "train rmse 1.9080874233250606 test rmse 1.8763489003708376\n",
      "train rmse 1.8512896888079393 test rmse 1.8272008271609637\n",
      "train rmse 1.7977327372804728 test rmse 1.7812667979736654\n",
      "train rmse 1.7471951759373088 test rmse 1.7382192536346084\n",
      "train rmse 1.699417769078714 test rmse 1.6977111442387545\n",
      "train rmse 1.6541340104858482 test rmse 1.6594251750824403\n",
      "train rmse 1.6110924750744686 test rmse 1.6231141518284298\n",
      "train rmse 1.5700654551960098 test rmse 1.5886262548052845\n",
      "train rmse 1.5308951141787785 test rmse 1.5558768472305797\n",
      "train rmse 1.493473716946448 test rmse 1.5248193977851332\n",
      "train rmse 1.4577567005006749 test rmse 1.495424763482297\n",
      "train rmse 1.4237304151626473 test rmse 1.4676870192457254\n",
      "train rmse 1.39142084387141 test rmse 1.4416325089187898\n",
      "train rmse 1.360892402611237 test rmse 1.4173342123079729\n",
      "train rmse 1.332248614075166 test rmse 1.3949024123236402\n",
      "train rmse 1.3056191869654703 test rmse 1.3744687874900703\n",
      "train rmse 1.2811393224783005 test rmse 1.356160627663815\n",
      "train rmse 1.2589326692261331 test rmse 1.3400723009389361\n",
      "train rmse 1.2390924438091009 test rmse 1.3262437399267932\n",
      "train rmse 1.2216697832738268 test rmse 1.3146497287497372\n",
      "train rmse 1.2066705652476044 test rmse 1.3052048713818807\n",
      "train rmse 1.1940449994264086 test rmse 1.297767148455183\n",
      "train rmse 1.1836931545703202 test rmse 1.2921598733626394\n",
      "train rmse 1.1754652340030276 test rmse 1.2881714949344414\n",
      "train rmse 1.1691691886147788 test rmse 1.2855725759036902\n",
      "train rmse 1.1645793919277656 test rmse 1.2841185116428973\n",
      "train rmse 1.1614616063275713 test rmse 1.2835654294493721\n",
      "train rmse 1.1595694345474454 test rmse 1.2836821197719444\n",
      "train rmse 1.1586743275974798 test rmse 1.2842645305266733\n",
      "train rmse 1.158564493500591 test rmse 1.2851434956297434\n",
      "train rmse 1.1590520568360736 test rmse 1.2861813801811195\n",
      "train rmse 1.159979090666591 test rmse 1.2872756132865946\n",
      "train rmse 1.1612168418823225 test rmse 1.288352262021497\n",
      "train rmse 1.1626640470694711 test rmse 1.2893669924762428\n",
      "train rmse 1.1642435944266092 test rmse 1.2902987014575944\n",
      "train rmse 1.1659006033288821 test rmse 1.2911478918422565\n",
      "train rmse 1.1675938543623858 test rmse 1.2919251993333853\n",
      "train rmse 1.1692929113579198 test rmse 1.2926447713114726\n",
      "train rmse 1.170977134064362 test rmse 1.2933191475794856\n",
      "train rmse 1.1726289993769337 test rmse 1.293956598196656\n",
      "train rmse 1.174233314286782 test rmse 1.2945616438249592\n",
      "train rmse 1.1757795771083146 test rmse 1.2951350662315937\n",
      "train rmse 1.1772573336836403 test rmse 1.2956749752875154\n",
      "train rmse 1.1786574862796544 test rmse 1.2961791596142715\n",
      "train rmse 1.1799736938938745 test rmse 1.2966467415194822\n",
      "train rmse 1.1811992230678579 test rmse 1.2970829532686865\n",
      "train rmse 1.1823290624320024 test rmse 1.297497289798089\n",
      "train rmse 1.1833610683647395 test rmse 1.2978995079673858\n",
      "train rmse 1.1842914702821037 test rmse 1.298291914544061\n",
      "train rmse 1.185118898068417 test rmse 1.2986723153871382\n",
      "train rmse 1.1858424112147172 test rmse 1.2990330125785332\n",
      "train rmse 1.1864650633467293 test rmse 1.299367187608625\n",
      "train rmse 1.1869874651246222 test rmse 1.2996697244715\n",
      "train rmse 1.1874157223158384 test rmse 1.299937343921484\n",
      "train rmse 1.1877536502782846 test rmse 1.3001642911974316\n",
      "train rmse 1.1880090024170329 test rmse 1.3003494874973822\n",
      "train rmse 1.188187901870092 test rmse 1.3004893757299263\n",
      "train rmse 1.18829740554692 test rmse 1.3005850704136948\n",
      "train rmse 1.188348216239607 test rmse 1.3006364896487688\n",
      "train rmse 1.1883481159244864 test rmse 1.3006452884585655\n",
      "train rmse 1.1883063339404873 test rmse 1.3006154089248205\n",
      "train rmse 1.1882274808637896 test rmse 1.3005539520817238\n",
      "train rmse 1.188119074421915 test rmse 1.3004686592869268\n",
      "train rmse 1.1879887328047076 test rmse 1.3003690140848687\n",
      "train rmse 1.1878438752849643 test rmse 1.3002609266171512\n",
      "train rmse 1.1876872567554617 test rmse 1.3001490250780865\n",
      "train rmse 1.1875261002734072 test rmse 1.300037388997502\n",
      "train rmse 1.1873637171393059 test rmse 1.2999293656600197\n",
      "train rmse 1.1872024163265265 test rmse 1.2998274321826686\n",
      "train rmse 1.1870468680521906 test rmse 1.299732002730961\n",
      "train rmse 1.186893257936778 test rmse 1.2996411983812008\n",
      "train rmse 1.1867460567906285 test rmse 1.2995538734667444\n",
      "train rmse 1.1866041116799801 test rmse 1.2994702121627408\n",
      "train rmse 1.186467876624805 test rmse 1.2993909491165216\n",
      "train rmse 1.186338308200836 test rmse 1.2993184705715703\n",
      "train rmse 1.1862156598313092 test rmse 1.299251584884267\n",
      "train rmse 1.1860997829050153 test rmse 1.2991919904144857\n",
      "train rmse 1.1859932927815258 test rmse 1.2991398716859643\n",
      "train rmse 1.1858962925120953 test rmse 1.2990950919534805\n",
      "train rmse 1.1858100913140996 test rmse 1.299056688432906\n",
      "train rmse 1.1857373054817344 test rmse 1.299023835774752\n",
      "train rmse 1.1856759769337624 test rmse 1.2989963049744289\n",
      "train rmse 1.1856267107205676 test rmse 1.298973132765261\n",
      "train rmse 1.1855891061516584 test rmse 1.298951336745707\n",
      "train rmse 1.1855621085531536 test rmse 1.2989291732602692\n",
      "train rmse 1.185544210364978 test rmse 1.2989042102068538\n",
      "train rmse 1.1855323954016925 test rmse 1.2988778241049461\n",
      "train rmse 1.1855264627374236 test rmse 1.298849831306519\n",
      "train rmse 1.1855252058132164 test rmse 1.2988255550961418\n",
      "train rmse 1.1855269655067335 test rmse 1.2988053628174618\n",
      "train rmse 1.1855309876535356 test rmse 1.2987897135855602\n",
      "train rmse 1.1855343059143746 test rmse 1.2987800761348536\n",
      "train rmse 1.1855392330118197 test rmse 1.298774890238927\n",
      "train rmse 1.185543556774341 test rmse 1.2987745689880557\n",
      "train rmse 1.185546824723923 test rmse 1.2987773684572628\n",
      "train rmse 1.185550042388556 test rmse 1.2987828755921902\n",
      "train rmse 1.1855535616992494 test rmse 1.2987897594780116\n",
      "train rmse 1.1855548185933937 test rmse 1.2987969186805615\n",
      "train rmse 1.1855561257618903 test rmse 1.2988036189241219\n",
      "train rmse 1.1855565279672915 test rmse 1.2988077491940273\n",
      "train rmse 1.1855554219021096 test rmse 1.298809171839509\n",
      "train rmse 1.185554165008605 test rmse 1.2988075197349327\n",
      "train rmse 1.1855513998382041 test rmse 1.2988029305445272\n",
      "train rmse 1.185546824723923 test rmse 1.298796092620743\n",
      "train rmse 1.1855410429608406 test rmse 1.2987870518206084\n",
      "train rmse 1.1855349595101117 test rmse 1.2987774602430362\n",
      "train rmse 1.1855272168913082 test rmse 1.2987666753702556\n",
      "train rmse 1.1855167592479765 test rmse 1.2987552478966962\n",
      "train rmse 1.1855053462338736 test rmse 1.29874285654668\n",
      "train rmse 1.1854926761511984 test rmse 1.2987292259251093\n",
      "train rmse 1.185477642821894 test rmse 1.298713667566056\n",
      "train rmse 1.1854615534288313 test rmse 1.298696778044945\n",
      "train rmse 1.185445664938887 test rmse 1.2986794752371198\n",
      "train rmse 1.1854283683659652 test rmse 1.298660978877271\n",
      "train rmse 1.1854112223861633 test rmse 1.2986420232773492\n",
      "train rmse 1.185393673897834 test rmse 1.298622149433588\n",
      "train rmse 1.1853766279827886 test rmse 1.29860259657917\n",
      "train rmse 1.1853576207442194 test rmse 1.298584695820507\n",
      "train rmse 1.1853401217471855 test rmse 1.2985677128213624\n",
      "train rmse 1.1853232762042738 test rmse 1.2985528410397054\n",
      "train rmse 1.18530497211909 test rmse 1.2985382444957918\n",
      "train rmse 1.1852863660285236 test rmse 1.298525392057136\n",
      "train rmse 1.1852695197216132 test rmse 1.2985124476867695\n",
      "train rmse 1.185251063940395 test rmse 1.298499732700889\n",
      "train rmse 1.185232104977619 test rmse 1.2984863290430635\n",
      "train rmse 1.185212894260381 test rmse 1.2984726039212977\n",
      "train rmse 1.1851938341049237 test rmse 1.2984583737087587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 1.185174924518511 test rmse 1.2984447401007861\n",
      "train rmse 1.1851574731165408 test rmse 1.2984311522547858\n",
      "train rmse 1.1851392670573222 test rmse 1.298417380644154\n",
      "train rmse 1.185121312189093 test rmse 1.2984049860696958\n",
      "train rmse 1.1851040108822481 test rmse 1.2983926372834058\n",
      "train rmse 1.185087866121009 test rmse 1.298380885169538\n",
      "train rmse 1.1850719726209662 test rmse 1.2983690870419775\n",
      "train rmse 1.1850573363295502 test rmse 1.2983565542832103\n",
      "train rmse 1.185042297477568 test rmse 1.298345398648915\n",
      "train rmse 1.1850286667812913 test rmse 1.298334105193105\n",
      "train rmse 1.1850157401089785 test rmse 1.2983234543642832\n",
      "train rmse 1.1850031150905693 test rmse 1.2983133543596876\n",
      "train rmse 1.1849914456312236 test rmse 1.2983041724691458\n",
      "train rmse 1.1849800778577413 test rmse 1.2982959087121373\n",
      "train rmse 1.1849688105764131 test rmse 1.2982883794655131\n",
      "train rmse 1.1849584989080932 test rmse 1.2982817683836945\n",
      "train rmse 1.1849486398632514 test rmse 1.2982758918382602\n",
      "train rmse 1.1849394346605997 test rmse 1.2982697857120915\n",
      "train rmse 1.184931537243955 test rmse 1.2982646895998442\n",
      "train rmse 1.1849235894721537 test rmse 1.2982604198687764\n",
      "train rmse 1.1849155410413204 test rmse 1.2982559664784004\n",
      "train rmse 1.1849075428590226 test rmse 1.298251237603855\n",
      "train rmse 1.184899947051009 test rmse 1.2982464168887733\n",
      "train rmse 1.1848930051441464 test rmse 1.298241320684794\n",
      "train rmse 1.1848854595470795 test rmse 1.2982367294929085\n",
      "train rmse 1.184877058725815 test rmse 1.2982315414265377\n",
      "train rmse 1.1848701166848545 test rmse 1.298225481004168\n",
      "train rmse 1.1848624703317971 test rmse 1.2982202469802628\n",
      "train rmse 1.1848554275945364 test rmse 1.2982148292841704\n",
      "train rmse 1.1848474290064495 test rmse 1.2982088146967716\n",
      "train rmse 1.1848401346494846 test rmse 1.2982034428661475\n",
      "train rmse 1.1848324377965465 test rmse 1.298198116926662\n",
      "train rmse 1.1848252942673831 test rmse 1.2981937551496017\n",
      "train rmse 1.184817697932078 test rmse 1.2981895310989013\n",
      "train rmse 1.1848109567725431 test rmse 1.2981851692929933\n",
      "train rmse 1.184804316189829 test rmse 1.2981817257516612\n",
      "train rmse 1.1847980277249175 test rmse 1.2981786495136798\n",
      "train rmse 1.18479158830226 test rmse 1.2981752518692284\n",
      "train rmse 1.1847855513117325 test rmse 1.2981724051873014\n",
      "train rmse 1.1847792124385719 test rmse 1.2981696962422513\n",
      "train rmse 1.1847735275472875 test rmse 1.2981664363178325\n",
      "train rmse 1.184767591083026 test rmse 1.2981638191894875\n",
      "train rmse 1.184761704898415 test rmse 1.29816097248249\n",
      "train rmse 1.1847557683749135 test rmse 1.298157942110117\n",
      "train rmse 1.1847502343007918 test rmse 1.2981550953902319\n",
      "train rmse 1.1847455554725044 test rmse 1.2981524782390232\n",
      "train rmse 1.1847403735228361 test rmse 1.2981497692523902\n",
      "train rmse 1.1847356946556058 test rmse 1.2981469684297582\n",
      "train rmse 1.1847301604877165 test rmse 1.2981447185842507\n",
      "train rmse 1.1847248778487973 test rmse 1.2981427442268176\n",
      "train rmse 1.184720852965239 test rmse 1.2981408157817749\n",
      "train rmse 1.184716727445401 test rmse 1.2981393005729456\n",
      "train rmse 1.1847129037800401 test rmse 1.298137785362348\n",
      "train rmse 1.1847085769859325 test rmse 1.2981366374743541\n",
      "train rmse 1.1847046023588153 test rmse 1.2981348008514526\n",
      "train rmse 1.1847004767823872 test rmse 1.2981333315512606\n",
      "train rmse 1.1846960996307132 test rmse 1.29813209182793\n",
      "train rmse 1.184693080895998 test rmse 1.2981310816821185\n",
      "train rmse 1.184689055904412 test rmse 1.298130714356174\n",
      "train rmse 1.1846857352760594 test rmse 1.298128923640706\n",
      "train rmse 1.184682263700102 test rmse 1.2981280053241233\n",
      "train rmse 1.184678389610574 test rmse 1.2981271788386435\n",
      "train rmse 1.184675018639248 test rmse 1.2981262605208264\n",
      "train rmse 1.1846711445260272 test rmse 1.2981256176979679\n",
      "train rmse 1.184668326981182 test rmse 1.298124561631152\n",
      "train rmse 1.1846655094296354 test rmse 1.2981239647234415\n",
      "train rmse 1.1846622390489017 test rmse 1.2981235055634772\n",
      "train rmse 1.1846595221103493 test rmse 1.2981223117468101\n",
      "train rmse 1.1846564529685637 test rmse 1.2981221280826103\n",
      "train rmse 1.1846536857027599 test rmse 1.2981219903344432\n",
      "train rmse 1.1846509184304919 test rmse 1.2981213015933883\n",
      "train rmse 1.1846485033512302 test rmse 1.298121255677305\n",
      "train rmse 1.1846456857525354 test rmse 1.2981207046841787\n",
      "train rmse 1.184643119719322 test rmse 1.2981209342646765\n",
      "train rmse 1.184640855567753 test rmse 1.2981207965163826\n",
      "train rmse 1.1846383901533455 test rmse 1.298121026096864\n",
      "train rmse 1.1846363272516331 test rmse 1.2981214852577054\n",
      "train rmse 1.184634767494295 test rmse 1.29812139342555\n",
      "train rmse 1.1846326542713994 test rmse 1.2981213015933883\n",
      "train rmse 1.1846308429344887 test rmse 1.2981219903344432\n",
      "train rmse 1.1846291825398876 test rmse 1.2981224494949433\n",
      "train rmse 1.1846272202523584 test rmse 1.298122908655281\n",
      "train rmse 1.184625459222321 test rmse 1.2981233678154562\n",
      "train rmse 1.184623798820174 test rmse 1.298124010639429\n",
      "train rmse 1.1846219874697228 test rmse 1.2981250207907427\n",
      "train rmse 1.1846200251702754 test rmse 1.2981256176979679\n",
      "train rmse 1.1846184653914726 test rmse 1.2981263523526372\n",
      "train rmse 1.1846169056106162 test rmse 1.29812694925925\n",
      "train rmse 1.184615396143316 test rmse 1.2981275461655883\n",
      "train rmse 1.1846143395150612 test rmse 1.298128464482496\n",
      "train rmse 1.1846133332015614 test rmse 1.2981296582935045\n",
      "train rmse 1.1846120249927337 test rmse 1.298130484777406\n",
      "train rmse 1.1846104652033973 test rmse 1.2981319081651137\n",
      "train rmse 1.1846098110975805 test rmse 1.2981325969005408\n",
      "train rmse 1.1846088550961213 test rmse 1.2981335152138755\n",
      "train rmse 1.1846077984620322 test rmse 1.2981345712734074\n",
      "train rmse 1.1846072449866571 test rmse 1.298135535500925\n",
      "train rmse 1.1846064902470925 test rmse 1.2981368211365014\n",
      "train rmse 1.1846054839269249 test rmse 1.2981383363482244\n",
      "train rmse 1.1846049304504682 test rmse 1.2981395760655916\n",
      "train rmse 1.1846041253933424 test rmse 1.2981410912740992\n",
      "train rmse 1.1846033706517902 test rmse 1.298142055496774\n",
      "train rmse 1.1846025655936043 test rmse 1.298143524787092\n",
      "train rmse 1.1846018108510583 test rmse 1.2981444890079594\n",
      "train rmse 1.1846017102186825 test rmse 1.2981456828042315\n",
      "train rmse 1.1846010057918124 test rmse 1.298147289835941\n",
      "train rmse 1.1846005529457462 test rmse 1.2981493101015533\n",
      "train rmse 1.1846001000995068 test rmse 1.2981504120632894\n",
      "train rmse 1.1845993956716794 test rmse 1.2981522486641042\n",
      "train rmse 1.1845985402944685 test rmse 1.2981540393473967\n",
      "train rmse 1.184598691243433 test rmse 1.2981555545390226\n",
      "train rmse 1.1845990937739108 test rmse 1.298157207473325\n",
      "train rmse 1.18459874155975 test rmse 1.2981587226612532\n",
      "train rmse 1.1845984899781428 test rmse 1.2981604215062212\n",
      "train rmse 1.1845982887128184 test rmse 1.298162166263604\n",
      "train rmse 1.184598238396482 test rmse 1.2981641405914983\n",
      "train rmse 1.184598439661815 test rmse 1.29816560985682\n",
      "train rmse 1.184598238396482 test rmse 1.2981674464361335\n",
      "train rmse 1.184598238396482 test rmse 1.2981689156977134\n",
      "train rmse 1.1845981880801435 test rmse 1.2981705227006624\n",
      "train rmse 1.184597886182067 test rmse 1.298172542930119\n",
      "train rmse 1.1845980874474598 test rmse 1.2981741958427904\n",
      "train rmse 1.1845983390291528 test rmse 1.2981757569250474\n",
      "train rmse 1.184598691243433 test rmse 1.2981780985449134\n",
      "train rmse 1.1845985402944685 test rmse 1.2981799810205665\n",
      "train rmse 1.1845984899781428 test rmse 1.298182001235304\n",
      "train rmse 1.18459874155975 test rmse 1.2981841132746226\n",
      "train rmse 1.184598842192378 test rmse 1.2981860875691393\n",
      "train rmse 1.1845992447228046 test rmse 1.2981883373429515\n",
      "train rmse 1.1845992950390984 test rmse 1.2981901279764645\n",
      "train rmse 1.1845998988344562 test rmse 1.298192010434674\n",
      "train rmse 1.184599949150722 test rmse 1.2981943061117232\n",
      "train rmse 1.1846003013645234 test rmse 1.2981963263041683\n",
      "train rmse 1.1846008045269154 test rmse 1.298198116926662\n",
      "train rmse 1.1846015089539053 test rmse 1.298200228939762\n",
      "train rmse 1.1846020624319604 test rmse 1.2982024786890674\n",
      "train rmse 1.1846028171743461 test rmse 1.298204912086988\n",
      "train rmse 1.1846029681227657 test rmse 1.2982065649584462\n",
      "train rmse 1.1846036725484688 test rmse 1.2982087228707937\n",
      "train rmse 1.1846042260255132 test rmse 1.2982111562570102\n",
      "train rmse 1.1846050813986184 test rmse 1.2982132223360665\n",
      "train rmse 1.1846051820307077 test rmse 1.2982152884118348\n",
      "train rmse 1.1846056851910267 test rmse 1.2982175381350427\n",
      "train rmse 1.184606087719128 test rmse 1.2982196501165473\n",
      "train rmse 1.1846061883511319 test rmse 1.298221762094616\n",
      "train rmse 1.1846064902470925 test rmse 1.2982241495439477\n",
      "train rmse 1.1846072449866571 test rmse 1.298226169689953\n",
      "train rmse 1.184608050041663 test rmse 1.29822837348201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 1.1846086035166619 test rmse 1.2982304854458875\n",
      "train rmse 1.1846091066755275 test rmse 1.298232872879177\n",
      "train rmse 1.184609911729268 test rmse 1.298235260308076\n",
      "train rmse 1.1846104652033973 test rmse 1.2982373722607499\n",
      "train rmse 1.1846115218351077 test rmse 1.298239713769481\n",
      "train rmse 1.1846123772029449 test rmse 1.2982419175385467\n",
      "train rmse 1.1846133332015614 test rmse 1.298244167215609\n",
      "train rmse 1.1846137860427421 test rmse 1.2982465546237383\n",
      "train rmse 1.1846147420402218 test rmse 1.2982488961159082\n",
      "train rmse 1.1846159999304668 test rmse 1.2982513753383087\n",
      "train rmse 1.1846165534017514 test rmse 1.2982533954419493\n",
      "train rmse 1.1846177106590567 test rmse 1.2982560583010365\n",
      "train rmse 1.1846182138140542 test rmse 1.2982581702198772\n",
      "train rmse 1.1846193710697372 test rmse 1.2982599607570728\n",
      "train rmse 1.1846203773781079 test rmse 1.2982623481361588\n",
      "train rmse 1.1846216855777119 test rmse 1.2982647814218635\n",
      "train rmse 1.1846222390463401 test rmse 1.2982672606139287\n",
      "train rmse 1.184623647874409 test rmse 1.2982697398012597\n",
      "train rmse 1.184624251657354 test rmse 1.2982719894301111\n",
      "train rmse 1.1846251573311948 test rmse 1.2982745604297394\n",
      "train rmse 1.1846263145800948 test rmse 1.2982765804973042\n",
      "train rmse 1.1846277737184008 test rmse 1.2982784628301607\n",
      "train rmse 1.1846287297045923 test rmse 1.2982810338169697\n",
      "train rmse 1.1846297863200117 test rmse 1.2982835588883443\n",
      "train rmse 1.184630641674661 test rmse 1.2982857625829853\n",
      "train rmse 1.1846316982883751 test rmse 1.2982878744535062\n",
      "train rmse 1.1846325536416438 test rmse 1.2982904913317772\n",
      "train rmse 1.1846335599388162 test rmse 1.298292603194606\n",
      "train rmse 1.1846350693829724 test rmse 1.2982950823335435\n",
      "train rmse 1.1846362769369123 test rmse 1.2982974237381932\n",
      "train rmse 1.1846369813428088 test rmse 1.2982995355897455\n",
      "train rmse 1.1846385914118567 test rmse 1.2983017851669683\n",
      "train rmse 1.1846397989622066 test rmse 1.2983046315652151\n",
      "train rmse 1.1846409058822793 test rmse 1.2983069729526435\n",
      "train rmse 1.1846422643736823 test rmse 1.2983091306981034\n",
      "train rmse 1.1846434216058708 test rmse 1.2983116098054814\n",
      "train rmse 1.184644780094389 test rmse 1.2983139511803252\n",
      "train rmse 1.184645283237889 test rmse 1.2983161548233797\n",
      "train rmse 1.1846463901528372 test rmse 1.2983186339173454\n",
      "train rmse 1.1846475473809956 test rmse 1.298320883461477\n",
      "train rmse 1.184648805236407 test rmse 1.2983231330017109\n",
      "train rmse 1.184649660577324 test rmse 1.2983257039000624\n",
      "train rmse 1.1846507171740748 test rmse 1.2983281829757947\n",
      "train rmse 1.1846517737698832 test rmse 1.2983305243207508\n",
      "train rmse 1.1846534341328208 test rmse 1.298332406575399\n",
      "train rmse 1.1846544404122565 test rmse 1.2983350233639126\n",
      "train rmse 1.184655698260349 test rmse 1.2983373646965333\n",
      "train rmse 1.184656805165566 test rmse 1.298339614208211\n",
      "train rmse 1.1846582642663168 test rmse 1.2983420014408311\n",
      "train rmse 1.184659371169136 test rmse 1.2983444804854447\n",
      "train rmse 1.184660478070921 test rmse 1.298346821801011\n",
      "train rmse 1.1846618868535148 test rmse 1.2983493008364204\n",
      "train rmse 1.184663144693701 test rmse 1.2983513207876989\n",
      "train rmse 1.1846643019054928 test rmse 1.2983537539066572\n",
      "train rmse 1.1846653584891849 test rmse 1.2983562329288307\n",
      "train rmse 1.1846664150719348 test rmse 1.2983583446847267\n",
      "train rmse 1.1846680251009796 test rmse 1.2983609155133553\n",
      "train rmse 1.184669182308004 test rmse 1.2983631649842298\n",
      "train rmse 1.184670339513898 test rmse 1.298365414451207\n",
      "train rmse 1.1846715470318867 test rmse 1.2983672966552746\n",
      "train rmse 1.1846728551749854 test rmse 1.298370234724463\n",
      "train rmse 1.1846743645690738 test rmse 1.2983723464575854\n",
      "train rmse 1.1846755720829598 test rmse 1.2983747336300229\n",
      "train rmse 1.1846766286566004 test rmse 1.298376615820581\n",
      "train rmse 1.1846778864811343 test rmse 1.2983790947991039\n",
      "train rmse 1.1846791443043327 test rmse 1.298381481959134\n",
      "train rmse 1.184680704003244 test rmse 1.2983837773011777\n",
      "train rmse 1.1846817605723077 test rmse 1.298386164452599\n",
      "train rmse 1.1846832699550498 test rmse 1.2983881384398819\n",
      "train rmse 1.1846842258964583 test rmse 1.2983906633029625\n",
      "train rmse 1.1846857855886794 test rmse 1.2983928668158207\n",
      "train rmse 1.1846869427783555 test rmse 1.2983948866993222\n",
      "train rmse 1.1846885024669997 test rmse 1.2983970902050128\n",
      "train rmse 1.1846899112162683 test rmse 1.2983989723631624\n",
      "train rmse 1.1846914205886259 test rmse 1.2984016808298575\n",
      "train rmse 1.1846928796467435 test rmse 1.2984040679483626\n",
      "train rmse 1.184694238078548 test rmse 1.298406317344476\n",
      "train rmse 1.1846955965087949 test rmse 1.2984085208307679\n",
      "train rmse 1.184696502128094 test rmse 1.2984106784074718\n",
      "train rmse 1.1846976593073022 test rmse 1.2984128818863627\n",
      "train rmse 1.184699017733626 test rmse 1.29841503945582\n",
      "train rmse 1.1847003761583923 test rmse 1.2984172888329262\n",
      "train rmse 1.1847013320859978 test rmse 1.2984198595448475\n",
      "train rmse 1.1847025395723971 test rmse 1.298421833480904\n",
      "train rmse 1.1847040489286653 test rmse 1.2984243582784627\n",
      "train rmse 1.184705105476909 test rmse 1.2984259649652619\n",
      "train rmse 1.1847063129594624 test rmse 1.2984284438495668\n",
      "train rmse 1.1847078726226055 test rmse 1.2984305554880196\n",
      "train rmse 1.1847089794790742 test rmse 1.2984329425534396\n",
      "train rmse 1.1847103882039935 test rmse 1.2984349623745979\n",
      "train rmse 1.1847115956811625 test rmse 1.2984370280975328\n",
      "train rmse 1.1847128534685716 test rmse 1.2984392774365467\n",
      "train rmse 1.1847142118774734 test rmse 1.298441802200186\n",
      "train rmse 1.184715670907522 test rmse 1.2984436842935232\n",
      "train rmse 1.1847172808696282 test rmse 1.2984460254302084\n",
      "train rmse 1.1847186392734534 test rmse 1.2984483665626725\n",
      "train rmse 1.1847195448751384 test rmse 1.2984504781687254\n",
      "train rmse 1.1847211548319803 test rmse 1.2984524520582348\n",
      "train rmse 1.1847223119871089 test rmse 1.2984546095619405\n",
      "train rmse 1.1847235697631406 test rmse 1.2984569965831398\n",
      "train rmse 1.1847247269159107 test rmse 1.2984591081751582\n",
      "train rmse 1.1847260350002895 test rmse 1.2984612197637428\n",
      "train rmse 1.1847274940157775 test rmse 1.2984632395409148\n",
      "train rmse 1.1847290033402538 test rmse 1.2984659019696594\n",
      "train rmse 1.1847301101769807 test rmse 1.2984675545088928\n",
      "train rmse 1.1847310660805948 test rmse 1.2984694365649028\n",
      "train rmse 1.1847322735366888 test rmse 1.2984715940403877\n",
      "train rmse 1.1847338834762333 test rmse 1.2984734760905425\n",
      "train rmse 1.1847349399978706 test rmse 1.2984755876557619\n",
      "train rmse 1.1847359965185655 test rmse 1.2984776074105846\n",
      "train rmse 1.1847371033487593 test rmse 1.2984795812588532\n",
      "train rmse 1.184738763592111 test rmse 1.2984819682341466\n",
      "train rmse 1.1847402225919237 test rmse 1.2984838043660019\n",
      "train rmse 1.1847414803489407 test rmse 1.2984859159144257\n",
      "train rmse 1.1847426374842172 test rmse 1.298487935653183\n",
      "train rmse 1.1847439958589692 test rmse 1.2984901849040111\n",
      "train rmse 1.184745404542252 test rmse 1.2984921587331608\n",
      "train rmse 1.184746461053615 test rmse 1.2984943161708922\n",
      "train rmse 1.1847478697339668 test rmse 1.2984964277022222\n",
      "train rmse 1.1847490771729352 test rmse 1.298497942494322\n",
      "train rmse 1.1847509386389339 test rmse 1.298500191727816\n",
      "train rmse 1.1847521460747747 test rmse 1.2985022114443685\n",
      "train rmse 1.1847533535093848 test rmse 1.2985039557424993\n",
      "train rmse 1.1847543093942452 test rmse 1.2985058836482335\n",
      "train rmse 1.1847557683749135 test rmse 1.2985078574535198\n",
      "train rmse 1.184757478901685 test rmse 1.2985097394511147\n",
      "train rmse 1.184758434783217 test rmse 1.2985118968596363\n",
      "train rmse 1.184759893758805 test rmse 1.2985138706557822\n",
      "train rmse 1.1847607490195213 test rmse 1.2985158903510583\n",
      "train rmse 1.184761956445364 test rmse 1.2985179559452504\n",
      "train rmse 1.1847631135606418 test rmse 1.2985200215361568\n",
      "train rmse 1.1847647234582788 test rmse 1.298521903516122\n",
      "train rmse 1.184765729643191 test rmse 1.2985236477878004\n",
      "train rmse 1.184767188609796 test rmse 1.298525437958929\n",
      "train rmse 1.1847684966472936 test rmse 1.298527411734492\n",
      "train rmse 1.1847694525199364 test rmse 1.2985296150152976\n",
      "train rmse 1.184770609627893 test rmse 1.2985317723907976\n",
      "train rmse 1.1847722195153443 test rmse 1.2985335625507255\n",
      "train rmse 1.1847732760027951 test rmse 1.298535536313939\n",
      "train rmse 1.1847744331070178 test rmse 1.2985375100741527\n",
      "train rmse 1.1847755902101103 test rmse 1.2985394379300697\n",
      "train rmse 1.1847770491645724 test rmse 1.2985414116843528\n",
      "train rmse 1.1847782565738034 test rmse 1.298543201830992\n",
      "train rmse 1.1847796149077172 test rmse 1.2985447165685282\n",
      "train rmse 1.1847809229314958 test rmse 1.2985466444137463\n",
      "train rmse 1.184781979411186 test rmse 1.298548526355127\n",
      "train rmse 1.1847831868153924 test rmse 1.2985504082937804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 1.184784042059294 test rmse 1.2985522443288626\n",
      "train rmse 1.1847854003865752 test rmse 1.2985544016667667\n",
      "train rmse 1.1847866077872953 test rmse 1.298555824589806\n",
      "train rmse 1.1847879158033536 test rmse 1.2985574770146044\n",
      "train rmse 1.1847890225850444 test rmse 1.2985597261449788\n",
      "train rmse 1.1847901796738882 test rmse 1.2985614703658528\n",
      "train rmse 1.1847912361453241 test rmse 1.298563076883006\n",
      "train rmse 1.1847922926158179 test rmse 1.2985653260036816\n",
      "train rmse 1.1847936509336399 test rmse 1.2985671161173535\n",
      "train rmse 1.1847948080179636 test rmse 1.2985690898295674\n",
      "train rmse 1.1847961663329019 test rmse 1.2985705586367098\n",
      "train rmse 1.1847973737226507 test rmse 1.2985724405434331\n",
      "train rmse 1.1847986314189973 test rmse 1.2985741847472292\n",
      "train rmse 1.1847997884984571 test rmse 1.2985755617485715\n",
      "train rmse 1.1848008449613248 test rmse 1.2985772600482157\n",
      "train rmse 1.1848018511155611 test rmse 1.2985792337450117\n",
      "train rmse 1.1848032597300566 test rmse 1.2985811615389864\n",
      "train rmse 1.1848044168049963 test rmse 1.2985826762322443\n",
      "train rmse 1.1848056744938666 test rmse 1.2985845581214068\n",
      "train rmse 1.184807032796347 test rmse 1.298586348208567\n",
      "train rmse 1.1848082401750224 test rmse 1.2985880923936832\n",
      "train rmse 1.1848090450934556 test rmse 1.2985898365764565\n",
      "train rmse 1.1848104536993982 test rmse 1.2985916266563404\n",
      "train rmse 1.184811761689131 test rmse 1.2985934626331455\n",
      "train rmse 1.1848131199846328 test rmse 1.2985951150100505\n",
      "train rmse 1.1848143776642646 test rmse 1.2985969050826585\n",
      "train rmse 1.1848153838070088 test rmse 1.2985982361606903\n",
      "train rmse 1.1848164905630405 test rmse 1.2986002098256055\n",
      "train rmse 1.1848173457829025 test rmse 1.2986017244966455\n",
      "train rmse 1.1848186034580486 test rmse 1.2986035145601424\n",
      "train rmse 1.1848196599041396 test rmse 1.298605258722202\n",
      "train rmse 1.1848209175768294 test rmse 1.298606727488435\n",
      "train rmse 1.18482192371402 test rmse 1.298608242151873\n",
      "train rmse 1.1848232316910905 test rmse 1.2986098945099724\n",
      "train rmse 1.184824640280167 test rmse 1.298611638663463\n",
      "train rmse 1.1848255458008317 test rmse 1.2986134287132938\n",
      "train rmse 1.1848267028540074 test rmse 1.2986151728620376\n",
      "train rmse 1.1848277592928766 test rmse 1.298616917008439\n",
      "train rmse 1.1848290672635051 test rmse 1.2986184775584966\n",
      "train rmse 1.184830073393775 test rmse 1.2986199922082298\n",
      "train rmse 1.1848314819747177 test rmse 1.2986217363481585\n",
      "train rmse 1.1848323874901536 test rmse 1.298623205095755\n",
      "train rmse 1.184833544536648 test rmse 1.2986249492313684\n",
      "train rmse 1.184834902807178 test rmse 1.29862660156821\n",
      "train rmse 1.1848360598512162 test rmse 1.2986281621066298\n",
      "train rmse 1.1848374181188628 test rmse 1.298629814439383\n",
      "train rmse 1.1848385751604447 test rmse 1.2986315126680779\n",
      "train rmse 1.1848396315887282 test rmse 1.2986329814046176\n",
      "train rmse 1.1848406377100271 test rmse 1.298634633731239\n",
      "train rmse 1.18484159352447 test rmse 1.2986362401578835\n",
      "train rmse 1.184842851173878 test rmse 1.2986378006847208\n",
      "train rmse 1.1848440082101541 test rmse 1.2986394071074474\n",
      "train rmse 1.184845316162845 test rmse 1.298640829937346\n",
      "train rmse 1.184846473196714 test rmse 1.2986420232773492\n",
      "train rmse 1.1848476302294535 test rmse 1.2986436755924664\n",
      "train rmse 1.184848988483836 test rmse 1.2986452361103689\n",
      "train rmse 1.1848501455141192 test rmse 1.2986462458562478\n",
      "train rmse 1.1848511516264901 test rmse 1.2986482653456501\n",
      "train rmse 1.1848518559046417 test rmse 1.298649779960641\n",
      "train rmse 1.1848529626266053 test rmse 1.2986513404712083\n",
      "train rmse 1.184854019042061 test rmse 1.2986529927744708\n",
      "train rmse 1.1848552766782798 test rmse 1.2986543696922506\n",
      "train rmse 1.1848565846185317 test rmse 1.2986559301973026\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_global_step()\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(cost, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(NB_EPOCHS):\n",
    "        _, train_pred, train_mse, reg, pen, train_cost = sess.run([train_op, pred, cost_l2, regularizer, penalty, cost], feed_dict={\n",
    "            user_batch: train['user'],\n",
    "            item_batch: train['item'],\n",
    "            rate_batch: train['rating']\n",
    "        })\n",
    "        test_pred, test_mse = sess.run([pred, cost_l2], feed_dict={\n",
    "            user_batch: test['user'],\n",
    "            item_batch: test['item'],\n",
    "            rate_batch: test['rating']\n",
    "        })\n",
    "        print('train rmse', train_mse ** 0.5, 'test rmse', test_mse ** 0.5)\n",
    "        # print('reg', reg, 'full cost', train_cost)\n",
    "        \n",
    "    #print(weights.eval(session=sess))\n",
    "    W_mat=weights.eval(session=sess)\n",
    "    alpha_vec= alpha.eval(session=sess)\n",
    "    beta_vec= beta.eval(session=sess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.6750633e-05  1.1477541e+00  2.8907898e-06  4.8404086e-06\n",
      "   3.4291856e-02  9.7424993e-07]\n",
      " [-6.1952232e-07  5.9558117e-01 -1.5323755e-08 -2.1215232e-08\n",
      "  -8.9905027e-04 -2.8798068e-08]\n",
      " [-5.5757242e-05  2.1133094e+00 -7.3574924e-06 -7.6904516e-06\n",
      "  -5.5326227e-02 -8.7951577e-07]\n",
      " ...\n",
      " [ 1.3119827e-06  1.9459037e+00 -2.2421301e-07 -8.5221046e-08\n",
      "   2.3196761e-03  7.1165225e-08]\n",
      " [ 5.5709603e-07  2.5175779e+00  1.7284140e-07 -2.4585785e-07\n",
      "  -2.9398920e-04  2.3925807e-07]\n",
      " [-4.6793176e-07  2.6858563e+00 -1.0248291e-07 -5.4735527e-10\n",
      "  -4.9463171e-04 -1.6811669e-08]]\n",
      "[ 0.0399156   7.96495    -0.78659195 -2.746602    2.6555805  -1.2326641 ]\n",
      "[-0.69059277  3.6026833  -1.6191064  -2.2190504   3.6800885  -3.942153  ]\n"
     ]
    }
   ],
   "source": [
    "print(W_mat)\n",
    "print(alpha_vec)\n",
    "print(beta_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b5a9f1d0140f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
