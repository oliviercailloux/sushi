{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USERS = 5000\n",
    "FEAT_USER = 3\n",
    "NB_ITEMS = 100\n",
    "FEAT_ITEM = 6\n",
    "\n",
    "NB_EPOCHS = 500\n",
    "LAMBDA_REG = 1e-5\n",
    "# LAMBDA_REG = 0\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv('data/sushi/sushi3.udata', sep='\\t', names=('uid', 'gender', 'age', 'time', 'old_prefecture', 'old_region', 'old_eastwest', 'prefecture', 'region', 'eastwest', 'same'))\n",
    "items = pd.read_csv('data/sushi/sushi3.idata', sep='\\t', names=('iid', 'name', 'style', 'major', 'minor', 'heaviness', 'frequency', 'price', 'popularity'))\n",
    "R = pd.read_csv('data/sushi/sushi3b.5000.10.score', sep=' ', header=None)\n",
    "triplets = []\n",
    "for i, line in enumerate(np.array(R)):\n",
    "    for j, v in enumerate(line):\n",
    "        if v != -1:\n",
    "            triplets.append((i, j, v))\n",
    "df_ratings = pd.DataFrame(triplets, columns=('user', 'item', 'rating'))\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "A = tf.constant(np.array(users[['age', 'gender', 'region']]).astype(np.float32))\n",
    "B = tf.constant(np.array(items[['heaviness', 'frequency', 'price', 'popularity', 'style', 'major']]).astype(np.float32))\n",
    "\n",
    "W_V = tf.get_variable('W_V', shape=[NB_ITEMS, FEAT_USER], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "W_U = tf.get_variable('W_U', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "M = tf.get_variable('M', shape=[FEAT_USER, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "user_bias = tf.get_variable(\"user_bias\", shape=[NB_USERS],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "item_bias = tf.get_variable(\"item_bias\", shape=[NB_ITEMS],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "rat_avg =  tf.get_variable(\"rat_avg\", shape=[1],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "weight_items = tf.nn.embedding_lookup(W_V, item_batch)\n",
    "weight_users = tf.nn.embedding_lookup(W_U, user_batch)\n",
    "\n",
    "bias_items = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "bias_users = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "\n",
    "feat_items = tf.nn.embedding_lookup(B, item_batch)\n",
    "feat_users = tf.nn.embedding_lookup(A, user_batch)\n",
    "\n",
    "pred = (tf.reduce_sum(tf.multiply(feat_users, weight_items), 1)\n",
    "        + tf.reduce_sum(tf.multiply(feat_items, weight_users), 1)\n",
    "        + bias_items\n",
    "        + bias_users)\n",
    "\n",
    "#pred = (tf.reduce_sum(tf.multiply(weight_users, weight_items), 1)\n",
    "#        + bias_items\n",
    "#        + bias_users)\n",
    "\n",
    "\n",
    "#pred = ( bias_items + bias_users)\n",
    "#pred = rat_avg\n",
    "\n",
    "# pred = (tf.reduce_sum(tf.multiply(tf.matmul(feat_users, M), feat_items), 1)\n",
    "#         + bias_items\n",
    "#         + bias_users)\n",
    "cost_l2 = tf.losses.mean_squared_error(rate_batch, pred)\n",
    "\n",
    "l2_user = tf.nn.l2_loss(weight_users)\n",
    "l2_item = tf.nn.l2_loss(weight_items)\n",
    "l2_bias_user = tf.nn.l2_loss(bias_users)\n",
    "l2_bias_item = tf.nn.l2_loss(bias_items)\n",
    "regularizer = tf.add(l2_user, l2_item)\n",
    "regularizer = tf.add(regularizer, l2_bias_user)\n",
    "regularizer = tf.add(regularizer, l2_bias_item)\n",
    "# regularizer = tf.nn.l2_loss(M)\n",
    "penalty = tf.constant(LAMBDA_REG, dtype=tf.float32, shape=[])\n",
    "cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 6.613511110244109 test rmse 5.8129410114953854\n",
      "train rmse 5.690195974719402 test rmse 5.08279457781001\n",
      "train rmse 4.868665835004437 test rmse 4.453771283283284\n",
      "train rmse 4.158459069154581 test rmse 3.92954795392452\n",
      "train rmse 3.5683466801994355 test rmse 3.5106454806743166\n",
      "train rmse 3.1028519647650903 test rmse 3.1921659240142874\n",
      "train rmse 2.7595864444995244 test rmse 2.96185803761307\n",
      "train rmse 2.5258527127937005 test rmse 2.802130763752682\n",
      "train rmse 2.379704183418568 test rmse 2.693699716333222\n",
      "train rmse 2.294761879396179 test rmse 2.6186983672448494\n",
      "train rmse 2.246025814189991 test rmse 2.5627771553325287\n",
      "train rmse 2.2138920645037565 test rmse 2.5155079500440105\n",
      "train rmse 2.1849936159158494 test rmse 2.4698334260132193\n",
      "train rmse 2.1512500455724433 test rmse 2.421666979316049\n",
      "train rmse 2.1089630855044637 test rmse 2.369416147738626\n",
      "train rmse 2.057671646244795 test rmse 2.313120990575507\n",
      "train rmse 1.9986586244844196 test rmse 2.2537298017750884\n",
      "train rmse 1.9339085794635005 test rmse 2.192671521044691\n",
      "train rmse 1.8657454025291649 test rmse 2.131413374649041\n",
      "train rmse 1.7965043058739267 test rmse 2.0710936026499382\n",
      "train rmse 1.728214767911413 test rmse 2.0125018623296045\n",
      "train rmse 1.6625130473607277 test rmse 1.956217278642576\n",
      "train rmse 1.6005935342921458 test rmse 1.9026338641959737\n",
      "train rmse 1.5430294749225422 test rmse 1.8519641424501891\n",
      "train rmse 1.4897368914364668 test rmse 1.8043242910597723\n",
      "train rmse 1.4402011703003328 test rmse 1.7598003083430065\n",
      "train rmse 1.3937715657024055 test rmse 1.7184631801919454\n",
      "train rmse 1.3498692572826811 test rmse 1.6803474726557224\n",
      "train rmse 1.3081487553786575 test rmse 1.6454287003423018\n",
      "train rmse 1.26856690386968 test rmse 1.6136790378608272\n",
      "train rmse 1.2313845178540077 test rmse 1.5851257953002562\n",
      "train rmse 1.1970470035951901 test rmse 1.5598315531924327\n",
      "train rmse 1.165976161065314 test rmse 1.5378144911761107\n",
      "train rmse 1.1384031525047493 test rmse 1.5189613509192879\n",
      "train rmse 1.1143287415458822 test rmse 1.5029112063059205\n",
      "train rmse 1.0934482158125995 test rmse 1.4890688917056878\n",
      "train rmse 1.075187329670638 test rmse 1.4767765112092786\n",
      "train rmse 1.0589112653721882 test rmse 1.4654152530984765\n",
      "train rmse 1.0440398584700772 test rmse 1.4544281397456809\n",
      "train rmse 1.0300400311366633 test rmse 1.443371763679495\n",
      "train rmse 1.0164942029683564 test rmse 1.4319475997575952\n",
      "train rmse 1.0031197283359703 test rmse 1.4199973483799637\n",
      "train rmse 0.9897413662993753 test rmse 1.407532763870532\n",
      "train rmse 0.9763585297242074 test rmse 1.3947475489998107\n",
      "train rmse 0.9631706279464809 test rmse 1.3819389385750556\n",
      "train rmse 0.9504561032067314 test rmse 1.3694293578870658\n",
      "train rmse 0.9384615417612155 test rmse 1.357507497159091\n",
      "train rmse 0.9273482633511397 test rmse 1.3463874483206844\n",
      "train rmse 0.9171664688844472 test rmse 1.3361928595377386\n",
      "train rmse 0.9078847347548034 test rmse 1.3269640616931135\n",
      "train rmse 0.8994401395747202 test rmse 1.3186803808411993\n",
      "train rmse 0.89177679573142 test rmse 1.3112902288384258\n",
      "train rmse 0.8848501674521099 test rmse 1.3047425492602587\n",
      "train rmse 0.8786239848937638 test rmse 1.2989986451151465\n",
      "train rmse 0.873051585754384 test rmse 1.294014176749467\n",
      "train rmse 0.8680583602012255 test rmse 1.2897126841137068\n",
      "train rmse 0.8635377114592296 test rmse 1.2859740742225128\n",
      "train rmse 0.8593774968891283 test rmse 1.2826543415439686\n",
      "train rmse 0.8554750555494104 test rmse 1.279601455422683\n",
      "train rmse 0.8517371532276398 test rmse 1.2766884338940216\n",
      "train rmse 0.8481001856075588 test rmse 1.2738183043100475\n",
      "train rmse 0.8445395554831975 test rmse 1.2709343771032042\n",
      "train rmse 0.8410669689389986 test rmse 1.2680281911327467\n",
      "train rmse 0.8377337186569408 test rmse 1.2651349284469675\n",
      "train rmse 0.8346017879350838 test rmse 1.2623085556667484\n",
      "train rmse 0.8317146236095874 test rmse 1.2596018130231537\n",
      "train rmse 0.8290800669189712 test rmse 1.2570493764382416\n",
      "train rmse 0.8266786504525468 test rmse 1.2546567008108827\n",
      "train rmse 0.8244768434653662 test rmse 1.252401048630964\n",
      "train rmse 0.8224424049827775 test rmse 1.250251840017605\n",
      "train rmse 0.8205491587448991 test rmse 1.248201363662515\n",
      "train rmse 0.8187710038614101 test rmse 1.2462748333169942\n",
      "train rmse 0.8170826131638215 test rmse 1.2445113798903222\n",
      "train rmse 0.8154655037512178 test rmse 1.2429340925965537\n",
      "train rmse 0.8139113492727234 test rmse 1.2415434889318446\n",
      "train rmse 0.8124193004732804 test rmse 1.2403204940572883\n",
      "train rmse 0.8109919785151088 test rmse 1.2392366977152396\n",
      "train rmse 0.8096290631483094 test rmse 1.2382586744879938\n",
      "train rmse 0.8083361892715288 test rmse 1.23735170342796\n",
      "train rmse 0.8071101117816147 test rmse 1.236484656285385\n",
      "train rmse 0.8059455148772432 test rmse 1.235635090132148\n",
      "train rmse 0.8048360742977969 test rmse 1.2347864841339338\n",
      "train rmse 0.8037754927488663 test rmse 1.2339275853009373\n",
      "train rmse 0.8027631842648156 test rmse 1.2330445471309424\n",
      "train rmse 0.8017984024534346 test rmse 1.2321302549535151\n",
      "train rmse 0.8008796445361491 test rmse 1.2311783455074046\n",
      "train rmse 0.8000047594167278 test rmse 1.230196580607289\n",
      "train rmse 0.7991716536948723 test rmse 1.2292069035210542\n",
      "train rmse 0.798378965020058 test rmse 1.2282399169893123\n",
      "train rmse 0.7976246467927917 test rmse 1.227321851354085\n",
      "train rmse 0.7969039088970585 test rmse 1.22647050646199\n",
      "train rmse 0.7962136609754634 test rmse 1.2256930239882027\n",
      "train rmse 0.7955516223378098 test rmse 1.224988522767777\n",
      "train rmse 0.7949171886259088 test rmse 1.2243507513709082\n",
      "train rmse 0.7943115140575774 test rmse 1.2237785964913284\n",
      "train rmse 0.7937317733474201 test rmse 1.2232737581435609\n",
      "train rmse 0.7931734394044306 test rmse 1.2228382206743276\n",
      "train rmse 0.7926365950644262 test rmse 1.2224685963690771\n",
      "train rmse 0.7921214345135716 test rmse 1.222155873828986\n",
      "train rmse 0.7916289412561341 test rmse 1.2218880479511078\n",
      "train rmse 0.7911580652158179 test rmse 1.2216495842582868\n",
      "train rmse 0.79070692278754 test rmse 1.221420492338328\n",
      "train rmse 0.7902725685683917 test rmse 1.2211811075916452\n",
      "train rmse 0.7898557848819999 test rmse 1.220916582899373\n",
      "train rmse 0.7894547120332692 test rmse 1.2206220676213058\n",
      "train rmse 0.7890683919856598 test rmse 1.2202979796421252\n",
      "train rmse 0.7886950704367085 test rmse 1.2199477154831568\n",
      "train rmse 0.7883343500072975 test rmse 1.2195743804870638\n",
      "train rmse 0.7879869665989894 test rmse 1.2191846023738684\n",
      "train rmse 0.787651878425016 test rmse 1.2187904938059486\n",
      "train rmse 0.7873280791676098 test rmse 1.218405552631382\n",
      "train rmse 0.7870145603369566 test rmse 1.2180408958027482\n",
      "train rmse 0.7867111826952266 test rmse 1.217700212689231\n",
      "train rmse 0.7864193980366373 test rmse 1.217382984843763\n",
      "train rmse 0.7861381577967537 test rmse 1.2170879083321289\n",
      "train rmse 0.7858654254604945 test rmse 1.2168129419334504\n",
      "train rmse 0.7856029169928748 test rmse 1.216556286486072\n",
      "train rmse 0.785349731898552 test rmse 1.2163172185186986\n",
      "train rmse 0.7851056134800796 test rmse 1.2160972187963717\n",
      "train rmse 0.7848689754137611 test rmse 1.2158961996271813\n",
      "train rmse 0.7846428250531637 test rmse 1.2157133369416484\n",
      "train rmse 0.7844244360068062 test rmse 1.2155451083959612\n",
      "train rmse 0.7842135107363605 test rmse 1.2153866159052111\n",
      "train rmse 0.7840089148846598 test rmse 1.2152332527844585\n",
      "train rmse 0.7838108815422167 test rmse 1.2150816362529286\n",
      "train rmse 0.78361979600058 test rmse 1.2149312273035588\n",
      "train rmse 0.783433685232582 test rmse 1.214782909574801\n",
      "train rmse 0.7832530093775997 test rmse 1.2146381559916701\n",
      "train rmse 0.7830786094716549 test rmse 1.2144974095272552\n",
      "train rmse 0.782908814800113 test rmse 1.2143586591641091\n",
      "train rmse 0.7827448467327459 test rmse 1.2142234273370534\n",
      "train rmse 0.7825865566046599 test rmse 1.2140933844174098\n",
      "train rmse 0.7824331098979251 test rmse 1.2139681883796487\n",
      "train rmse 0.7822833665660587 test rmse 1.213848626385395\n",
      "train rmse 0.7821401103009429 test rmse 1.2137321955654736\n",
      "train rmse 0.7820002958438635 test rmse 1.2136189459342175\n",
      "train rmse 0.7818649542000797 test rmse 1.213507061029677\n",
      "train rmse 0.781733744582499 test rmse 1.2133979657708316\n",
      "train rmse 0.7816064784242232 test rmse 1.2132897941057614\n",
      "train rmse 0.7814825474821531 test rmse 1.2131838236819843\n",
      "train rmse 0.7813615719281106 test rmse 1.2130798585365066\n",
      "train rmse 0.7812448882889411 test rmse 1.2129770146807897\n",
      "train rmse 0.7811312394475171 test rmse 1.2128740638179678\n",
      "train rmse 0.7810202451467968 test rmse 1.212772382049588\n",
      "train rmse 0.7809138910175524 test rmse 1.212673788298977\n",
      "train rmse 0.7808090491438836 test rmse 1.212580298684861\n",
      "train rmse 0.7807082777598612 test rmse 1.2124945689600801\n",
      "train rmse 0.7806097840657956 test rmse 1.2124163057995903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.7805141034852867 test rmse 1.2123462972848993\n",
      "train rmse 0.7804208551780888 test rmse 1.2122830206617403\n",
      "train rmse 0.7803317204599552 test rmse 1.2122247560484136\n",
      "train rmse 0.7802440270143626 test rmse 1.212169832325573\n",
      "train rmse 0.780160067346678 test rmse 1.2121166272058617\n",
      "train rmse 0.7800793460078036 test rmse 1.2120659769091269\n",
      "train rmse 0.7799999153919663 test rmse 1.2120154720300842\n",
      "train rmse 0.7799227311913843 test rmse 1.2119656535686119\n",
      "train rmse 0.7798482526596832 test rmse 1.2119149477799447\n",
      "train rmse 0.7797756015333149 test rmse 1.2118634037369957\n",
      "train rmse 0.7797041667617006 test rmse 1.211811267264871\n",
      "train rmse 0.7796356306233545 test rmse 1.2117589809839662\n",
      "train rmse 0.7795696115908755 test rmse 1.211707381115821\n",
      "train rmse 0.7795047339392909 test rmse 1.211657894335391\n",
      "train rmse 0.7794421450180041 test rmse 1.2116110620416194\n",
      "train rmse 0.7793813865173579 test rmse 1.2115670813276345\n",
      "train rmse 0.7793223823828177 test rmse 1.2115260508999703\n",
      "train rmse 0.7792652859894256 test rmse 1.2114888566496915\n",
      "train rmse 0.7792099065223352 test rmse 1.2114541213054475\n",
      "train rmse 0.7791557853537309 test rmse 1.2114201722020994\n",
      "train rmse 0.7791036495353878 test rmse 1.2113878458672636\n",
      "train rmse 0.779052734375 test rmse 1.2113563551530229\n",
      "train rmse 0.7790039582794658 test rmse 1.2113249128262795\n",
      "train rmse 0.7789561356129795 test rmse 1.2112940601724445\n",
      "train rmse 0.7789090369814616 test rmse 1.2112631575240813\n",
      "train rmse 0.778863963487747 test rmse 1.2112332382853275\n",
      "train rmse 0.7788194231099556 test rmse 1.2112040564737458\n",
      "train rmse 0.7787765257161586 test rmse 1.2111764487501477\n",
      "train rmse 0.7787347740656512 test rmse 1.2111500215161333\n",
      "train rmse 0.7786940535261841 test rmse 1.2111257099201107\n",
      "train rmse 0.778654555630179 test rmse 1.2111021852796267\n",
      "train rmse 0.7786158212514814 test rmse 1.2110798413695238\n",
      "train rmse 0.778578769173021 test rmse 1.2110595149434427\n",
      "train rmse 0.7785425192039123 test rmse 1.2110404678374667\n",
      "train rmse 0.7785074159885151 test rmse 1.2110219618351763\n",
      "train rmse 0.7784726940209937 test rmse 1.2110042922765263\n",
      "train rmse 0.7784393487553827 test rmse 1.2109871146589948\n",
      "train rmse 0.7784064614973608 test rmse 1.210970429003516\n",
      "train rmse 0.7783749512145108 test rmse 1.2109542353304261\n",
      "train rmse 0.7783445500485248 test rmse 1.210938287550147\n",
      "train rmse 0.7783141094041942 test rmse 1.2109229794523815\n",
      "train rmse 0.7782848163393598 test rmse 1.2109077696073147\n",
      "train rmse 0.7782563263402862 test rmse 1.210893347150575\n",
      "train rmse 0.7782295968714019 test rmse 1.2108793183162396\n",
      "train rmse 0.7782022537427241 test rmse 1.2108657815676247\n",
      "train rmse 0.7781761351792525 test rmse 1.210853573751825\n",
      "train rmse 0.7781510881087523 test rmse 1.2108409227805903\n",
      "train rmse 0.7781264615332424 test rmse 1.2108288623931176\n",
      "train rmse 0.7781022554926483 test rmse 1.2108178356480186\n",
      "train rmse 0.7780787381434185 test rmse 1.2108073995289124\n",
      "train rmse 0.7780555265121639 test rmse 1.2107971110026863\n",
      "train rmse 0.7780342294228613 test rmse 1.2107875115811977\n",
      "train rmse 0.7780122805535714 test rmse 1.2107781089971377\n",
      "train rmse 0.7779909439736618 test rmse 1.2107693463137992\n",
      "train rmse 0.7779707177346638 test rmse 1.2107607312543107\n",
      "train rmse 0.7779503377347216 test rmse 1.210752411510107\n",
      "train rmse 0.7779314129709948 test rmse 1.2107446824657457\n",
      "train rmse 0.7779130624062055 test rmse 1.2107372487523282\n",
      "train rmse 0.7778945581624636 test rmse 1.2107304549875664\n",
      "train rmse 0.7778766664771892 test rmse 1.210723956568209\n",
      "train rmse 0.7778595789588854 test rmse 1.210717359652276\n",
      "train rmse 0.777843027462194 test rmse 1.2107120427087967\n",
      "train rmse 0.7778262840389136 test rmse 1.210706873435806\n",
      "train rmse 0.7778104215159679 test rmse 1.2107016056777669\n",
      "train rmse 0.7777945586695182 test rmse 1.2106969286771216\n",
      "train rmse 0.7777799982841523 test rmse 1.2106929901362489\n",
      "train rmse 0.7777650927652117 test rmse 1.2106879684780538\n",
      "train rmse 0.7777504168723598 test rmse 1.2106849653195726\n",
      "train rmse 0.7777368519627181 test rmse 1.2106784666560457\n",
      "train rmse 0.7777230568967077 test rmse 1.210677285077111\n",
      "train rmse 0.7777106411280821 test rmse 1.2106701955792871\n",
      "train rmse 0.7776971904883908 test rmse 1.2106684231983444\n",
      "train rmse 0.7776845443756488 test rmse 1.2106636968364783\n",
      "train rmse 0.7776720513472011 test rmse 1.2106595120215085\n",
      "train rmse 0.7776602096104237 test rmse 1.2106581827243141\n",
      "train rmse 0.7776490958422978 test rmse 1.210654096357133\n",
      "train rmse 0.7776369471623307 test rmse 1.2106511915817135\n",
      "train rmse 0.7776262546339707 test rmse 1.2106501084433323\n",
      "train rmse 0.7776156769343704 test rmse 1.2106465143863072\n",
      "train rmse 0.7776046775071821 test rmse 1.2106445450354673\n",
      "train rmse 0.7775950959978173 test rmse 1.2106439542295904\n",
      "train rmse 0.7775852077563608 test rmse 1.21064050785623\n",
      "train rmse 0.7775749744438459 test rmse 1.2106389816020358\n",
      "train rmse 0.7775661207951414 test rmse 1.210638243091251\n",
      "train rmse 0.7775568837641486 test rmse 1.21063563368287\n",
      "train rmse 0.7775479915808579 test rmse 1.2106340581882946\n",
      "train rmse 0.7775392526119932 test rmse 1.2106335166115607\n",
      "train rmse 0.7775307818510606 test rmse 1.2106311533647958\n",
      "train rmse 0.7775228092858809 test rmse 1.2106297748020525\n",
      "train rmse 0.7775147599784878 test rmse 1.2106294301611213\n",
      "train rmse 0.7775069789021293 test rmse 1.2106278054239816\n",
      "train rmse 0.7774992360788989 test rmse 1.2106271161409001\n",
      "train rmse 0.7774912248588491 test rmse 1.2106270176718563\n",
      "train rmse 0.7774847851580062 test rmse 1.2106252452277002\n",
      "train rmse 0.7774775787628692 test rmse 1.2106247528816407\n",
      "train rmse 0.7774709089546582 test rmse 1.2106246544124049\n",
      "train rmse 0.7774643157546725 test rmse 1.210623522015616\n",
      "train rmse 0.7774574925004654 test rmse 1.210623078903541\n",
      "train rmse 0.7774512825204274 test rmse 1.210623226607584\n",
      "train rmse 0.7774446508203084 test rmse 1.2106220942094599\n",
      "train rmse 0.7774390157474278 test rmse 1.2106217495663423\n",
      "train rmse 0.7774332656309563 test rmse 1.2106220449747347\n",
      "train rmse 0.7774273237992557 test rmse 1.2106217495663423\n",
      "train rmse 0.7774219186096823 test rmse 1.2106209618102763\n",
      "train rmse 0.7774162450369382 test rmse 1.2106214541578775\n",
      "train rmse 0.7774115681417856 test rmse 1.210620863340732\n",
      "train rmse 0.7774061628426654 test rmse 1.2106209125755052\n",
      "train rmse 0.7774012175360808 test rmse 1.2106209125755052\n",
      "train rmse 0.7773961955258819 test rmse 1.2106211095145778\n",
      "train rmse 0.7773915185101087 test rmse 1.2106211095145778\n",
      "train rmse 0.7773869948123452 test rmse 1.2106213556883734\n",
      "train rmse 0.7773825477617665 test rmse 1.2106213064536182\n",
      "train rmse 0.7773778706638829 test rmse 1.2106217495663423\n",
      "train rmse 0.7773736152470976 test rmse 1.2106217495663423\n",
      "train rmse 0.7773690531077335 test rmse 1.2106219957400077\n",
      "train rmse 0.7773647593050507 test rmse 1.210622192678904\n",
      "train rmse 0.7773612705729074 test rmse 1.2106221434441828\n",
      "train rmse 0.777357360107279 test rmse 1.2106227342607037\n",
      "train rmse 0.7773538330037605 test rmse 1.2106225865566005\n",
      "train rmse 0.7773502292075494 test rmse 1.2106235712502813\n",
      "train rmse 0.777346625394631 test rmse 1.2106230296688558\n",
      "train rmse 0.7773432132584909 test rmse 1.210624703647024\n",
      "train rmse 0.7773395710741747 test rmse 1.2106235712502813\n",
      "train rmse 0.7773360822289851 test rmse 1.2106250975239035\n",
      "train rmse 0.7773333218129039 test rmse 1.2106253436968881\n",
      "train rmse 0.7773297946003127 test rmse 1.2106226850260047\n",
      "train rmse 0.7773270725013925 test rmse 1.2106303163804601\n",
      "train rmse 0.777324043675897 test rmse 1.2106193370617715\n",
      "train rmse 0.7773207081202408 test rmse 1.2106331227374203\n",
      "train rmse 0.7773186377681515 test rmse 1.2106229804341688\n",
      "train rmse 0.7773151488290062 test rmse 1.2106265253265178\n",
      "train rmse 0.7773123116581563 test rmse 1.21063179341125\n",
      "train rmse 0.7773100495820859 test rmse 1.2106230296688558\n",
      "train rmse 0.7773073274140196 test rmse 1.2106309071929924\n",
      "train rmse 0.7773049503017139 test rmse 1.2106302671460696\n",
      "train rmse 0.7773025731821385 test rmse 1.2106251467585043\n",
      "train rmse 0.777300272736918 test rmse 1.2106338120170819\n",
      "train rmse 0.7772983556940336 test rmse 1.2106295286299686\n",
      "train rmse 0.7772959402132675 test rmse 1.2106287901134174\n",
      "train rmse 0.7772933330192553 test rmse 1.2106337627828334\n",
      "train rmse 0.777291837712861 test rmse 1.2106305625523837\n",
      "train rmse 0.777289038798289 test rmse 1.2106289378167638\n",
      "train rmse 0.7772872750935192 test rmse 1.2106375045800157\n",
      "train rmse 0.7772854347016668 test rmse 1.2106262299192185\n",
      "train rmse 0.7772833259139788 test rmse 1.2106385384956189\n",
      "train rmse 0.777281140437065 test rmse 1.2106269192028047\n",
      "train rmse 0.7772801052090137 test rmse 1.2106444958016551\n",
      "train rmse 0.7772774979618866 test rmse 1.2106177615459877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.7772772295683035 test rmse 1.2106580350245357\n",
      "train rmse 0.7772736254169238 test rmse 1.210612099519214\n",
      "train rmse 0.777272973600528 test rmse 1.2106499115089773\n",
      "train rmse 0.7772700595911335 test rmse 1.2106367168342016\n",
      "train rmse 0.7772687176094488 test rmse 1.210623374311609\n",
      "train rmse 0.777267605680012 test rmse 1.2106550317918015\n",
      "train rmse 0.7772651134185994 test rmse 1.210625934511847\n",
      "train rmse 0.7772646916504928 test rmse 1.2106369630048235\n",
      "train rmse 0.777262889547823 test rmse 1.2106496161073843\n",
      "train rmse 0.7772614325255883 test rmse 1.2106246544124049\n",
      "train rmse 0.7772608190417251 test rmse 1.2106454312437416\n",
      "train rmse 0.7772579816725589 test rmse 1.2106432157218392\n",
      "train rmse 0.7772569847566092 test rmse 1.210628445472206\n",
      "train rmse 0.7772561412113453 test rmse 1.210649419172949\n",
      "train rmse 0.7772545691472743 test rmse 1.2106378984527306\n",
      "train rmse 0.7772542240596135 test rmse 1.2106335166115607\n",
      "train rmse 0.7772534188544761 test rmse 1.2106499115089773\n",
      "train rmse 0.7772516550688793 test rmse 1.2106360275561938\n",
      "train rmse 0.7772511566070068 test rmse 1.2106389323679974\n",
      "train rmse 0.777249584532854 test rmse 1.2106492222384817\n",
      "train rmse 0.7772483192025677 test rmse 1.2106343535936837\n",
      "train rmse 0.777247513991313 test rmse 1.2106445450354673\n",
      "train rmse 0.7772459802532821 test rmse 1.2106453820099656\n",
      "train rmse 0.7772447149171281 test rmse 1.2106380461549655\n",
      "train rmse 0.7772446765735761 test rmse 1.2106450866072676\n",
      "train rmse 0.7772428360808569 test rmse 1.2106479913973351\n",
      "train rmse 0.7772424909879868 test rmse 1.2106339597198155\n",
      "train rmse 0.7772416090832894 test rmse 1.2106551302585666\n",
      "train rmse 0.7772410722712445 test rmse 1.210634501296351\n",
      "train rmse 0.7772401520211623 test rmse 1.210651683917021\n",
      "train rmse 0.777240650490092 test rmse 1.21063972011237\n",
      "train rmse 0.7772382348299912 test rmse 1.2106498622753834\n",
      "train rmse 0.7772385032370399 test rmse 1.210641000195882\n",
      "train rmse 0.7772371228569452 test rmse 1.2106465143863072\n",
      "train rmse 0.7772360108823083 test rmse 1.2106473513594442\n",
      "train rmse 0.777236394322018 test rmse 1.210639917048383\n",
      "train rmse 0.7772346688418348 test rmse 1.2106531609217417\n",
      "train rmse 0.7772348605620443 test rmse 1.2106402616863288\n",
      "train rmse 0.7772337485841707 test rmse 1.2106463666851055\n",
      "train rmse 0.7772330583902076 test rmse 1.2106506500126442\n",
      "train rmse 0.7772328666695536 test rmse 1.2106387354318242\n",
      "train rmse 0.7772316013120507 test rmse 1.2106526193535534\n",
      "train rmse 0.7772312945584063 test rmse 1.210644348100207\n",
      "train rmse 0.7772304893295141 test rmse 1.2106434618911397\n",
      "train rmse 0.7772294540339982 test rmse 1.2106525701200697\n",
      "train rmse 0.7772296074111987 test rmse 1.2106411971316868\n",
      "train rmse 0.7772289172135582 test rmse 1.2106489268367207\n",
      "train rmse 0.7772285337701598 test rmse 1.2106496653409882\n",
      "train rmse 0.7772276518496255 test rmse 1.2106415417692682\n",
      "train rmse 0.7772269999946743 test rmse 1.2106529147544134\n",
      "train rmse 0.7772267699280904 test rmse 1.2106428218508543\n",
      "train rmse 0.7772258113165912 test rmse 1.2106486314348874\n",
      "train rmse 0.7772258496610739 test rmse 1.2106477452289555\n",
      "train rmse 0.7772252744936352 test rmse 1.2106476467615896\n",
      "train rmse 0.7772247376703084 test rmse 1.2106436095926958\n",
      "train rmse 0.7772246226366901 test rmse 1.2106569518922774\n",
      "train rmse 0.7772240858129131 test rmse 1.2106300702084867\n",
      "train rmse 0.7772240858129131 test rmse 1.210674084961706\n",
      "train rmse 0.777223932434623 test rmse 1.2106125918704196\n",
      "train rmse 0.7772231271981034 test rmse 1.2106946147859114\n",
      "train rmse 0.7772233189211599 test rmse 1.2106013662131714\n",
      "train rmse 0.7772226670625747 test rmse 1.210688017710098\n",
      "train rmse 0.7772228587857447 test rmse 1.210631251833503\n",
      "train rmse 0.7772214783778648 test rmse 1.2106371107071727\n",
      "train rmse 0.7772212866543543 test rmse 1.2106789589802613\n",
      "train rmse 0.7772217851353833 test rmse 1.2106146597432959\n",
      "train rmse 0.7772207114835388 test rmse 1.210667684705519\n",
      "train rmse 0.777220749828273 test rmse 1.2106513392823266\n",
      "train rmse 0.77722025134658 test rmse 1.210624506708536\n",
      "train rmse 0.7772196761749983 test rmse 1.210675709636738\n",
      "train rmse 0.7772199445884561 test rmse 1.2106322365201356\n",
      "train rmse 0.7772187558995824 test rmse 1.2106422802780408\n",
      "train rmse 0.7772194844510433 test rmse 1.2106677339383882\n",
      "train rmse 0.7772194461062466 test rmse 1.2106258852772782\n",
      "train rmse 0.7772202896913369 test rmse 1.2106565580257613\n",
      "train rmse 0.7772190626581755 test rmse 1.2106548840916387\n",
      "train rmse 0.7772195227958381 test rmse 1.210628445472206\n",
      "train rmse 0.7772170303802406 test rmse 1.2106639430016144\n",
      "train rmse 0.7772171070700705 test rmse 1.2106428218508543\n",
      "train rmse 0.7772175672088908 test rmse 1.2106376522822988\n",
      "train rmse 0.7772164168613291 test rmse 1.2106621706115182\n",
      "train rmse 0.7772167619657765 test rmse 1.2106362244928075\n",
      "train rmse 0.7772159950667966 test rmse 1.2106459235813916\n",
      "train rmse 0.7772157649969552 test rmse 1.2106571488254874\n",
      "train rmse 0.7772167619657765 test rmse 1.210632679628859\n",
      "train rmse 0.777215266512065 test rmse 1.210654588691259\n",
      "train rmse 0.777215496582054 test rmse 1.2106467113212152\n",
      "train rmse 0.7772155732720353 test rmse 1.2106402616863288\n",
      "train rmse 0.7772153432020689 test rmse 1.2106496653409882\n",
      "train rmse 0.7772149214069537 test rmse 1.2106506992462058\n",
      "train rmse 0.7772158800318845 test rmse 1.210630808724257\n",
      "train rmse 0.7772146529917612 test rmse 1.2106660600197179\n",
      "train rmse 0.7772180273474387 test rmse 1.2106278054239816\n",
      "train rmse 0.7772147680268551 test rmse 1.2106541455905546\n",
      "train rmse 0.7772171070700705 test rmse 1.210646859022375\n",
      "train rmse 0.7772141928512155 test rmse 1.2106378492186483\n",
      "train rmse 0.7772146529917612 test rmse 1.2106521270186263\n",
      "train rmse 0.7772135793300642 test rmse 1.2106431664879729\n",
      "train rmse 0.7772138094005527 test rmse 1.2106394739423085\n",
      "train rmse 0.7772141545061577 test rmse 1.2106521270186263\n",
      "train rmse 0.7772129274633103 test rmse 1.210638243091251\n",
      "train rmse 0.7772134259497008 test rmse 1.2106445450354673\n",
      "train rmse 0.7772127740828182 test rmse 1.2106490253039823\n",
      "train rmse 0.7772126973925607 test rmse 1.2106359783220353\n",
      "train rmse 0.7772129274633103 test rmse 1.2106489760703525\n",
      "train rmse 0.7772118154540565 test rmse 1.2106431664879729\n",
      "train rmse 0.777212083870229 test rmse 1.2106387354318242\n",
      "train rmse 0.7772120071799035 test rmse 1.2106500592097467\n",
      "train rmse 0.7772112786214335 test rmse 1.2106377507504775\n",
      "train rmse 0.7772118154540565 test rmse 1.2106430187863626\n",
      "train rmse 0.7772116620733449 test rmse 1.2106470067235167\n",
      "train rmse 0.7772115470377914 test rmse 1.2106357813853814\n",
      "train rmse 0.7772115470377914 test rmse 1.2106470067235167\n",
      "train rmse 0.777211125240616 test rmse 1.21064050785623\n",
      "train rmse 0.7772108184788901 test rmse 1.2106386369637254\n",
      "train rmse 0.7772113936570267 test rmse 1.2106464651525752\n",
      "train rmse 0.7772108184788901 test rmse 1.210638243091251\n",
      "train rmse 0.7772115470377914 test rmse 1.2106411971316868\n",
      "train rmse 0.7772105500622806 test rmse 1.2106474005931402\n",
      "train rmse 0.7772127740828182 test rmse 1.2106335658458192\n",
      "train rmse 0.7772108568241124 test rmse 1.2106493699393353\n",
      "train rmse 0.7772127740828182 test rmse 1.2106373568777147\n",
      "train rmse 0.7772096681213396 test rmse 1.2106381446231123\n",
      "train rmse 0.7772098598477163 test rmse 1.2106471544246407\n",
      "train rmse 0.7772116620733449 test rmse 1.2106337135485827\n",
      "train rmse 0.7772098215024448 test rmse 1.2106427233830879\n",
      "train rmse 0.7772101666098206 test rmse 1.210641492535334\n",
      "train rmse 0.7772101666098206 test rmse 1.2106345997647863\n",
      "train rmse 0.7772093230137426 test rmse 1.2106445450354673\n",
      "train rmse 0.7772101666098206 test rmse 1.21063676606833\n",
      "train rmse 0.7772092846684445 test rmse 1.2106378492186483\n",
      "train rmse 0.7772089779059922 test rmse 1.2106426249153137\n",
      "train rmse 0.7772099748835196 test rmse 1.2106351413410354\n",
      "train rmse 0.777209169632539 test rmse 1.2106396216443513\n",
      "train rmse 0.777209092941926 test rmse 1.2106387354318242\n",
      "train rmse 0.7772099365382538 test rmse 1.2106360275561938\n",
      "train rmse 0.777209169632539 test rmse 1.2106389816020358\n",
      "train rmse 0.7772089779059922 test rmse 1.2106364706635295\n",
      "train rmse 0.7772098215024448 test rmse 1.2106376030482064\n",
      "train rmse 0.7772088628700413 test rmse 1.2106368645365808\n",
      "train rmse 0.7772087478340735 test rmse 1.2106368645365808\n",
      "train rmse 0.7772096681213396 test rmse 1.2106379969208894\n",
      "train rmse 0.7772085944527565 test rmse 1.2106353382777935\n",
      "train rmse 0.7772089012153601 test rmse 1.2106378984527306\n",
      "train rmse 0.7772092079778428 test rmse 1.2106359783220353\n",
      "train rmse 0.7772085561074225 test rmse 1.210634205890998\n",
      "train rmse 0.7772087478340735 test rmse 1.2106382923253172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.777209092941926 test rmse 1.2106330735031436\n",
      "train rmse 0.7772082876900317 test rmse 1.2106369630048235\n",
      "train rmse 0.7772092846684445 test rmse 1.2106351413410354\n",
      "train rmse 0.777208939560677 test rmse 1.210634501296351\n",
      "train rmse 0.777209169632539 test rmse 1.2106349444042457\n",
      "train rmse 0.7772088245247206 test rmse 1.2106342551252287\n",
      "train rmse 0.7772085561074225 test rmse 1.2106340581882946\n",
      "train rmse 0.7772084410714092 test rmse 1.2106338612513285\n",
      "train rmse 0.7772082493446826 test rmse 1.2106346489990008\n",
      "train rmse 0.777209553085491 test rmse 1.2106322365201356\n",
      "train rmse 0.7772078658910874 test rmse 1.2106357321512131\n",
      "train rmse 0.7772099365382538 test rmse 1.21063179341125\n",
      "train rmse 0.7772081343086239 test rmse 1.21063563368287\n",
      "train rmse 0.7772112019310286 test rmse 1.2106313010678538\n",
      "train rmse 0.777209169632539 test rmse 1.2106360275561938\n",
      "train rmse 0.7772115470377914 test rmse 1.210630661021139\n",
      "train rmse 0.7772074057465235 test rmse 1.2106338612513285\n",
      "train rmse 0.7772086711434187 test rmse 1.2106320395828731\n",
      "train rmse 0.7772088628700413 test rmse 1.210632433457366\n",
      "train rmse 0.7772087478340735 test rmse 1.2106313010678538\n",
      "train rmse 0.7772096681213396 test rmse 1.2106347474674237\n",
      "train rmse 0.7772077892003457 test rmse 1.2106262791537734\n",
      "train rmse 0.7772085561074225 test rmse 1.2106395231763247\n",
      "train rmse 0.7772098598477163 test rmse 1.2106223403830552\n",
      "train rmse 0.7772082876900317 test rmse 1.2106415417692682\n",
      "train rmse 0.7772092463231446 test rmse 1.2106209125755052\n",
      "train rmse 0.7772085944527565 test rmse 1.2106418864067514\n",
      "train rmse 0.7772097831571714 test rmse 1.2106220942094599\n",
      "train rmse 0.7772087478340735 test rmse 1.2106361260245047\n",
      "train rmse 0.7772087861793979 test rmse 1.2106297255676397\n",
      "train rmse 0.7772075974734582 test rmse 1.2106255406352397\n",
      "train rmse 0.7772092463231446 test rmse 1.2106394247082903\n",
      "train rmse 0.7772084794167489 test rmse 1.2106192878269344\n",
      "train rmse 0.7772084027260676 test rmse 1.2106383415593815\n",
      "train rmse 0.7772086711434187 test rmse 1.210624605177784\n",
      "train rmse 0.7772086327980885 test rmse 1.2106291839889676\n",
      "train rmse 0.7772078275457175 test rmse 1.2106314487708936\n",
      "train rmse 0.7772101666098206 test rmse 1.2106283470032704\n",
      "train rmse 0.7772073674011308 test rmse 1.2106249498200887\n",
      "train rmse 0.7772107801336658 test rmse 1.2106380461549655\n",
      "train rmse 0.7772079042364555 test rmse 1.2106142658630203\n",
      "train rmse 0.777208939560677 test rmse 1.21064370806039\n",
      "train rmse 0.7772092463231446 test rmse 1.210613921217674\n",
      "train rmse 0.777207750854972 test rmse 1.2106384892615625\n",
      "train rmse 0.7772098598477163 test rmse 1.2106206171668363\n",
      "train rmse 0.7772082876900317 test rmse 1.2106332212059674\n",
      "train rmse 0.7772082493446826 test rmse 1.2106193862966068\n",
      "train rmse 0.777209553085491 test rmse 1.210640754026081\n",
      "train rmse 0.7772069072562718 test rmse 1.2106073729374534\n",
      "train rmse 0.7772110868954069 test rmse 1.2106520777851226\n",
      "train rmse 0.7772071756741394 test rmse 1.2106019570398323\n",
      "train rmse 0.7772109335145515 test rmse 1.2106467113212152\n",
      "train rmse 0.7772079042364555 test rmse 1.2106152997984703\n",
      "train rmse 0.7772102049550751 test rmse 1.21062869164451\n",
      "train rmse 0.777208939560677 test rmse 1.210629676333225\n",
      "train rmse 0.7772087861793979 test rmse 1.210621700331603\n",
      "train rmse 0.7772087861793979 test rmse 1.2106270669063792\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_global_step()\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(cost, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(NB_EPOCHS):\n",
    "        _, train_pred, train_mse, reg, pen, train_cost = sess.run([train_op, pred, cost_l2, regularizer, penalty, cost], feed_dict={\n",
    "            user_batch: train['user'],\n",
    "            item_batch: train['item'],\n",
    "            rate_batch: train['rating']\n",
    "        })\n",
    "        test_pred, test_mse = sess.run([pred, cost_l2], feed_dict={\n",
    "            user_batch: test['user'],\n",
    "            item_batch: test['item'],\n",
    "            rate_batch: test['rating']\n",
    "        })\n",
    "        print('train rmse', train_mse ** 0.5, 'test rmse', test_mse ** 0.5)\n",
    "        # print('reg', reg, 'full cost', train_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
