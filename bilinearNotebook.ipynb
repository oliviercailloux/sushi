{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_USERS = 5000\n",
    "FEAT_USER = 3\n",
    "NB_ITEMS = 100\n",
    "FEAT_ITEM = 6\n",
    "\n",
    "NB_EPOCHS = 500\n",
    "LAMBDA_REG = 1e-5\n",
    "# LAMBDA_REG = 0\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv('data/sushi/sushi3.udata', sep='\\t', names=('uid', 'gender', 'age', 'time', 'old_prefecture', 'old_region', 'old_eastwest', 'prefecture', 'region', 'eastwest', 'same'))\n",
    "items = pd.read_csv('data/sushi/sushi3.idata', sep='\\t', names=('iid', 'name', 'style', 'major', 'minor', 'heaviness', 'frequency', 'price', 'popularity'))\n",
    "R = pd.read_csv('data/sushi/sushi3b.5000.10.score', sep=' ', header=None)\n",
    "triplets = []\n",
    "for i, line in enumerate(np.array(R)):\n",
    "    for j, v in enumerate(line):\n",
    "        if v != -1:\n",
    "            triplets.append((i, j, v))\n",
    "df_ratings = pd.DataFrame(triplets, columns=('user', 'item', 'rating'))\n",
    "train, test = train_test_split(df_ratings, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "A = tf.constant(np.array(users[['age', 'gender', 'region']]).astype(np.float32))\n",
    "B = tf.constant(np.array(items[['heaviness', 'frequency', 'price', 'popularity', 'style', 'major']]).astype(np.float32))\n",
    "\n",
    "W_V = tf.get_variable('W_V', shape=[NB_ITEMS, FEAT_USER], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "W_U = tf.get_variable('W_U', shape=[NB_USERS, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "M = tf.get_variable('M', shape=[FEAT_USER, FEAT_ITEM], dtype=np.float32, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "user_bias = tf.get_variable(\"user_bias\", shape=[NB_USERS],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "item_bias = tf.get_variable(\"item_bias\", shape=[NB_ITEMS],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None])\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "weight_items = tf.nn.embedding_lookup(W_V, item_batch)\n",
    "weight_users = tf.nn.embedding_lookup(W_U, user_batch)\n",
    "\n",
    "bias_items = tf.nn.embedding_lookup(item_bias, item_batch)\n",
    "bias_users = tf.nn.embedding_lookup(user_bias, user_batch)\n",
    "\n",
    "feat_items = tf.nn.embedding_lookup(B, item_batch)\n",
    "feat_users = tf.nn.embedding_lookup(A, user_batch)\n",
    "\n",
    "pred = (tf.reduce_sum(tf.multiply(feat_users, weight_items), 1)\n",
    "        + tf.reduce_sum(tf.multiply(feat_items, weight_users), 1)\n",
    "        + bias_items\n",
    "        + bias_users)\n",
    "# pred = (tf.reduce_sum(tf.multiply(tf.matmul(feat_users, M), feat_items), 1)\n",
    "#         + bias_items\n",
    "#         + bias_users)\n",
    "cost_l2 = tf.losses.mean_squared_error(rate_batch, pred)\n",
    "\n",
    "l2_user = tf.nn.l2_loss(weight_users)\n",
    "l2_item = tf.nn.l2_loss(weight_items)\n",
    "l2_bias_user = tf.nn.l2_loss(bias_users)\n",
    "l2_bias_item = tf.nn.l2_loss(bias_items)\n",
    "regularizer = tf.add(l2_user, l2_item)\n",
    "regularizer = tf.add(regularizer, l2_bias_user)\n",
    "regularizer = tf.add(regularizer, l2_bias_item)\n",
    "# regularizer = tf.nn.l2_loss(M)\n",
    "penalty = tf.constant(LAMBDA_REG, dtype=tf.float32, shape=[])\n",
    "cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 6.632528382092143 test rmse 5.743397778492466\n",
      "train rmse 5.671019098106088 test rmse 4.985956879724265\n",
      "train rmse 4.838722425464532 test rmse 4.368438377781428\n",
      "train rmse 4.147162347423678 test rmse 3.8839308665101235\n",
      "train rmse 3.5970648031922163 test rmse 3.5155957030029286\n",
      "train rmse 3.1793589129302475 test rmse 3.2409100693158055\n",
      "train rmse 2.875633501663359 test rmse 3.036608335128065\n",
      "train rmse 2.6620647737344982 test rmse 2.8824899384454508\n",
      "train rmse 2.514386553414219 test rmse 2.7628161288314064\n",
      "train rmse 2.4110227538862175 test rmse 2.6660776878650654\n",
      "train rmse 2.334549098946485 test rmse 2.5844804513951734\n",
      "train rmse 2.2724199147900728 test rmse 2.5132934949468084\n",
      "train rmse 2.21677095507286 test rmse 2.449752822396297\n",
      "train rmse 2.1632177583986927 test rmse 2.391944371581309\n",
      "train rmse 2.109438633841968 test rmse 2.338069083558\n",
      "train rmse 2.054124994480725 test rmse 2.2861775520571515\n",
      "train rmse 1.9963812275787551 test rmse 2.2343649697245365\n",
      "train rmse 1.9356050145783714 test rmse 2.181136111371348\n",
      "train rmse 1.8716336230007593 test rmse 2.125792187400083\n",
      "train rmse 1.8050017640588651 test rmse 2.0686051147987814\n",
      "train rmse 1.7370159902105657 test rmse 2.0106374853219044\n",
      "train rmse 1.6695277055587594 test rmse 1.9532855158650098\n",
      "train rmse 1.6044307425325712 test rmse 1.8978165603963606\n",
      "train rmse 1.5431881519248425 test rmse 1.8451329879593001\n",
      "train rmse 1.48663543451686 test rmse 1.795800194430848\n",
      "train rmse 1.435057430779556 test rmse 1.750167429952755\n",
      "train rmse 1.3883841386835838 test rmse 1.7084637878974447\n",
      "train rmse 1.3463366696114196 test rmse 1.6708352964901827\n",
      "train rmse 1.3085414135843652 test rmse 1.6373393263867653\n",
      "train rmse 1.2746075830379777 test rmse 1.6078951059072049\n",
      "train rmse 1.2441360257960516 test rmse 1.5822141941869379\n",
      "train rmse 1.2166855762701825 test rmse 1.5597747687824817\n",
      "train rmse 1.1917342438021352 test rmse 1.5398915957406516\n",
      "train rmse 1.1687313894330285 test rmse 1.521834092109337\n",
      "train rmse 1.147180972744488 test rmse 1.5049557677451064\n",
      "train rmse 1.126700599449095 test rmse 1.488775937025002\n",
      "train rmse 1.107034605420936 test rmse 1.4729811413730238\n",
      "train rmse 1.0880095658267466 test rmse 1.4573862906278017\n",
      "train rmse 1.0694887095671757 test rmse 1.4419070978647275\n",
      "train rmse 1.0513857552669115 test rmse 1.4265767425795473\n",
      "train rmse 1.0337294047655394 test rmse 1.4115474527336904\n",
      "train rmse 1.0166752007131468 test rmse 1.3970431403705894\n",
      "train rmse 1.000452118234462 test rmse 1.3833017722899512\n",
      "train rmse 0.9852561942338727 test rmse 1.3705214911956998\n",
      "train rmse 0.9712076085092692 test rmse 1.3588071930753038\n",
      "train rmse 0.9583263431515279 test rmse 1.3481502788439144\n",
      "train rmse 0.9465526611280239 test rmse 1.3384486461730172\n",
      "train rmse 0.9357872894953373 test rmse 1.3295603905426465\n",
      "train rmse 0.9259362714683991 test rmse 1.3213493293951335\n",
      "train rmse 0.9169138914569739 test rmse 1.313704709797707\n",
      "train rmse 0.9086419798759718 test rmse 1.3065433209372657\n",
      "train rmse 0.9010452241037222 test rmse 1.299805879771476\n",
      "train rmse 0.8940570357261379 test rmse 1.293459197086121\n",
      "train rmse 0.88762296039759 test rmse 1.2874912743133855\n",
      "train rmse 0.8817105800879516 test rmse 1.2818894535107284\n",
      "train rmse 0.8762971935542012 test rmse 1.2766123319539107\n",
      "train rmse 0.8713462361037372 test rmse 1.271600861494547\n",
      "train rmse 0.8667933281322663 test rmse 1.2668023461321638\n",
      "train rmse 0.8625522487128786 test rmse 1.2622004200745622\n",
      "train rmse 0.8585403897866005 test rmse 1.2578121682871506\n",
      "train rmse 0.854696439037267 test rmse 1.2536659366954115\n",
      "train rmse 0.8509913637819037 test rmse 1.2497873125330579\n",
      "train rmse 0.8474308188792803 test rmse 1.246185490690058\n",
      "train rmse 0.8440300865121537 test rmse 1.242854437136093\n",
      "train rmse 0.8407969543544331 test rmse 1.239785276787625\n",
      "train rmse 0.8377340744062128 test rmse 1.2369772124693308\n",
      "train rmse 0.8348371804414575 test rmse 1.2344410552687826\n",
      "train rmse 0.8321078269714162 test rmse 1.2321897551099674\n",
      "train rmse 0.8295567928598717 test rmse 1.2302325309593765\n",
      "train rmse 0.8271900153040789 test rmse 1.2285577860816155\n",
      "train rmse 0.8250086827254718 test rmse 1.2271315111288883\n",
      "train rmse 0.8230044201445975 test rmse 1.2258938467172389\n",
      "train rmse 0.821152611100868 test rmse 1.224771978604295\n",
      "train rmse 0.819429049015001 test rmse 1.2236921413253143\n",
      "train rmse 0.8178016382588625 test rmse 1.2225934582268634\n",
      "train rmse 0.8162448851350649 test rmse 1.2214355224755769\n",
      "train rmse 0.8147447096122352 test rmse 1.2201957441638709\n",
      "train rmse 0.8132951220411252 test rmse 1.2188663427135238\n",
      "train rmse 0.8118984122714538 test rmse 1.2174528995622738\n",
      "train rmse 0.8105613982956207 test rmse 1.2159786014085763\n",
      "train rmse 0.8092894578894507 test rmse 1.2144785144958046\n",
      "train rmse 0.8080860700324597 test rmse 1.2129932796263805\n",
      "train rmse 0.8069528709674915 test rmse 1.2115667369533285\n",
      "train rmse 0.8058859409792501 test rmse 1.2102390366884894\n",
      "train rmse 0.804879026859955 test rmse 1.209036929169991\n",
      "train rmse 0.8039263486435022 test rmse 1.207972187574689\n",
      "train rmse 0.8030220500108636 test rmse 1.2070368300620664\n",
      "train rmse 0.8021604616477469 test rmse 1.2062149003214337\n",
      "train rmse 0.8013368121532877 test rmse 1.2054870505189224\n",
      "train rmse 0.8005458578857322 test rmse 1.2048365321417465\n",
      "train rmse 0.7997882547891202 test rmse 1.2042497604018265\n",
      "train rmse 0.7990627550490967 test rmse 1.2037191038831119\n",
      "train rmse 0.7983701181066882 test rmse 1.2032348780804285\n",
      "train rmse 0.7977099812463128 test rmse 1.2027903995678813\n",
      "train rmse 0.7970811539809775 test rmse 1.2023766407176637\n",
      "train rmse 0.7964791829537239 test rmse 1.2019852528262482\n",
      "train rmse 0.7959010211926046 test rmse 1.2016069818207515\n",
      "train rmse 0.7953466831498073 test rmse 1.2012344964304134\n",
      "train rmse 0.7948139689171624 test rmse 1.200864923226232\n",
      "train rmse 0.794305098164543 test rmse 1.2004993075456571\n",
      "train rmse 0.793819741316597 test rmse 1.2001385469723969\n",
      "train rmse 0.7933586176948934 test rmse 1.1997816522949474\n",
      "train rmse 0.7929193265198439 test rmse 1.1994232599787549\n",
      "train rmse 0.7925018664918695 test rmse 1.1990636172275742\n",
      "train rmse 0.7921026225770565 test rmse 1.1987044135406653\n",
      "train rmse 0.7917207565568273 test rmse 1.1983520656407294\n",
      "train rmse 0.7913534690978054 test rmse 1.1980140425315995\n",
      "train rmse 0.7909981808092922 test rmse 1.197694188342614\n",
      "train rmse 0.7906550209467369 test rmse 1.19739445900171\n",
      "train rmse 0.7903228363290339 test rmse 1.1971121809489056\n",
      "train rmse 0.7900013767302867 test rmse 1.196846420305426\n",
      "train rmse 0.7896906552480046 test rmse 1.1965977360062092\n",
      "train rmse 0.7893912131123704 test rmse 1.1963645444139501\n",
      "train rmse 0.7891036296738283 test rmse 1.1961476020475459\n",
      "train rmse 0.7888276912192714 test rmse 1.1959430801504634\n",
      "train rmse 0.7885615958983914 test rmse 1.1957473462566044\n",
      "train rmse 0.7883073573708789 test rmse 1.195558410481192\n",
      "train rmse 0.7880601465100979 test rmse 1.1953784700028378\n",
      "train rmse 0.7878215965705172 test rmse 1.195208426538634\n",
      "train rmse 0.7875900126285557 test rmse 1.1950528729285783\n",
      "train rmse 0.7873668013050609 test rmse 1.194909719783816\n",
      "train rmse 0.7871499630896833 test rmse 1.1947806178510265\n",
      "train rmse 0.786939919833478 test rmse 1.1946634762060047\n",
      "train rmse 0.7867369421453205 test rmse 1.1945555041428704\n",
      "train rmse 0.7865408839338593 test rmse 1.1944510653161968\n",
      "train rmse 0.7863515987767836 test rmse 1.1943452199979119\n",
      "train rmse 0.7861690536580319 test rmse 1.1942348733782076\n",
      "train rmse 0.7859931774401432 test rmse 1.1941202737847871\n",
      "train rmse 0.7858237849761489 test rmse 1.194001619673354\n",
      "train rmse 0.7856606149301472 test rmse 1.1938831035445636\n",
      "train rmse 0.7855027226692863 test rmse 1.1937675714447846\n",
      "train rmse 0.7853504529070943 test rmse 1.1936564723419918\n",
      "train rmse 0.785202366627524 test rmse 1.1935533531355043\n",
      "train rmse 0.7850590356264888 test rmse 1.1934592646921354\n",
      "train rmse 0.7849206143833538 test rmse 1.1933756575901757\n",
      "train rmse 0.7847862320696519 test rmse 1.1933014351472606\n",
      "train rmse 0.7846558147977513 test rmse 1.193234001603884\n",
      "train rmse 0.7845289846697939 test rmse 1.1931708104174927\n",
      "train rmse 0.7844063893150843 test rmse 1.1931104634569925\n",
      "train rmse 0.784287270735001 test rmse 1.1930533608320302\n",
      "train rmse 0.7841715545042525 test rmse 1.1930001525143368\n",
      "train rmse 0.7840600023347702 test rmse 1.1929513886293228\n",
      "train rmse 0.7839508292752837 test rmse 1.1929059704722866\n",
      "train rmse 0.7838454430856172 test rmse 1.1928634487161638\n",
      "train rmse 0.783743807267785 test rmse 1.1928220747417748\n",
      "train rmse 0.7836442879746052 test rmse 1.192780949188061\n",
      "train rmse 0.7835484454520932 test rmse 1.1927388227583104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.7834564712473342 test rmse 1.192696345017776\n",
      "train rmse 0.7833662743101483 test rmse 1.192653515928921\n",
      "train rmse 0.7832789967000569 test rmse 1.1926114349761263\n",
      "train rmse 0.7831946393930316 test rmse 1.1925710018781142\n",
      "train rmse 0.7831125944331664 test rmse 1.1925316670049866\n",
      "train rmse 0.7830327864270761 test rmse 1.192494929960657\n",
      "train rmse 0.7829554063782449 test rmse 1.1924605410623286\n",
      "train rmse 0.7828811402217873 test rmse 1.192429050358236\n",
      "train rmse 0.7828072096594486 test rmse 1.1924010079360075\n",
      "train rmse 0.7827360134867196 test rmse 1.1923768139437567\n",
      "train rmse 0.782667476294043 test rmse 1.1923566685715206\n",
      "train rmse 0.7826001516972274 test rmse 1.1923393722731757\n",
      "train rmse 0.7825352587084023 test rmse 1.1923252251142922\n",
      "train rmse 0.7824713125241387 test rmse 1.192311877640887\n",
      "train rmse 0.782409684633314 test rmse 1.192299029931721\n",
      "train rmse 0.7823493089701253 test rmse 1.1922853322216063\n",
      "train rmse 0.782291062037763 test rmse 1.1922717343391265\n",
      "train rmse 0.7822340680356406 test rmse 1.1922568364810253\n",
      "train rmse 0.7821796988975697 test rmse 1.1922413385116208\n",
      "train rmse 0.7821255546056955 test rmse 1.1922260903128519\n",
      "train rmse 0.7820729308401052 test rmse 1.1922109419091633\n",
      "train rmse 0.7820231998421787 test rmse 1.1921966932349766\n",
      "train rmse 0.7819731607880223 test rmse 1.1921846942193648\n",
      "train rmse 0.7819257484024359 test rmse 1.1921738450054538\n",
      "train rmse 0.7818792479328783 test rmse 1.1921638456425265\n",
      "train rmse 0.7818340788453932 test rmse 1.1921567460439342\n",
      "train rmse 0.7817899364057935 test rmse 1.1921494964101929\n",
      "train rmse 0.7817471638924804 test rmse 1.1921434466820848\n",
      "train rmse 0.7817055709066469 test rmse 1.1921394968430359\n",
      "train rmse 0.781665538903611 test rmse 1.1921355469908999\n",
      "train rmse 0.7816258480080233 test rmse 1.192133047077722\n",
      "train rmse 0.7815871083603987 test rmse 1.1921308971481965\n",
      "train rmse 0.7815497014257283 test rmse 1.1921299471781013\n",
      "train rmse 0.7815131316518651 test rmse 1.192128897210273\n",
      "train rmse 0.7814775135639446 test rmse 1.1921269972661381\n",
      "train rmse 0.7814434193552697 test rmse 1.1921255973053642\n",
      "train rmse 0.7814095524941421 test rmse 1.192123347364962\n",
      "train rmse 0.7813767521078347 test rmse 1.1921213974131784\n",
      "train rmse 0.7813456286077576 test rmse 1.1921187474735908\n",
      "train rmse 0.7813137791349033 test rmse 1.1921169975102563\n",
      "train rmse 0.7812834923240813 test rmse 1.1921155975377393\n",
      "train rmse 0.7812542343024901 test rmse 1.192114847551786\n",
      "train rmse 0.7812252422224597 test rmse 1.1921154475405864\n",
      "train rmse 0.7811974698526258 test rmse 1.1921162975242034\n",
      "train rmse 0.781169925400752 test rmse 1.1921182474843288\n",
      "train rmse 0.7811430667174193 test rmse 1.1921197974503583\n",
      "train rmse 0.7811175424821413 test rmse 1.1921219473999018\n",
      "train rmse 0.7810928186608332 test rmse 1.1921241973429464\n",
      "train rmse 0.7810678651217462 test rmse 1.1921267472732633\n",
      "train rmse 0.7810437120838368 test rmse 1.1921285472207914\n",
      "train rmse 0.7810203596213718 test rmse 1.1921304471624563\n",
      "train rmse 0.7809983038767861 test rmse 1.19213314707435\n",
      "train rmse 0.7809759422265057 test rmse 1.1921351470051438\n",
      "train rmse 0.7809543813260031 test rmse 1.1921375969157937\n",
      "train rmse 0.7809335067543127 test rmse 1.1921399968233857\n",
      "train rmse 0.7809132422396309 test rmse 1.19214289670528\n",
      "train rmse 0.7808933588431166 test rmse 1.1921456465867672\n",
      "train rmse 0.7808737802631835 test rmse 1.1921489964338272\n",
      "train rmse 0.7808551553494824 test rmse 1.1921524962639845\n",
      "train rmse 0.7808361483197138 test rmse 1.1921559960838675\n",
      "train rmse 0.780817827853126 test rmse 1.1921595458906817\n",
      "train rmse 0.780800728363143 test rmse 1.1921632956751322\n",
      "train rmse 0.7807836284986739 test rmse 1.1921667954633115\n",
      "train rmse 0.7807671771595275 test rmse 1.1921705952217019\n",
      "train rmse 0.7807503437599698 test rmse 1.192173445032547\n",
      "train rmse 0.7807351514007106 test rmse 1.1921778447271414\n",
      "train rmse 0.7807193098067375 test rmse 1.1921797945865986\n",
      "train rmse 0.7807046512744208 test rmse 1.1921848442078053\n",
      "train rmse 0.7806892289797687 test rmse 1.1921880939527159\n",
      "train rmse 0.780675218858901 test rmse 1.1921917936515234\n",
      "train rmse 0.780660979432104 test rmse 1.1921964432567163\n",
      "train rmse 0.7806471978628553 test rmse 1.192199243010236\n",
      "train rmse 0.7806343704772954 test rmse 1.1922043925397203\n",
      "train rmse 0.7806205502605441 test rmse 1.192208342163767\n",
      "train rmse 0.7806078751505197 test rmse 1.192211791824698\n",
      "train rmse 0.7805960397716151 test rmse 1.1922162413726072\n",
      "train rmse 0.7805835933408619 test rmse 1.1922198409947196\n",
      "train rmse 0.7805711848917946 test rmse 1.192223190633312\n",
      "train rmse 0.7805603416548037 test rmse 1.192228190076392\n",
      "train rmse 0.7805489255488576 test rmse 1.19223093976115\n",
      "train rmse 0.780538043820873 test rmse 1.1922342393744885\n",
      "train rmse 0.7805270473942718 test rmse 1.1922383888752692\n",
      "train rmse 0.7805163180926059 test rmse 1.1922415384867027\n",
      "train rmse 0.7805066577766712 test rmse 1.1922452380196644\n",
      "train rmse 0.780496386400517 test rmse 1.1922492375018447\n",
      "train rmse 0.7804868403907085 test rmse 1.1922520871247133\n",
      "train rmse 0.7804773706336187 test rmse 1.192257636370753\n",
      "train rmse 0.7804680916876017 test rmse 1.1922601860156792\n",
      "train rmse 0.7804588508169095 test rmse 1.192264435411773\n",
      "train rmse 0.7804502971861957 test rmse 1.1922668350640524\n",
      "train rmse 0.7804412470373471 test rmse 1.1922731341283068\n",
      "train rmse 0.7804333805779874 test rmse 1.1922718343241223\n",
      "train rmse 0.7804251321665098 test rmse 1.1922832325587007\n",
      "train rmse 0.7804165017908034 test rmse 1.192275083831927\n",
      "train rmse 0.7804088642105752 test rmse 1.1922963803869309\n",
      "train rmse 0.7804010738017381 test rmse 1.1922729841509734\n",
      "train rmse 0.7803941234693894 test rmse 1.1923161268527527\n",
      "train rmse 0.7803869057510474 test rmse 1.1922676349470722\n",
      "train rmse 0.7803800698610128 test rmse 1.1923254750655525\n",
      "train rmse 0.7803731193415943 test rmse 1.1922893315692924\n",
      "train rmse 0.7803661687602693 test rmse 1.1922995798365057\n",
      "train rmse 0.780359485450613 test rmse 1.1923283245062157\n",
      "train rmse 0.7803528020837176 test rmse 1.1922921810963345\n",
      "train rmse 0.7803473407757002 test rmse 1.1923226756084448\n",
      "train rmse 0.780340924644725 test rmse 1.1923269247818287\n",
      "train rmse 0.7803345084609947 test rmse 1.1923034791540672\n",
      "train rmse 0.7803287796809406 test rmse 1.192336722818032\n",
      "train rmse 0.7803228980896629 test rmse 1.19232792458513\n",
      "train rmse 0.7803174365723526 test rmse 1.1923174266086518\n",
      "train rmse 0.7803116312812732 test rmse 1.192346170848118\n",
      "train rmse 0.7803060551057543 test rmse 1.1923320237699004\n",
      "train rmse 0.7803012809512154 test rmse 1.1923303241096328\n",
      "train rmse 0.7802966213482185 test rmse 1.192353119351569\n",
      "train rmse 0.7802909686777224 test rmse 1.1923378225925798\n",
      "train rmse 0.780286461789473 test rmse 1.192342021722432\n",
      "train rmse 0.7802814583491454 test rmse 1.1923596179152742\n",
      "train rmse 0.7802774861293978 test rmse 1.1923449711024157\n",
      "train rmse 0.7802727499948082 test rmse 1.1923528694061039\n",
      "train rmse 0.7802685103660729 test rmse 1.1923652666380289\n",
      "train rmse 0.7802640797384539 test rmse 1.1923520195911301\n",
      "train rmse 0.7802598782579886 test rmse 1.1923626672291117\n",
      "train rmse 0.7802557149504838 test rmse 1.192370015543533\n",
      "train rmse 0.7802512842502066 test rmse 1.1923593179823444\n",
      "train rmse 0.7802480757963685 test rmse 1.1923711152873737\n",
      "train rmse 0.7802439124258869 test rmse 1.1923745144891955\n",
      "train rmse 0.7802400164077 test rmse 1.1923670662254982\n",
      "train rmse 0.7802357766010976 test rmse 1.192379263357868\n",
      "train rmse 0.7802326062802051 test rmse 1.192378713489832\n",
      "train rmse 0.7802292831589354 test rmse 1.1923745144891955\n",
      "train rmse 0.7802259218264707 test rmse 1.1923862616561753\n",
      "train rmse 0.780222369493468 test rmse 1.1923834623417813\n",
      "train rmse 0.7802192755128361 test rmse 1.1923822126457337\n",
      "train rmse 0.7802163343099958 test rmse 1.192392460114661\n",
      "train rmse 0.7802132021077672 test rmse 1.1923883111500468\n",
      "train rmse 0.7802102226842219 test rmse 1.1923898607650254\n",
      "train rmse 0.7802070904574577 test rmse 1.1923971589246225\n",
      "train rmse 0.7802041110105734 test rmse 1.1923925600895338\n",
      "train rmse 0.7802012843453192 test rmse 1.1923964591030358\n",
      "train rmse 0.7801985722649758 test rmse 1.1924017077549245\n",
      "train rmse 0.7801957073811343 test rmse 1.1923976088097112\n",
      "train rmse 0.7801933390692158 test rmse 1.1924029574305404\n",
      "train rmse 0.7801907033587897 test rmse 1.1924059066598012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.7801876474514967 test rmse 1.192403057404533\n",
      "train rmse 0.7801853555131718 test rmse 1.1924086559347462\n",
      "train rmse 0.7801830253689727 test rmse 1.1924094057359038\n",
      "train rmse 0.7801803896237034 test rmse 1.1924081060802643\n",
      "train rmse 0.7801785942558852 test rmse 1.1924138545464076\n",
      "train rmse 0.7801764550888355 test rmse 1.1924127048553959\n",
      "train rmse 0.7801739339201397 test rmse 1.192413804559865\n",
      "train rmse 0.7801717565406187 test rmse 1.1924181533812384\n",
      "train rmse 0.7801696555546183 test rmse 1.1924165538166003\n",
      "train rmse 0.7801677455624354 test rmse 1.192419103121727\n",
      "train rmse 0.7801659501655199 test rmse 1.19242155244897\n",
      "train rmse 0.7801637345636309 test rmse 1.1924206027104323\n",
      "train rmse 0.7801620919581688 test rmse 1.192424001771182\n",
      "train rmse 0.7801596853439207 test rmse 1.1924247515626902\n",
      "train rmse 0.780157889928456 test rmse 1.1924242517017372\n",
      "train rmse 0.7801560563083976 test rmse 1.1924278506959218\n",
      "train rmse 0.7801543754862249 test rmse 1.1924274508082153\n",
      "train rmse 0.7801528474629253 test rmse 1.192428550499085\n",
      "train rmse 0.7801510902324311 test rmse 1.1924311497643818\n",
      "train rmse 0.7801495240019153 test rmse 1.192430449962744\n",
      "train rmse 0.7801479959691136 test rmse 1.1924327493094407\n",
      "train rmse 0.7801460859239027 test rmse 1.192433848995424\n",
      "train rmse 0.7801450926985448 test rmse 1.192434098923915\n",
      "train rmse 0.7801433354505833 test rmse 1.1924363482779778\n",
      "train rmse 0.7801416546010034 test rmse 1.1924371480472888\n",
      "train rmse 0.7801403557602112 test rmse 1.1924372480184149\n",
      "train rmse 0.7801392479237091 test rmse 1.1924391974536996\n",
      "train rmse 0.7801376816694183 test rmse 1.192438647613301\n",
      "train rmse 0.7801363828220117 test rmse 1.1924404470900265\n",
      "train rmse 0.7801350839724425 test rmse 1.192441446798145\n",
      "train rmse 0.7801340907330776 test rmse 1.192441346827371\n",
      "train rmse 0.7801329446860852 test rmse 1.1924438960794932\n",
      "train rmse 0.7801319514439966 test rmse 1.1924437961089245\n",
      "train rmse 0.7801304615784928 test rmse 1.1924448957847202\n",
      "train rmse 0.7801293919297095 test rmse 1.1924455955778803\n",
      "train rmse 0.7801280930685012 test rmse 1.1924467452371807\n",
      "train rmse 0.7801270234164706 test rmse 1.1924462953706298\n",
      "train rmse 0.7801256099454662 test rmse 1.1924499442833132\n",
      "train rmse 0.780125800955211 test rmse 1.1924467452371807\n",
      "train rmse 0.7801243874819918 test rmse 1.1924520936376135\n",
      "train rmse 0.7801248077040271 test rmse 1.19244939444787\n",
      "train rmse 0.7801219807514284 test rmse 1.1924499942683409\n",
      "train rmse 0.7801207964844572 test rmse 1.1924552926693976\n",
      "train rmse 0.7801199942380682 test rmse 1.1924464453261656\n",
      "train rmse 0.7801185425520292 test rmse 1.1924614907690576\n",
      "train rmse 0.7801191537885854 test rmse 1.1924433962265655\n",
      "train rmse 0.7801169762561679 test rmse 1.1924683886162857\n",
      "train rmse 0.780117052660917 test rmse 1.1924375979172905\n",
      "train rmse 0.7801167470418756 test rmse 1.192476336086448\n",
      "train rmse 0.7801163650179055 test rmse 1.1924406470317173\n",
      "train rmse 0.7801141110726739 test rmse 1.1924608909543695\n",
      "train rmse 0.7801135380347124 test rmse 1.19246578943216\n",
      "train rmse 0.7801128121860235 test rmse 1.1924429963440724\n",
      "train rmse 0.7801123537549246 test rmse 1.1924704879531745\n",
      "train rmse 0.7801113604866196 test rmse 1.1924575419834824\n",
      "train rmse 0.7801098323790653 test rmse 1.1924500942383902\n",
      "train rmse 0.7801099469872357 test rmse 1.1924715876043557\n",
      "train rmse 0.7801094121489631 test rmse 1.192454792821247\n",
      "train rmse 0.7801075402121216 test rmse 1.1924561424107727\n",
      "train rmse 0.7801072345893535 test rmse 1.1924718375249372\n",
      "train rmse 0.7801069289664658 test rmse 1.1924546928515918\n",
      "train rmse 0.7801055154590524 test rmse 1.1924610409080698\n",
      "train rmse 0.7801051334295821 test rmse 1.1924705879215056\n",
      "train rmse 0.7801048660088415 test rmse 1.1924557925173387\n",
      "train rmse 0.7801043311670854 test rmse 1.1924646397912202\n",
      "train rmse 0.7801036435128603 test rmse 1.1924694382851921\n",
      "train rmse 0.7801031086702661 test rmse 1.1924576419528987\n",
      "train rmse 0.7801025356242225 test rmse 1.192467089024931\n",
      "train rmse 0.7801018479684145 test rmse 1.192469038411432\n",
      "train rmse 0.7801013895308725 test rmse 1.1924592914470593\n",
      "train rmse 0.7801010457025391 test rmse 1.1924692883325478\n",
      "train rmse 0.7801003962486072 test rmse 1.1924685885532915\n",
      "train rmse 0.7801000906230403 test rmse 1.1924618406608196\n",
      "train rmse 0.7800993647618392 test rmse 1.1924697881746222\n",
      "train rmse 0.7800992119488687 test rmse 1.1924692883325478\n",
      "train rmse 0.7800986388999626 test rmse 1.1924624404750301\n",
      "train rmse 0.7800977984274723 test rmse 1.1924726372704462\n",
      "train rmse 0.7800973399875505 test rmse 1.1924676388522155\n",
      "train rmse 0.7800978366307869 test rmse 1.1924669890563067\n",
      "train rmse 0.7800965377170389 test rmse 1.192471787540825\n",
      "train rmse 0.7800966905305333 test rmse 1.1924695382536112\n",
      "train rmse 0.7800957736491181 test rmse 1.192467388930754\n",
      "train rmse 0.7800951623942426 test rmse 1.192473237079226\n",
      "train rmse 0.780095506225169 test rmse 1.1924696882062242\n",
      "train rmse 0.7800947039527716 test rmse 1.1924682386635095\n",
      "train rmse 0.7800946275458336 test rmse 1.1924739868397762\n",
      "train rmse 0.7800943601214916 test rmse 1.192468938442971\n",
      "train rmse 0.7800939398830546 test rmse 1.192470188048131\n",
      "train rmse 0.7800935960514378 test rmse 1.192474636631872\n",
      "train rmse 0.7800932140161303 test rmse 1.1924682886477702\n",
      "train rmse 0.7800931376090464 test rmse 1.192473237079226\n",
      "train rmse 0.7800923735377953 test rmse 1.1924725872863675\n",
      "train rmse 0.7800925263521054 test rmse 1.1924712377154534\n",
      "train rmse 0.7800919150946853 test rmse 1.1924721874136632\n",
      "train rmse 0.7800918386874741 test rmse 1.1924761361507414\n",
      "train rmse 0.780091189225877 test rmse 1.1924689884272026\n",
      "train rmse 0.7800920679090853 test rmse 1.1924804847447996\n",
      "train rmse 0.7800909600040077 test rmse 1.1924706878898284\n",
      "train rmse 0.7800921825198656 test rmse 1.192476885909469\n",
      "train rmse 0.7800896992825227 test rmse 1.1924742867438645\n",
      "train rmse 0.7800899667084625 test rmse 1.1924727872226693\n",
      "train rmse 0.7800896228750945 test rmse 1.1924773857483588\n",
      "train rmse 0.7800890880228876 test rmse 1.1924734869994618\n",
      "train rmse 0.7800897756899434 test rmse 1.1924756862952786\n",
      "train rmse 0.7800883239476699 test rmse 1.1924744866798813\n",
      "train rmse 0.7800886295778467 test rmse 1.192474386711877\n",
      "train rmse 0.7800888970041533 test rmse 1.19247713582894\n",
      "train rmse 0.7800877890945723 test rmse 1.1924743367278718\n",
      "train rmse 0.780088056521167 test rmse 1.1924756862952786\n",
      "train rmse 0.7800872924449391 test rmse 1.192476436054289\n",
      "train rmse 0.7800869868142384 test rmse 1.1924746866158646\n",
      "train rmse 0.7800873306487682 test rmse 1.1924774857161118\n",
      "train rmse 0.7800862609408443 test rmse 1.1924756862952786\n",
      "train rmse 0.780086146329194 test rmse 1.192476436054289\n",
      "train rmse 0.7800863373486018 test rmse 1.19247798555475\n",
      "train rmse 0.7800856114746033 test rmse 1.192475936215001\n",
      "train rmse 0.780085802494142 test rmse 1.1924774857161118\n",
      "train rmse 0.7800854586589386 test rmse 1.1924777856193198\n",
      "train rmse 0.7800851912314535 test rmse 1.192476885909469\n",
      "train rmse 0.7800853440471706 test rmse 1.19247838542551\n",
      "train rmse 0.7800849620078217 test rmse 1.1924778855870393\n",
      "train rmse 0.7800847327841226 test rmse 1.1924769358933673\n",
      "train rmse 0.7800848855999295 test rmse 1.1924788852637713\n",
      "train rmse 0.7800840069086311 test rmse 1.192477235796714\n",
      "train rmse 0.7800841215205957 test rmse 1.1924780855224526\n",
      "train rmse 0.7800840833166094 test rmse 1.1924788852637713\n",
      "train rmse 0.7800838922966497 test rmse 1.1924777856193198\n",
      "train rmse 0.7800838922966497 test rmse 1.1924787852961358\n",
      "train rmse 0.7800836248686275 test rmse 1.192478935247586\n",
      "train rmse 0.7800834338485555 test rmse 1.1924780355386024\n",
      "train rmse 0.7800837012766432 test rmse 1.1924796850045536\n",
      "train rmse 0.7800832046244073 test rmse 1.1924782854578326\n",
      "train rmse 0.7800832428284367 test rmse 1.1924791351828234\n",
      "train rmse 0.7800830518082712 test rmse 1.1924791351828234\n",
      "train rmse 0.7800827079718553 test rmse 1.1924787852961358\n",
      "train rmse 0.7800829754001919 test rmse 1.1924791851666277\n",
      "train rmse 0.7800828225840107 test rmse 1.192479934923438\n",
      "train rmse 0.7800825551556219 test rmse 1.1924784853931791\n",
      "train rmse 0.780082593359683 test rmse 1.192480984582181\n",
      "train rmse 0.7800823259312155 test rmse 1.1924788852637713\n",
      "train rmse 0.7800823259312155 test rmse 1.192481084549632\n",
      "train rmse 0.7800821731149072 test rmse 1.1924786853284919\n",
      "train rmse 0.7800821349108255 test rmse 1.1924817343378602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse 0.7800818674822008 test rmse 1.1924774357322363\n",
      "train rmse 0.7800819056862957 test rmse 1.1924836337168045\n",
      "train rmse 0.780081829278104 test rmse 1.1924754363755037\n",
      "train rmse 0.780082020298569 test rmse 1.1924872325317708\n",
      "train rmse 0.7800816764616985 test rmse 1.192471987477261\n",
      "train rmse 0.780081829278104 test rmse 1.1924919809515655\n",
      "train rmse 0.7800814854411493 test rmse 1.1924662392913563\n",
      "train rmse 0.7800816000534844 test rmse 1.1924994284350547\n",
      "train rmse 0.7800811416040431 test rmse 1.1924583917232545\n",
      "train rmse 0.7800814090329166 test rmse 1.1925058262360795\n",
      "train rmse 0.7800807977667853 test rmse 1.1924576419528987\n",
      "train rmse 0.7800813708287974 test rmse 1.1924974790982326\n",
      "train rmse 0.7800805303377022 test rmse 1.1924745366638803\n",
      "train rmse 0.7800808359709325 test rmse 1.1924759861989394\n",
      "train rmse 0.7800807213584852 test rmse 1.1924945800786038\n",
      "train rmse 0.7800803393168724 test rmse 1.1924642399158512\n",
      "train rmse 0.7800807595626361 test rmse 1.1924962295216706\n",
      "train rmse 0.780080110091815 test rmse 1.1924735369835024\n",
      "train rmse 0.7800803775210421 test rmse 1.1924793851018232\n",
      "train rmse 0.7800801482959959 test rmse 1.1924910812524248\n",
      "train rmse 0.780080110091815 test rmse 1.192467838789347\n",
      "train rmse 0.7800804539293759 test rmse 1.1924943301627886\n",
      "train rmse 0.7800800718876322 test rmse 1.192474086807814\n",
      "train rmse 0.7800800718876322 test rmse 1.1924829339459666\n",
      "train rmse 0.7800803011127009 test rmse 1.1924868326639777\n",
      "train rmse 0.7800799190708823 test rmse 1.1924764860382062\n",
      "train rmse 0.7800811416040431 test rmse 1.192487182548304\n",
      "train rmse 0.7800804539293759 test rmse 1.1924818842889395\n",
      "train rmse 0.7800808359709325 test rmse 1.1924802848097884\n",
      "train rmse 0.780080110091815 test rmse 1.1924850332572487\n",
      "train rmse 0.7800793078035835 test rmse 1.1924818842889395\n",
      "train rmse 0.7800813326246764 test rmse 1.192481484419353\n",
      "train rmse 0.7800794988246658 test rmse 1.1924879822835217\n",
      "train rmse 0.7800814090329166 test rmse 1.1924769858772637\n",
      "train rmse 0.7800790021697543 test rmse 1.1924867326970083\n",
      "train rmse 0.7800797280499029 test rmse 1.1924814344356451\n",
      "train rmse 0.7800794224162385 test rmse 1.1924796850045536\n",
      "train rmse 0.7800792313951374 test rmse 1.1924900316025693\n",
      "train rmse 0.7800795370288767 test rmse 1.192473237079226\n",
      "train rmse 0.7800802629085274 test rmse 1.1924965794032398\n",
      "train rmse 0.7800794988246658 test rmse 1.1924718375249372\n",
      "train rmse 0.7800810269916406 test rmse 1.1924923808176324\n",
      "train rmse 0.7800785055145267 test rmse 1.192480984582181\n",
      "train rmse 0.7800793078035835 test rmse 1.1924777856193198\n",
      "train rmse 0.7800790785782229 test rmse 1.1924952798426078\n",
      "train rmse 0.7800788875570376 test rmse 1.192470038095581\n",
      "train rmse 0.7800797662541025 test rmse 1.1924943301627886\n",
      "train rmse 0.7800782380846578 test rmse 1.1924780355386024\n",
      "train rmse 0.7800786201272997 test rmse 1.1924839836020695\n",
      "train rmse 0.7800790021697543 test rmse 1.1924857330268548\n",
      "train rmse 0.7800786201272997 test rmse 1.1924833837986952\n",
      "train rmse 0.7800788875570376 test rmse 1.1924793351180274\n",
      "train rmse 0.7800784291060021 test rmse 1.1924905814192754\n",
      "train rmse 0.7800782380846578 test rmse 1.1924767359577613\n",
      "train rmse 0.7800783144932012 test rmse 1.1924866327300307\n",
      "train rmse 0.7800783144932012 test rmse 1.1924853831421032\n",
      "train rmse 0.7800778178375357 test rmse 1.192477235796714\n",
      "train rmse 0.7800787347400557 test rmse 1.192490931302502\n",
      "train rmse 0.7800779706546973 test rmse 1.1924787852961358\n",
      "train rmse 0.7800784291060021 test rmse 1.1924834337823214\n",
      "train rmse 0.7800780852675487 test rmse 1.1924877323663239\n",
      "train rmse 0.7800786965358056 test rmse 1.192477735635457\n",
      "train rmse 0.7800781998803834 test rmse 1.1924877323663239\n",
      "train rmse 0.7800795752330856 test rmse 1.1924829839296114\n",
      "train rmse 0.7800776268160418 test rmse 1.1924807846472536\n",
      "train rmse 0.780079651641498 test rmse 1.1924880822503863\n",
      "train rmse 0.7800776268160418 test rmse 1.1924796850045536\n",
      "train rmse 0.7800781998803834 test rmse 1.1924850332572487\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_global_step()\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(cost, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(NB_EPOCHS):\n",
    "        _, train_pred, train_mse, reg, pen, train_cost = sess.run([train_op, pred, cost_l2, regularizer, penalty, cost], feed_dict={\n",
    "            user_batch: train['user'],\n",
    "            item_batch: train['item'],\n",
    "            rate_batch: train['rating']\n",
    "        })\n",
    "        test_pred, test_mse = sess.run([pred, cost_l2], feed_dict={\n",
    "            user_batch: test['user'],\n",
    "            item_batch: test['item'],\n",
    "            rate_batch: test['rating']\n",
    "        })\n",
    "        print('train rmse', train_mse ** 0.5, 'test rmse', test_mse ** 0.5)\n",
    "        # print('reg', reg, 'full cost', train_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
